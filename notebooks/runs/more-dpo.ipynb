{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d243bd0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-01T12:11:02.875453Z",
     "iopub.status.busy": "2024-06-01T12:11:02.874463Z",
     "iopub.status.idle": "2024-06-01T12:13:55.462561Z",
     "shell.execute_reply": "2024-06-01T12:13:55.461387Z"
    },
    "papermill": {
     "duration": 172.595949,
     "end_time": "2024-06-01T12:13:55.465136",
     "exception": false,
     "start_time": "2024-06-01T12:11:02.869187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\r\n",
      "Channels:\r\n",
      " - rapidsai\r\n",
      " - nvidia\r\n",
      " - conda-forge\r\n",
      " - defaults\r\n",
      " - pytorch\r\n",
      "Platform: linux-64\r\n",
      "Collecting package metadata (repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  added / updated specs:\r\n",
      "    - aiohttp\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be downloaded:\r\n",
      "\r\n",
      "    package                    |            build\r\n",
      "    ---------------------------|-----------------\r\n",
      "    aiohttp-3.9.5              |  py310h2372a71_0         682 KB  conda-forge\r\n",
      "    ------------------------------------------------------------\r\n",
      "                                           Total:         682 KB\r\n",
      "\r\n",
      "The following packages will be UPDATED:\r\n",
      "\r\n",
      "  aiohttp                             3.9.1-py310h2372a71_0 --> 3.9.5-py310h2372a71_0 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages:\r\n",
      "\r\n",
      "Preparing transaction: / \b\bdone\r\n",
      "Verifying transaction: \\ \b\bdone\r\n",
      "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\r\n",
      "Cloning into 'cs234-project'...\r\n",
      "remote: Enumerating objects: 280, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (60/60), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (40/40), done.\u001b[K\r\n",
      "remote: Total 280 (delta 35), reused 45 (delta 20), pack-reused 220\u001b[K\r\n",
      "Receiving objects: 100% (280/280), 59.40 MiB | 22.06 MiB/s, done.\r\n",
      "Resolving deltas: 100% (142/142), done.\r\n",
      "Updating files: 100% (121/121), done.\r\n",
      "/kaggle/working/cs234-project\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\r\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3.1)\r\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Collecting git+https://github.com/mattjhayes3/trl.git\r\n",
      "  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-q313ebpv\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-q313ebpv\r\n",
      "  Resolved https://github.com/mattjhayes3/trl.git to commit 4c8e35e0e0a6b90b4b7c506cce7e31eb51b170b9\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.41.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.30.1)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.19.1)\r\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.3.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.23.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.4)\r\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\r\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.5)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\r\n",
      "Building wheels for collected packages: trl\r\n",
      "  Building wheel for trl (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for trl: filename=trl-0.8.7.dev0-py3-none-any.whl size=209532 sha256=ea7ad982f9ce24eeee8155b25fe9fe21d508572e9621d7de6380c79bc9a39fb0\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lx61krk1/wheels/b5/f5/74/f982e27bdb1c23b205f8bfcaaed174cf3d2c06bd1a5f5926cc\r\n",
      "Successfully built trl\r\n",
      "Installing collected packages: trl\r\n",
      "Successfully installed trl-0.8.7.dev0\r\n"
     ]
    }
   ],
   "source": [
    "# !conda install -y gdown\n",
    "# print('hi')\n",
    "!conda install aiohttp -y\n",
    "!git clone https://github.com/mattjhayes3/cs234-project.git\n",
    "%cd /kaggle/working/cs234-project\n",
    "!git pull --rebase\n",
    "!pip install wandb\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"KEY\")\n",
    "# wandb.init()\n",
    "\n",
    "from huggingface_hub import notebook_login,login\n",
    "\n",
    "# notebook_login(\"KEY\")\n",
    "login(\"KEY\")\n",
    "\n",
    "# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n",
    "# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n",
    "!mkdir results\n",
    "!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n",
    "!pip install git+https://github.com/mattjhayes3/trl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83dc0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T12:13:55.513774Z",
     "iopub.status.busy": "2024-06-01T12:13:55.513368Z",
     "iopub.status.idle": "2024-06-01T12:44:55.213196Z",
     "shell.execute_reply": "2024-06-01T12:44:55.211872Z"
    },
    "papermill": {
     "duration": 1859.727926,
     "end_time": "2024-06-01T12:44:55.216369",
     "exception": false,
     "start_time": "2024-06-01T12:13:55.488443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 12:14:07.034208: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 12:14:07.034323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 12:14:07.160480: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 2.67MB/s]\r\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:14<00:00, 38.2MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.0/17.0 [00:00<00:00, 97.7kB/s]\r\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 13.3MB/s]\r\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 14.7MB/s]\r\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 471kB/s]\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_121437-yj9e4xyv\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_flipped_2c2_0_2-0_05\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/yj9e4xyv\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.175926923751831, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -64.33436584472656, 'logps/chosen': -59.26305389404297, 'logits/rejected': -35.94441604614258, 'logits/chosen': -35.62873077392578, 'epoch': 0.01}\r\n",
      "{'loss': 0.693, 'grad_norm': 1.0921388864517212, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 8.151328802341595e-05, 'rewards/rejected': -0.00011849201837321743, 'rewards/accuracies': 0.5455994606018066, 'rewards/margins': 0.00020000532094854861, 'logps/rejected': -60.52128601074219, 'logps/chosen': -60.36626052856445, 'logits/rejected': -37.118011474609375, 'logits/chosen': -36.64077377319336, 'epoch': 0.27}\r\n",
      "{'loss': 0.6925, 'grad_norm': 1.142431616783142, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.00030213958234526217, 'rewards/rejected': -0.0010259978007525206, 'rewards/accuracies': 0.5699999928474426, 'rewards/margins': 0.0013281373539939523, 'logps/rejected': -60.70668411254883, 'logps/chosen': -59.924381256103516, 'logits/rejected': -37.103145599365234, 'logits/chosen': -36.761505126953125, 'epoch': 0.54}\r\n",
      "{'loss': 0.6915, 'grad_norm': 1.1598345041275024, 'learning_rate': 2e-06, 'rewards/chosen': -4.98078293276194e-07, 'rewards/rejected': -0.003401991678401828, 'rewards/accuracies': 0.5714062452316284, 'rewards/margins': 0.003401493653655052, 'logps/rejected': -60.52933120727539, 'logps/chosen': -60.248680114746094, 'logits/rejected': -36.977317810058594, 'logits/chosen': -36.478126525878906, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:05<12:02,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.74it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.76it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6905444264411926, 'eval_runtime': 5.6522, 'eval_samples_per_second': 220.267, 'eval_steps_per_second': 1.769, 'eval_rewards/chosen': 0.00044805611832998693, 'eval_rewards/rejected': -0.0050238147377967834, 'eval_rewards/accuracies': 0.5884408354759216, 'eval_rewards/margins': 0.005471870768815279, 'eval_logps/rejected': -60.951698303222656, 'eval_logps/chosen': -60.447021484375, 'eval_logits/rejected': -36.704795837402344, 'eval_logits/chosen': -36.19252014160156, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:10<12:02,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6892, 'grad_norm': 1.1113044023513794, 'learning_rate': 2e-06, 'rewards/chosen': -0.0015117275761440396, 'rewards/rejected': -0.009600329212844372, 'rewards/accuracies': 0.5838935375213623, 'rewards/margins': 0.00808860082179308, 'logps/rejected': -60.583866119384766, 'logps/chosen': -60.12287139892578, 'logits/rejected': -36.95164489746094, 'logits/chosen': -36.77421951293945, 'epoch': 1.08}\r\n",
      "{'loss': 0.685, 'grad_norm': 1.117501974105835, 'learning_rate': 2e-06, 'rewards/chosen': -0.0025914213620126247, 'rewards/rejected': -0.019602209329605103, 'rewards/accuracies': 0.6360937356948853, 'rewards/margins': 0.01701078936457634, 'logps/rejected': -60.74211502075195, 'logps/chosen': -60.075592041015625, 'logits/rejected': -36.74028396606445, 'logits/chosen': -36.41789245605469, 'epoch': 1.35}\r\n",
      "{'loss': 0.6836, 'grad_norm': 1.1130421161651611, 'learning_rate': 2e-06, 'rewards/chosen': -0.009017156437039375, 'rewards/rejected': -0.02913457341492176, 'rewards/accuracies': 0.6267187595367432, 'rewards/margins': 0.020117416977882385, 'logps/rejected': -60.97117233276367, 'logps/chosen': -60.223724365234375, 'logits/rejected': -36.737518310546875, 'logits/chosen': -36.243125915527344, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:17<09:06,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.81it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.80it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.81it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.82it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6864760518074036, 'eval_runtime': 5.5834, 'eval_samples_per_second': 222.984, 'eval_steps_per_second': 1.791, 'eval_rewards/chosen': -0.013554951176047325, 'eval_rewards/rejected': -0.028342386707663536, 'eval_rewards/accuracies': 0.5821908712387085, 'eval_rewards/margins': 0.014787433668971062, 'eval_logps/rejected': -61.41807174682617, 'eval_logps/chosen': -60.727081298828125, 'eval_logits/rejected': -36.15749740600586, 'eval_logits/chosen': -35.67258071899414, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:22<09:06,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.01it/s]\u001b[A\r\n",
      "{'loss': 0.6829, 'grad_norm': 1.0837911367416382, 'learning_rate': 2e-06, 'rewards/chosen': -0.01742737554013729, 'rewards/rejected': -0.039671774953603745, 'rewards/accuracies': 0.6075000166893005, 'rewards/margins': 0.022244401276111603, 'logps/rejected': -61.48102951049805, 'logps/chosen': -60.440277099609375, 'logits/rejected': -36.2435417175293, 'logits/chosen': -35.88984298706055, 'epoch': 1.89}\r\n",
      "{'loss': 0.6771, 'grad_norm': 1.0521961450576782, 'learning_rate': 2e-06, 'rewards/chosen': -0.0231283288449049, 'rewards/rejected': -0.05762190744280815, 'rewards/accuracies': 0.6356568932533264, 'rewards/margins': 0.0344935767352581, 'logps/rejected': -61.63333511352539, 'logps/chosen': -60.63865280151367, 'logits/rejected': -36.31248474121094, 'logits/chosen': -35.801910400390625, 'epoch': 2.16}\r\n",
      "{'loss': 0.6732, 'grad_norm': 1.1342365741729736, 'learning_rate': 2e-06, 'rewards/chosen': -0.030218910425901413, 'rewards/rejected': -0.0735790804028511, 'rewards/accuracies': 0.6526562571525574, 'rewards/margins': 0.04336017742753029, 'logps/rejected': -61.951358795166016, 'logps/chosen': -60.89726638793945, 'logits/rejected': -36.139583587646484, 'logits/chosen': -35.888893127441406, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:29<05:59,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.74it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.75it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6825454235076904, 'eval_runtime': 5.6533, 'eval_samples_per_second': 220.227, 'eval_steps_per_second': 1.769, 'eval_rewards/chosen': -0.04272085800766945, 'eval_rewards/rejected': -0.06765575706958771, 'eval_rewards/accuracies': 0.5855090618133545, 'eval_rewards/margins': 0.02493489533662796, 'eval_logps/rejected': -62.2043342590332, 'eval_logps/chosen': -61.310401916503906, 'eval_logits/rejected': -35.8151741027832, 'eval_logits/chosen': -35.351356506347656, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:34<05:59,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6733, 'grad_norm': 1.0605523586273193, 'learning_rate': 2e-06, 'rewards/chosen': -0.0443456657230854, 'rewards/rejected': -0.08857870101928711, 'rewards/accuracies': 0.6354687213897705, 'rewards/margins': 0.044233039021492004, 'logps/rejected': -62.360939025878906, 'logps/chosen': -61.189353942871094, 'logits/rejected': -36.102848052978516, 'logits/chosen': -35.623233795166016, 'epoch': 2.7}\r\n",
      "{'loss': 0.6721, 'grad_norm': 1.1727267503738403, 'learning_rate': 2e-06, 'rewards/chosen': -0.05518574267625809, 'rewards/rejected': -0.10264504700899124, 'rewards/accuracies': 0.6364062428474426, 'rewards/margins': 0.047459300607442856, 'logps/rejected': -62.46845626831055, 'logps/chosen': -61.23048782348633, 'logits/rejected': -35.7228889465332, 'logits/chosen': -35.32160949707031, 'epoch': 2.97}\r\n",
      "{'loss': 0.6658, 'grad_norm': 1.0953052043914795, 'learning_rate': 2e-06, 'rewards/chosen': -0.06370145827531815, 'rewards/rejected': -0.12534748017787933, 'rewards/accuracies': 0.6510554552078247, 'rewards/margins': 0.06164601817727089, 'logps/rejected': -62.992713928222656, 'logps/chosen': -61.28923797607422, 'logits/rejected': -35.58164596557617, 'logits/chosen': -35.131595611572266, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:42<02:56,  1.26s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.01it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.94it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.85it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.85it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6797346472740173, 'eval_runtime': 5.4354, 'eval_samples_per_second': 229.053, 'eval_steps_per_second': 1.84, 'eval_rewards/chosen': -0.07966987788677216, 'eval_rewards/rejected': -0.11326553672552109, 'eval_rewards/accuracies': 0.5808215737342834, 'eval_rewards/margins': 0.03359567001461983, 'eval_logps/rejected': -63.11653518676758, 'eval_logps/chosen': -62.04938507080078, 'eval_logits/rejected': -35.35832977294922, 'eval_logits/chosen': -34.92192459106445, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:47<02:56,  1.26s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.03it/s]\u001b[A\r\n",
      "{'loss': 0.6632, 'grad_norm': 1.109884262084961, 'learning_rate': 2e-06, 'rewards/chosen': -0.07677087932825089, 'rewards/rejected': -0.1446985900402069, 'rewards/accuracies': 0.6581249833106995, 'rewards/margins': 0.06792772561311722, 'logps/rejected': -63.3596076965332, 'logps/chosen': -62.054195404052734, 'logits/rejected': -35.472923278808594, 'logits/chosen': -35.16044998168945, 'epoch': 3.51}\r\n",
      "{'loss': 0.6619, 'grad_norm': 1.1139402389526367, 'learning_rate': 2e-06, 'rewards/chosen': -0.09011535346508026, 'rewards/rejected': -0.16211462020874023, 'rewards/accuracies': 0.649218738079071, 'rewards/margins': 0.07199927419424057, 'logps/rejected': -63.84601593017578, 'logps/chosen': -61.810951232910156, 'logits/rejected': -35.437957763671875, 'logits/chosen': -35.08317184448242, 'epoch': 3.78}\r\n",
      "{'train_runtime': 958.9998, 'train_samples_per_second': 98.644, 'train_steps_per_second': 0.772, 'train_loss': 0.6778821139722258, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:42<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14/checkpoint-740\r\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.81k/7.81k [00:00<00:00, 12.3MB/s]\r\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0M/21.0M [00:00<00:00, 60.1MB/s]\r\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 86.9MB/s]\r\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.0M/42.0M [00:00<00:00, 138MB/s]\r\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 25000/25000 [00:00<00:00, 100117.54 examples/s]\r\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 112094.66 examples/s]\r\n",
      "Generating unsupervised split: 100%|â–ˆ| 50000/50000 [00:00<00:00, 105572.38 examp\r\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 36135.36 examples/s]\r\n",
      "Map:   0%|                                       | 0/250 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\r\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 684.77 examples/s]\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 687/687 [00:00<00:00, 2.03MB/s]\r\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [00:25<00:00, 54.8MB/s]\r\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 747kB/s]\r\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 11.7MB/s]\r\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 47.6MB/s]\r\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 494kB/s]\r\n",
      "eval batch size 256\r\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [00:00<00:00, 118611.83 examples/s]\r\n",
      "Map:   3%|â–Š                           | 72/2487 [00:00<00:03, 672.88 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\r\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2487/2487 [00:03<00:00, 715.67 examples/s]\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:57, 29.66s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:58<03:23, 29.14s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:27<02:54, 29.16s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:57<02:26, 29.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:27<01:59, 29.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:57<01:29, 29.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:27<00:59, 29.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:57<00:29, 29.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:27<00:00, 29.73s/it]\r\n",
      "mean test reward 0.7661769605870935 +/- 0.008714289500502702 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.998217761516571 from 0.00922983093187213\r\n",
      "mean KL 1.7413191013749585 +/- 0.06588410671872932 full 2.974493874418032 +/- 0.021815128433738822\r\n",
      "median KL 2.1313971281051636 full 2.831762433052063\r\n",
      "Loading from dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:59, 29.88s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:59<03:27, 29.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:28<02:57, 29.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:58<02:27, 29.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:28<01:59, 29.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:58<01:29, 29.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:28<00:59, 29.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:59<00:30, 30.08s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:28<00:00, 29.81s/it]\r\n",
      "mean test reward 0.7257222745498666 +/- 0.0091788287584752 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9977959394454956 from 0.00922983093187213\r\n",
      "mean KL 1.4258489794511762 +/- 0.04745869820042151 full 1.8436828668798424 +/- 0.014651722122155661\r\n",
      "median KL 1.6173174381256104 full 1.729195237159729\r\n",
      "Loading from dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:49, 28.69s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:57<03:22, 28.88s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:26<02:54, 29.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:56<02:25, 29.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:25<01:56, 29.20s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:54<01:27, 29.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:24<00:58, 29.27s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:53<00:29, 29.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:23<00:00, 29.23s/it]\r\n",
      "mean test reward 0.6416725419580618 +/- 0.00988050984365165 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9958224892616272 from 0.00922983093187213\r\n",
      "mean KL 0.8501047608798318 +/- 0.030065627940233625 full 0.8548366109995792 +/- 0.007792076895977422\r\n",
      "median KL 0.9552590847015381 full 0.7781958878040314\r\n",
      "dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14,0.05,epoch 4,0.7661769605870935,0.008714289500502702,2.974493874418032,0.021815128433738822\r\n",
      "dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14,0.05,epoch 3,0.7257222745498666,0.0091788287584752,1.8436828668798424,0.014651722122155661\r\n",
      "dpo_flipped_2c2_0_2-0_05-2024.06.01.12.14,0.05,epoch 2,0.6416725419580618,0.00988050984365165,0.8548366109995792,0.007792076895977422\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_flipped_2c2_0_2-0_05 --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_flipped_0_2_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5576a110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T12:44:55.460058Z",
     "iopub.status.busy": "2024-06-01T12:44:55.459046Z",
     "iopub.status.idle": "2024-06-01T13:14:28.670617Z",
     "shell.execute_reply": "2024-06-01T13:14:28.669620Z"
    },
    "papermill": {
     "duration": 1773.337585,
     "end_time": "2024-06-01T13:14:28.673085",
     "exception": false,
     "start_time": "2024-06-01T12:44:55.335500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 12:45:02.808465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 12:45:02.808539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 12:45:02.810184: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_124509-pekqzl0q\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_flipped_2c2_-0_3-0_05\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/pekqzl0q\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.1712502241134644, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -61.61871337890625, 'logps/chosen': -61.97871017456055, 'logits/rejected': -36.16529083251953, 'logits/chosen': -35.40785598754883, 'epoch': 0.01}\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.0983729362487793, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 2.9188331609475426e-06, 'rewards/rejected': -1.627998244657647e-05, 'rewards/accuracies': 0.5007972121238708, 'rewards/margins': 1.9198816517018713e-05, 'logps/rejected': -60.29901123046875, 'logps/chosen': -60.588069915771484, 'logits/rejected': -36.86351013183594, 'logits/chosen': -36.95051956176758, 'epoch': 0.27}\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.0881474018096924, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 3.397507680347189e-05, 'rewards/rejected': -0.00011488958989502862, 'rewards/accuracies': 0.515625, 'rewards/margins': 0.00014886468125041574, 'logps/rejected': -60.168460845947266, 'logps/chosen': -60.449745178222656, 'logits/rejected': -37.07706832885742, 'logits/chosen': -36.86819839477539, 'epoch': 0.54}\r\n",
      "{'loss': 0.693, 'grad_norm': 1.0911195278167725, 'learning_rate': 2e-06, 'rewards/chosen': -3.387867764104158e-05, 'rewards/rejected': -0.0003956786822527647, 'rewards/accuracies': 0.520312488079071, 'rewards/margins': 0.00036180001916363835, 'logps/rejected': -60.367496490478516, 'logps/chosen': -60.351070404052734, 'logits/rejected': -36.915096282958984, 'logits/chosen': -36.6646614074707, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:05<12:07,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.69it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.73it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.75it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.77it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.79it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6929122805595398, 'eval_runtime': 5.6909, 'eval_samples_per_second': 218.772, 'eval_steps_per_second': 1.757, 'eval_rewards/chosen': -0.00010788835061248392, 'eval_rewards/rejected': -0.000601164938416332, 'eval_rewards/accuracies': 0.5269993543624878, 'eval_rewards/margins': 0.0004932766896672547, 'eval_logps/rejected': -60.6945915222168, 'eval_logps/chosen': -60.626800537109375, 'eval_logits/rejected': -36.731719970703125, 'eval_logits/chosen': -36.30280303955078, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:11<12:07,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.98it/s]\u001b[A\r\n",
      "{'loss': 0.6916, 'grad_norm': 1.0525068044662476, 'learning_rate': 2e-06, 'rewards/chosen': 0.0007811830146238208, 'rewards/rejected': -0.0023647279012948275, 'rewards/accuracies': 0.5917538404464722, 'rewards/margins': 0.0031459107995033264, 'logps/rejected': -60.170162200927734, 'logps/chosen': -60.346012115478516, 'logits/rejected': -37.233089447021484, 'logits/chosen': -36.87252426147461, 'epoch': 1.08}\r\n",
      "{'loss': 0.6888, 'grad_norm': 1.1015777587890625, 'learning_rate': 2e-06, 'rewards/chosen': 0.002713561989367008, 'rewards/rejected': -0.006027615163475275, 'rewards/accuracies': 0.6884375214576721, 'rewards/margins': 0.008741176687180996, 'logps/rejected': -60.252899169921875, 'logps/chosen': -60.18721008300781, 'logits/rejected': -36.946006774902344, 'logits/chosen': -36.79317855834961, 'epoch': 1.35}\r\n",
      "{'loss': 0.6886, 'grad_norm': 1.156522274017334, 'learning_rate': 2e-06, 'rewards/chosen': 0.0018140589818358421, 'rewards/rejected': -0.007419083267450333, 'rewards/accuracies': 0.6540625095367432, 'rewards/margins': 0.009233142249286175, 'logps/rejected': -60.259422302246094, 'logps/chosen': -60.2845458984375, 'logits/rejected': -36.89046859741211, 'logits/chosen': -36.850364685058594, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:16<09:01,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.74it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.76it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.78it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.79it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6925116181373596, 'eval_runtime': 5.6404, 'eval_samples_per_second': 220.727, 'eval_steps_per_second': 1.773, 'eval_rewards/chosen': -0.0020856542978435755, 'eval_rewards/rejected': -0.0035469401627779007, 'eval_rewards/accuracies': 0.5214381814002991, 'eval_rewards/margins': 0.0014612855156883597, 'eval_logps/rejected': -60.753501892089844, 'eval_logps/chosen': -60.666351318359375, 'eval_logits/rejected': -36.65367889404297, 'eval_logits/chosen': -36.22794723510742, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:22<09:01,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.6883, 'grad_norm': 1.1021479368209839, 'learning_rate': 2e-06, 'rewards/chosen': 0.0011872285977005959, 'rewards/rejected': -0.008725035935640335, 'rewards/accuracies': 0.6446874737739563, 'rewards/margins': 0.009912265464663506, 'logps/rejected': -60.163787841796875, 'logps/chosen': -60.766292572021484, 'logits/rejected': -36.75175857543945, 'logits/chosen': -36.58457565307617, 'epoch': 1.89}\r\n",
      "{'loss': 0.6845, 'grad_norm': 1.1121363639831543, 'learning_rate': 2e-06, 'rewards/chosen': 0.002908719703555107, 'rewards/rejected': -0.014658868312835693, 'rewards/accuracies': 0.70121169090271, 'rewards/margins': 0.01756758615374565, 'logps/rejected': -60.53810501098633, 'logps/chosen': -60.353878021240234, 'logits/rejected': -37.000694274902344, 'logits/chosen': -36.736297607421875, 'epoch': 2.16}\r\n",
      "{'loss': 0.6822, 'grad_norm': 1.1174278259277344, 'learning_rate': 2e-06, 'rewards/chosen': 0.003279047319665551, 'rewards/rejected': -0.01913948729634285, 'rewards/accuracies': 0.7235937714576721, 'rewards/margins': 0.022418536245822906, 'logps/rejected': -60.76860427856445, 'logps/chosen': -60.32126998901367, 'logits/rejected': -37.03313064575195, 'logits/chosen': -36.83076095581055, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:28<06:00,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.71it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6921399831771851, 'eval_runtime': 5.662, 'eval_samples_per_second': 219.888, 'eval_steps_per_second': 1.766, 'eval_rewards/chosen': -0.007043672259896994, 'eval_rewards/rejected': -0.009558023884892464, 'eval_rewards/accuracies': 0.5274949669837952, 'eval_rewards/margins': 0.0025143525563180447, 'eval_logps/rejected': -60.87372970581055, 'eval_logps/chosen': -60.76552200317383, 'eval_logits/rejected': -36.868431091308594, 'eval_logits/chosen': -36.44834518432617, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:33<06:00,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6822, 'grad_norm': 1.067421793937683, 'learning_rate': 2e-06, 'rewards/chosen': 0.0015154099091887474, 'rewards/rejected': -0.021059922873973846, 'rewards/accuracies': 0.6989062428474426, 'rewards/margins': 0.02257533371448517, 'logps/rejected': -60.91887664794922, 'logps/chosen': -60.36382293701172, 'logits/rejected': -37.057472229003906, 'logits/chosen': -37.073097229003906, 'epoch': 2.7}\r\n",
      "{'loss': 0.6809, 'grad_norm': 1.131759524345398, 'learning_rate': 2e-06, 'rewards/chosen': 0.000613137090113014, 'rewards/rejected': -0.024730468168854713, 'rewards/accuracies': 0.7004687786102295, 'rewards/margins': 0.025343604385852814, 'logps/rejected': -60.558799743652344, 'logps/chosen': -60.46586990356445, 'logits/rejected': -36.95771789550781, 'logits/chosen': -36.8128776550293, 'epoch': 2.97}\r\n",
      "{'loss': 0.6754, 'grad_norm': 1.1740663051605225, 'learning_rate': 2e-06, 'rewards/chosen': 0.0019097088370472193, 'rewards/rejected': -0.03470831736922264, 'rewards/accuracies': 0.7434343099594116, 'rewards/margins': 0.036618031561374664, 'logps/rejected': -60.790245056152344, 'logps/chosen': -60.3666877746582, 'logits/rejected': -36.72328186035156, 'logits/chosen': -36.7540168762207, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:40<02:56,  1.26s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6917726397514343, 'eval_runtime': 5.3904, 'eval_samples_per_second': 230.967, 'eval_steps_per_second': 1.855, 'eval_rewards/chosen': -0.016015175729990005, 'eval_rewards/rejected': -0.019803486764431, 'eval_rewards/accuracies': 0.5260332822799683, 'eval_rewards/margins': 0.0037883080076426268, 'eval_logps/rejected': -61.07863235473633, 'eval_logps/chosen': -60.9449462890625, 'eval_logits/rejected': -36.881614685058594, 'eval_logits/chosen': -36.4417839050293, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:46<02:56,  1.26s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\r\n",
      "{'loss': 0.6745, 'grad_norm': 1.2049251794815063, 'learning_rate': 2e-06, 'rewards/chosen': -0.0025641187094151974, 'rewards/rejected': -0.0413549579679966, 'rewards/accuracies': 0.7260937690734863, 'rewards/margins': 0.03879083693027496, 'logps/rejected': -61.29720687866211, 'logps/chosen': -60.565589904785156, 'logits/rejected': -37.14952850341797, 'logits/chosen': -36.81204605102539, 'epoch': 3.51}\r\n",
      "{'loss': 0.6746, 'grad_norm': 1.0812093019485474, 'learning_rate': 2e-06, 'rewards/chosen': -0.006591366603970528, 'rewards/rejected': -0.045427724719047546, 'rewards/accuracies': 0.7085937261581421, 'rewards/margins': 0.03883635625243187, 'logps/rejected': -60.947811126708984, 'logps/chosen': -60.70493698120117, 'logits/rejected': -37.248504638671875, 'logits/chosen': -37.07756042480469, 'epoch': 3.78}\r\n",
      "{'train_runtime': 958.1262, 'train_samples_per_second': 98.734, 'train_steps_per_second': 0.772, 'train_loss': 0.6844832362355413, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:41<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:45, 28.20s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:56<03:18, 28.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:24<02:49, 28.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:53<02:21, 28.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:21<01:53, 28.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:50<01:25, 28.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:19<00:57, 28.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:47<00:28, 28.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:16<00:00, 28.49s/it]\r\n",
      "mean test reward 0.5602701643139072 +/- 0.010213938495217121 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9904680848121643 from 0.00922983093187213\r\n",
      "mean KL -1.0675635624275956 +/- 0.04177055964464879 full 0.715887435246259 +/- 0.0056490653074659026\r\n",
      "median KL -0.914438396692276 full 0.6777804493904114\r\n",
      "Loading from dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:49, 28.75s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:57<03:21, 28.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:26<02:54, 29.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:56<02:26, 29.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:25<01:56, 29.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:55<01:28, 29.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:25<00:59, 29.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:55<00:29, 29.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:24<00:00, 29.39s/it]\r\n",
      "mean test reward 0.5282357540550543 +/- 0.010272421040122317 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9825810194015503 from 0.00922983093187213\r\n",
      "mean KL -0.6905454346512366 +/- 0.02788015370631609 full 0.3505739253887441 +/- 0.00306107682264754\r\n",
      "median KL -0.6022920310497284 full 0.3259420543909073\r\n",
      "Loading from dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:45, 28.15s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:56<03:16, 28.14s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:24<02:48, 28.14s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:52<02:20, 28.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:21<01:53, 28.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:49<01:25, 28.35s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:18<00:56, 28.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:46<00:28, 28.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:15<00:00, 28.35s/it]\r\n",
      "mean test reward 0.5033583303661291 +/- 0.010287551200906396 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.8153786659240723 from 0.00922983093187213\r\n",
      "mean KL -0.4470245530085069 +/- 0.016013176978005132 full 0.12935075949159605 +/- 0.0011971071374094356\r\n",
      "median KL -0.4061923921108246 full 0.119040347635746\r\n",
      "dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45,0.05,epoch 4,0.5602701643139072,0.010213938495217121,0.715887435246259,0.0056490653074659026\r\n",
      "dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45,0.05,epoch 3,0.5282357540550543,0.010272421040122317,0.3505739253887441,0.00306107682264754\r\n",
      "dpo_flipped_2c2_-0_3-0_05-2024.06.01.12.45,0.05,epoch 2,0.5033583303661291,0.010287551200906396,0.12935075949159605,0.0011971071374094356\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_flipped_2c2_-0_3-0_05 --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_flipped_-0_3_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cef7288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T13:14:29.043116Z",
     "iopub.status.busy": "2024-06-01T13:14:29.042571Z",
     "iopub.status.idle": "2024-06-01T13:44:29.101520Z",
     "shell.execute_reply": "2024-06-01T13:44:29.100168Z"
    },
    "papermill": {
     "duration": 1800.246318,
     "end_time": "2024-06-01T13:44:29.104273",
     "exception": false,
     "start_time": "2024-06-01T13:14:28.857955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 13:14:36.206463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 13:14:36.206519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 13:14:36.208113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_131443-7xq72zrc\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_flipped_2c2_-0_3-0_2\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/7xq72zrc\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.685000896453857, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -61.61871337890625, 'logps/chosen': -61.97871017456055, 'logits/rejected': -36.16529083251953, 'logits/chosen': -35.40785598754883, 'epoch': 0.01}\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.392945289611816, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 1.1745898518711329e-05, 'rewards/rejected': -6.500962626887485e-05, 'rewards/accuracies': 0.49904337525367737, 'rewards/margins': 7.675552478758618e-05, 'logps/rejected': -60.29901123046875, 'logps/chosen': -60.58805847167969, 'logits/rejected': -36.86349105834961, 'logits/chosen': -36.950496673583984, 'epoch': 0.27}\r\n",
      "{'loss': 0.6929, 'grad_norm': 4.353118896484375, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.00014519206888508052, 'rewards/rejected': -0.00044729001820087433, 'rewards/accuracies': 0.5145312547683716, 'rewards/margins': 0.0005924820434302092, 'logps/rejected': -60.168399810791016, 'logps/chosen': -60.449703216552734, 'logits/rejected': -37.0772819519043, 'logits/chosen': -36.868408203125, 'epoch': 0.54}\r\n",
      "{'loss': 0.6925, 'grad_norm': 4.3571343421936035, 'learning_rate': 2e-06, 'rewards/chosen': -7.057056063786149e-05, 'rewards/rejected': -0.00146590662188828, 'rewards/accuracies': 0.5228124856948853, 'rewards/margins': 0.0013953357702121139, 'logps/rejected': -60.36690902709961, 'logps/chosen': -60.350738525390625, 'logits/rejected': -36.92433547973633, 'logits/chosen': -36.673797607421875, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:05<12:11,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.69it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.73it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.77it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6923402547836304, 'eval_runtime': 5.685, 'eval_samples_per_second': 218.998, 'eval_steps_per_second': 1.759, 'eval_rewards/chosen': -0.00035092647885903716, 'eval_rewards/rejected': -0.002232338534668088, 'eval_rewards/accuracies': 0.5271925330162048, 'eval_rewards/margins': 0.0018814122304320335, 'eval_logps/rejected': -60.6937255859375, 'eval_logps/chosen': -60.62639617919922, 'eval_logits/rejected': -36.75013732910156, 'eval_logits/chosen': -36.32123565673828, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:10<12:11,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6873, 'grad_norm': 4.146123886108398, 'learning_rate': 2e-06, 'rewards/chosen': 0.0034683295525610447, 'rewards/rejected': -0.00890113040804863, 'rewards/accuracies': 0.5961288213729858, 'rewards/margins': 0.012369461357593536, 'logps/rejected': -60.1673698425293, 'logps/chosen': -60.344295501708984, 'logits/rejected': -37.256839752197266, 'logits/chosen': -36.896141052246094, 'epoch': 1.08}\r\n",
      "{'loss': 0.6767, 'grad_norm': 4.356448173522949, 'learning_rate': 2e-06, 'rewards/chosen': 0.011925973929464817, 'rewards/rejected': -0.02237672172486782, 'rewards/accuracies': 0.7043750286102295, 'rewards/margins': 0.03430269658565521, 'logps/rejected': -60.24422836303711, 'logps/chosen': -60.181854248046875, 'logits/rejected': -36.986934661865234, 'logits/chosen': -36.83366775512695, 'epoch': 1.35}\r\n",
      "{'loss': 0.6762, 'grad_norm': 4.56610107421875, 'learning_rate': 2e-06, 'rewards/chosen': 0.009843419305980206, 'rewards/rejected': -0.02591121755540371, 'rewards/accuracies': 0.6764062643051147, 'rewards/margins': 0.03575463593006134, 'logps/rejected': -60.240596771240234, 'logps/chosen': -60.2716178894043, 'logits/rejected': -36.96249008178711, 'logits/chosen': -36.9219970703125, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:16<09:02,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.74it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.76it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6914961934089661, 'eval_runtime': 5.6498, 'eval_samples_per_second': 220.363, 'eval_steps_per_second': 1.77, 'eval_rewards/chosen': -0.005150116980075836, 'eval_rewards/rejected': -0.010147815570235252, 'eval_rewards/accuracies': 0.51948082447052, 'eval_rewards/margins': 0.004997698124498129, 'eval_logps/rejected': -60.733299255371094, 'eval_logps/chosen': -60.65039825439453, 'eval_logits/rejected': -36.741111755371094, 'eval_logits/chosen': -36.31611633300781, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:22<09:02,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6754, 'grad_norm': 4.345877170562744, 'learning_rate': 2e-06, 'rewards/chosen': 0.009070206433534622, 'rewards/rejected': -0.02874012291431427, 'rewards/accuracies': 0.6656249761581421, 'rewards/margins': 0.037810325622558594, 'logps/rejected': -60.13298797607422, 'logps/chosen': -60.744693756103516, 'logits/rejected': -36.846588134765625, 'logits/chosen': -36.67866897583008, 'epoch': 1.89}\r\n",
      "{'loss': 0.6616, 'grad_norm': 4.297374248504639, 'learning_rate': 2e-06, 'rewards/chosen': 0.019053440541028976, 'rewards/rejected': -0.04810813069343567, 'rewards/accuracies': 0.7319929599761963, 'rewards/margins': 0.06716156005859375, 'logps/rejected': -60.4854736328125, 'logps/chosen': -60.3167839050293, 'logits/rejected': -37.10636901855469, 'logits/chosen': -36.84000015258789, 'epoch': 2.16}\r\n",
      "{'loss': 0.6526, 'grad_norm': 4.292185306549072, 'learning_rate': 2e-06, 'rewards/chosen': 0.024721279740333557, 'rewards/rejected': -0.061903439462184906, 'rewards/accuracies': 0.7637500166893005, 'rewards/margins': 0.08662471175193787, 'logps/rejected': -60.69532775878906, 'logps/chosen': -60.263240814208984, 'logits/rejected': -37.1422119140625, 'logits/chosen': -36.937923431396484, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:28<06:03,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.70it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.77it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6908837556838989, 'eval_runtime': 5.6729, 'eval_samples_per_second': 219.463, 'eval_steps_per_second': 1.763, 'eval_rewards/chosen': -0.014585909433662891, 'eval_rewards/rejected': -0.022882049903273582, 'eval_rewards/accuracies': 0.5363827347755432, 'eval_rewards/margins': 0.008296141400933266, 'eval_logps/rejected': -60.79697799682617, 'eval_logps/chosen': -60.69757843017578, 'eval_logits/rejected': -36.98212432861328, 'eval_logits/chosen': -36.56254959106445, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:34<06:03,  1.25s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6533, 'grad_norm': 4.167051315307617, 'learning_rate': 2e-06, 'rewards/chosen': 0.021677272394299507, 'rewards/rejected': -0.06409485638141632, 'rewards/accuracies': 0.7475000023841858, 'rewards/margins': 0.08577213436365128, 'logps/rejected': -60.818145751953125, 'logps/chosen': -60.285743713378906, 'logits/rejected': -37.17176818847656, 'logits/chosen': -37.18376159667969, 'epoch': 2.7}\r\n",
      "{'loss': 0.6499, 'grad_norm': 4.421957969665527, 'learning_rate': 2e-06, 'rewards/chosen': 0.022808194160461426, 'rewards/rejected': -0.0708865150809288, 'rewards/accuracies': 0.7478125095367432, 'rewards/margins': 0.09369469434022903, 'logps/rejected': -60.41862487792969, 'logps/chosen': -60.364097595214844, 'logits/rejected': -37.065895080566406, 'logits/chosen': -36.91688537597656, 'epoch': 2.97}\r\n",
      "{'loss': 0.6304, 'grad_norm': 4.401209831237793, 'learning_rate': 2e-06, 'rewards/chosen': 0.036490436643362045, 'rewards/rejected': -0.10011932253837585, 'rewards/accuracies': 0.8100669980049133, 'rewards/margins': 0.1366097629070282, 'logps/rejected': -60.5966796875, 'logps/chosen': -60.22243118286133, 'logits/rejected': -36.84628677368164, 'logits/chosen': -36.87432861328125, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:41<02:55,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.02it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.94it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.89it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6904966235160828, 'eval_runtime': 5.4179, 'eval_samples_per_second': 229.792, 'eval_steps_per_second': 1.846, 'eval_rewards/chosen': -0.027487099170684814, 'eval_rewards/rejected': -0.03956208378076553, 'eval_rewards/accuracies': 0.5324764847755432, 'eval_rewards/margins': 0.012074986472725868, 'eval_logps/rejected': -60.88037872314453, 'eval_logps/chosen': -60.76207733154297, 'eval_logits/rejected': -37.007320404052734, 'eval_logits/chosen': -36.57276153564453, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:46<02:55,  1.25s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\r\n",
      "{'loss': 0.628, 'grad_norm': 4.56644868850708, 'learning_rate': 2e-06, 'rewards/chosen': 0.030551666393876076, 'rewards/rejected': -0.11250421404838562, 'rewards/accuracies': 0.7928125262260437, 'rewards/margins': 0.14305587112903595, 'logps/rejected': -61.03262710571289, 'logps/chosen': -60.361549377441406, 'logits/rejected': -37.29267501831055, 'logits/chosen': -36.95169448852539, 'epoch': 3.51}\r\n",
      "{'loss': 0.6282, 'grad_norm': 4.0460381507873535, 'learning_rate': 2e-06, 'rewards/chosen': 0.026730280369520187, 'rewards/rejected': -0.11694227904081345, 'rewards/accuracies': 0.7762500047683716, 'rewards/margins': 0.14367258548736572, 'logps/rejected': -60.62397384643555, 'logps/chosen': -60.439453125, 'logits/rejected': -37.39628982543945, 'logits/chosen': -37.21714401245117, 'epoch': 3.78}\r\n",
      "{'train_runtime': 958.8321, 'train_samples_per_second': 98.662, 'train_steps_per_second': 0.772, 'train_loss': 0.6622407597464484, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:42<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:01, 30.24s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:59<03:28, 29.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:29<02:59, 29.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:59<02:28, 29.66s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:28<01:58, 29.70s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:58<01:29, 29.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:28<00:59, 29.88s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:58<00:29, 29.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:28<00:00, 29.79s/it]\r\n",
      "mean test reward 0.4968686932150932 +/- 0.010293482630132933 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.3236646354198456 from 0.00922983093187213\r\n",
      "mean KL -0.939700461590999 +/- 0.029733676525888472 full 0.293319082261102 +/- 0.002214184461070754\r\n",
      "median KL -0.8297414779663086 full 0.27796635031700134\r\n",
      "Loading from dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:47, 28.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:57<03:19, 28.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:25<02:50, 28.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:54<02:23, 28.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:22<01:54, 28.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:51<01:25, 28.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:20<00:57, 28.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:49<00:28, 28.79s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:18<00:00, 28.76s/it]\r\n",
      "mean test reward 0.49584323444969414 +/- 0.010292922992006124 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.15672572702169418 from 0.00922983093187213\r\n",
      "mean KL -0.6372857878417967 +/- 0.02146111735005149 full 0.17315727322582258 +/- 0.0014178344243918923\r\n",
      "median KL -0.5574531853199005 full 0.16181768476963043\r\n",
      "Loading from dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:51, 28.96s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:57<03:22, 28.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:27<02:55, 29.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:56<02:26, 29.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:26<01:58, 29.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:57<01:29, 29.78s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:27<01:00, 30.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:58<00:30, 30.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:29<00:00, 29.92s/it]\r\n",
      "mean test reward 0.4935965295593026 +/- 0.010288865948301206 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.07053884863853455 from 0.00922983093187213\r\n",
      "mean KL -0.38202246122333844 +/- 0.012807959626564653 full 0.08043054404576348 +/- 0.000701268679628763\r\n",
      "median KL -0.34590113162994385 full 0.07444209605455399\r\n",
      "dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14,0.2,epoch 4,0.4968686932150932,0.010293482630132933,0.293319082261102,0.002214184461070754\r\n",
      "dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14,0.2,epoch 3,0.49584323444969414,0.010292922992006124,0.17315727322582258,0.0014178344243918923\r\n",
      "dpo_flipped_2c2_-0_3-0_2-2024.06.01.13.14,0.2,epoch 2,0.4935965295593026,0.010288865948301206,0.08043054404576348,0.000701268679628763\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_flipped_2c2_-0_3-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_flipped_-0_3_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f62aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T13:44:29.678747Z",
     "iopub.status.busy": "2024-06-01T13:44:29.677668Z",
     "iopub.status.idle": "2024-06-01T14:14:57.548055Z",
     "shell.execute_reply": "2024-06-01T14:14:57.546726Z"
    },
    "papermill": {
     "duration": 1828.157796,
     "end_time": "2024-06-01T14:14:57.550611",
     "exception": false,
     "start_time": "2024-06-01T13:44:29.392815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 13:44:37.851626: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 13:44:37.851694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 13:44:37.853672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_134445-17w1mdi0\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_uniform_flipped_2c2_0_25-0_2\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/17w1mdi0\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.317891597747803, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -62.52097702026367, 'logps/chosen': -61.076446533203125, 'logits/rejected': -35.7431755065918, 'logits/chosen': -35.82997131347656, 'epoch': 0.01}\r\n",
      "{'loss': 0.6927, 'grad_norm': 4.472739219665527, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.0003502444887999445, 'rewards/rejected': -0.0005305991508066654, 'rewards/accuracies': 0.5699936151504517, 'rewards/margins': 0.0008808436105027795, 'logps/rejected': -60.39039611816406, 'logps/chosen': -60.497314453125, 'logits/rejected': -37.19399642944336, 'logits/chosen': -36.57442092895508, 'epoch': 0.27}\r\n",
      "{'loss': 0.6901, 'grad_norm': 4.426309585571289, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0017172913067042828, 'rewards/rejected': -0.004599742591381073, 'rewards/accuracies': 0.5896875262260437, 'rewards/margins': 0.006317033898085356, 'logps/rejected': -60.35283660888672, 'logps/chosen': -60.278160095214844, 'logits/rejected': -37.1427116394043, 'logits/chosen': -36.69151306152344, 'epoch': 0.54}\r\n",
      "{'loss': 0.685, 'grad_norm': 4.5115509033203125, 'learning_rate': 2e-06, 'rewards/chosen': 0.001548676285892725, 'rewards/rejected': -0.016277357935905457, 'rewards/accuracies': 0.5959374904632568, 'rewards/margins': 0.017826035618782043, 'logps/rejected': -60.54082107543945, 'logps/chosen': -60.242794036865234, 'logits/rejected': -36.78044128417969, 'logits/chosen': -36.5268669128418, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:05<12:10,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.69it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.72it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.75it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.76it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.79it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6840755343437195, 'eval_runtime': 5.7149, 'eval_samples_per_second': 217.853, 'eval_steps_per_second': 1.75, 'eval_rewards/chosen': -9.506921924185008e-05, 'eval_rewards/rejected': -0.02087300829589367, 'eval_rewards/accuracies': 0.5852150321006775, 'eval_rewards/margins': 0.020777937024831772, 'eval_logps/rejected': -61.26007843017578, 'eval_logps/chosen': -60.1519775390625, 'eval_logits/rejected': -36.499656677246094, 'eval_logits/chosen': -36.188602447509766, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:11<12:10,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.98it/s]\u001b[A\r\n",
      "{'loss': 0.6745, 'grad_norm': 4.645379066467285, 'learning_rate': 2e-06, 'rewards/chosen': -0.00460634334012866, 'rewards/rejected': -0.046985410153865814, 'rewards/accuracies': 0.6255484819412231, 'rewards/margins': 0.04237906634807587, 'logps/rejected': -60.77298355102539, 'logps/chosen': -59.969482421875, 'logits/rejected': -37.2061653137207, 'logits/chosen': -36.593162536621094, 'epoch': 1.08}\r\n",
      "{'loss': 0.6614, 'grad_norm': 4.352488994598389, 'learning_rate': 2e-06, 'rewards/chosen': -0.010090193711221218, 'rewards/rejected': -0.08386560529470444, 'rewards/accuracies': 0.6567187309265137, 'rewards/margins': 0.07377541065216064, 'logps/rejected': -60.695655822753906, 'logps/chosen': -60.147953033447266, 'logits/rejected': -36.7551155090332, 'logits/chosen': -36.45621109008789, 'epoch': 1.35}\r\n",
      "{'loss': 0.657, 'grad_norm': 4.5241265296936035, 'learning_rate': 2e-06, 'rewards/chosen': -0.028330283239483833, 'rewards/rejected': -0.11672339588403702, 'rewards/accuracies': 0.657031238079071, 'rewards/margins': 0.08839311450719833, 'logps/rejected': -60.718624114990234, 'logps/chosen': -60.43852233886719, 'logits/rejected': -36.64250564575195, 'logits/chosen': -36.254295349121094, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:17<09:05,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.73it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.75it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6759223341941833, 'eval_runtime': 5.6705, 'eval_samples_per_second': 219.557, 'eval_steps_per_second': 1.764, 'eval_rewards/chosen': -0.04678228497505188, 'eval_rewards/rejected': -0.097221739590168, 'eval_rewards/accuracies': 0.5859962701797485, 'eval_rewards/margins': 0.050439439713954926, 'eval_logps/rejected': -61.641815185546875, 'eval_logps/chosen': -60.385414123535156, 'eval_logits/rejected': -36.146392822265625, 'eval_logits/chosen': -35.860740661621094, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:23<09:05,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6561, 'grad_norm': 4.197821617126465, 'learning_rate': 2e-06, 'rewards/chosen': -0.044666837900877, 'rewards/rejected': -0.13956719636917114, 'rewards/accuracies': 0.6385937333106995, 'rewards/margins': 0.09490036219358444, 'logps/rejected': -60.940574645996094, 'logps/chosen': -60.75992202758789, 'logits/rejected': -36.598995208740234, 'logits/chosen': -36.025306701660156, 'epoch': 1.89}\r\n",
      "{'loss': 0.6387, 'grad_norm': 4.141510486602783, 'learning_rate': 2e-06, 'rewards/chosen': -0.04897916316986084, 'rewards/rejected': -0.1843758076429367, 'rewards/accuracies': 0.6796364784240723, 'rewards/margins': 0.13539662957191467, 'logps/rejected': -61.423728942871094, 'logps/chosen': -60.4000244140625, 'logits/rejected': -36.759517669677734, 'logits/chosen': -36.346778869628906, 'epoch': 2.16}\r\n",
      "{'loss': 0.6284, 'grad_norm': 4.240574359893799, 'learning_rate': 2e-06, 'rewards/chosen': -0.054181307554244995, 'rewards/rejected': -0.2175542116165161, 'rewards/accuracies': 0.7032812237739563, 'rewards/margins': 0.1633729189634323, 'logps/rejected': -61.66435241699219, 'logps/chosen': -60.46698760986328, 'logits/rejected': -36.6701545715332, 'logits/chosen': -36.441287994384766, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:30<06:02,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.19it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.69it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.72it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.74it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.75it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.78it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6731088161468506, 'eval_runtime': 5.725, 'eval_samples_per_second': 217.466, 'eval_steps_per_second': 1.747, 'eval_rewards/chosen': -0.10118655860424042, 'eval_rewards/rejected': -0.17117515206336975, 'eval_rewards/accuracies': 0.5776965618133545, 'eval_rewards/margins': 0.06998858600854874, 'eval_logps/rejected': -62.0115852355957, 'eval_logps/chosen': -60.657432556152344, 'eval_logits/rejected': -36.313926696777344, 'eval_logits/chosen': -36.0526123046875, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:35<06:02,  1.25s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.97it/s]\u001b[A\r\n",
      "{'loss': 0.6282, 'grad_norm': 4.064820766448975, 'learning_rate': 2e-06, 'rewards/chosen': -0.07134360820055008, 'rewards/rejected': -0.24041058123111725, 'rewards/accuracies': 0.6951562762260437, 'rewards/margins': 0.16906696557998657, 'logps/rejected': -61.61155319213867, 'logps/chosen': -60.839019775390625, 'logits/rejected': -36.716270446777344, 'logits/chosen': -36.332332611083984, 'epoch': 2.7}\r\n",
      "{'loss': 0.6246, 'grad_norm': 4.134665012359619, 'learning_rate': 2e-06, 'rewards/chosen': -0.0788583755493164, 'rewards/rejected': -0.2580353021621704, 'rewards/accuracies': 0.6978124976158142, 'rewards/margins': 0.179176926612854, 'logps/rejected': -61.6618766784668, 'logps/chosen': -60.56492614746094, 'logits/rejected': -36.631771087646484, 'logits/chosen': -36.17336654663086, 'epoch': 2.97}\r\n",
      "{'loss': 0.6034, 'grad_norm': 4.319583892822266, 'learning_rate': 2e-06, 'rewards/chosen': -0.07393822073936462, 'rewards/rejected': -0.3040187358856201, 'rewards/accuracies': 0.7372799515724182, 'rewards/margins': 0.2300805300474167, 'logps/rejected': -61.84880447387695, 'logps/chosen': -60.54194641113281, 'logits/rejected': -36.559635162353516, 'logits/chosen': -36.21559143066406, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:43<02:56,  1.26s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.61it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.00it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.93it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.85it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6709297299385071, 'eval_runtime': 5.4458, 'eval_samples_per_second': 228.616, 'eval_steps_per_second': 1.836, 'eval_rewards/chosen': -0.14092430472373962, 'eval_rewards/rejected': -0.22641244530677795, 'eval_rewards/accuracies': 0.5892220735549927, 'eval_rewards/margins': 0.08548813313245773, 'eval_logps/rejected': -62.28776168823242, 'eval_logps/chosen': -60.85612106323242, 'eval_logits/rejected': -36.41300964355469, 'eval_logits/chosen': -36.17628860473633, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:49<02:56,  1.26s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.03it/s]\u001b[A\r\n",
      "{'loss': 0.6042, 'grad_norm': 3.964488983154297, 'learning_rate': 2e-06, 'rewards/chosen': -0.09544128179550171, 'rewards/rejected': -0.3277473747730255, 'rewards/accuracies': 0.7285937666893005, 'rewards/margins': 0.2323060780763626, 'logps/rejected': -62.2890510559082, 'logps/chosen': -60.811309814453125, 'logits/rejected': -36.6603889465332, 'logits/chosen': -36.4267578125, 'epoch': 3.51}\r\n",
      "{'loss': 0.6027, 'grad_norm': 4.131308078765869, 'learning_rate': 2e-06, 'rewards/chosen': -0.10278309136629105, 'rewards/rejected': -0.34357166290283203, 'rewards/accuracies': 0.7217187285423279, 'rewards/margins': 0.24078857898712158, 'logps/rejected': -61.906246185302734, 'logps/chosen': -60.93790054321289, 'logits/rejected': -37.009586334228516, 'logits/chosen': -36.58560562133789, 'epoch': 3.78}\r\n",
      "{'train_runtime': 961.6429, 'train_samples_per_second': 98.373, 'train_steps_per_second': 0.77, 'train_loss': 0.6437125663499574, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:44<00:00,  1.28s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:05, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:33, 30.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:31<03:02, 30.41s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:31, 30.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:01, 30.40s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:02<01:30, 30.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:32<01:00, 30.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:03<00:30, 30.43s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:33<00:00, 30.42s/it]\r\n",
      "mean test reward 0.6909411944028787 +/- 0.009507316826163842 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9975618720054626 from 0.00922983093187213\r\n",
      "mean KL -0.13310731051670802 +/- 0.04323271991619392 full 1.2996608626821802 +/- 0.012482771633129846\r\n",
      "median KL 0.09167134761810303 full 1.1770763397216797\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:03, 30.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:31, 30.15s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:01, 30.21s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:00<02:31, 30.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:31<02:00, 30.24s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:01<01:30, 30.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:31<01:00, 30.33s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:02<00:30, 30.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:34<00:00, 30.45s/it]\r\n",
      "mean test reward 0.671717229659445 +/- 0.009666915802263884 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9970801770687103 from 0.00922983093187213\r\n",
      "mean KL 0.12384010396393326 +/- 0.03611853055890708 full 1.014849248165976 +/- 0.010640409973068544\r\n",
      "median KL 0.3012007474899292 full 0.907717376947403\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:03, 30.42s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:32, 30.33s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:00, 30.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:00<02:30, 30.17s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:31<02:01, 30.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:02<01:31, 30.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:33<01:01, 30.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:04<00:30, 30.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:34<00:00, 30.51s/it]\r\n",
      "mean test reward 0.6255046376274499 +/- 0.009963271934528445 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9956938922405243 from 0.00922983093187213\r\n",
      "mean KL 0.21946713588679106 +/- 0.026984236551546584 full 0.6364083042135462 +/- 0.0071466723951760175\r\n",
      "median KL 0.32704809308052063 full 0.5661595165729523\r\n",
      "dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44,0.2,epoch 4,0.6909411944028787,0.009507316826163842,1.2996608626821802,0.012482771633129846\r\n",
      "dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44,0.2,epoch 3,0.671717229659445,0.009666915802263884,1.014849248165976,0.010640409973068544\r\n",
      "dpo_uniform_flipped_2c2_0_25-0_2-2024.06.01.13.44,0.2,epoch 2,0.6255046376274499,0.009963271934528445,0.6364083042135462,0.0071466723951760175\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_uniform_flipped_2c2_0_25-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_uniform_flipped_0_25_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf31947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T14:14:58.251473Z",
     "iopub.status.busy": "2024-06-01T14:14:58.251022Z",
     "iopub.status.idle": "2024-06-01T14:45:34.339135Z",
     "shell.execute_reply": "2024-06-01T14:45:34.337896Z"
    },
    "papermill": {
     "duration": 1836.446219,
     "end_time": "2024-06-01T14:45:34.341756",
     "exception": false,
     "start_time": "2024-06-01T14:14:57.895537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 14:15:05.912050: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 14:15:05.912116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 14:15:05.913784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_141513-hs3qdodf\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_uniform_flipped_2c2_0_25-0_05\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/hs3qdodf\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.0794728994369507, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -62.52097702026367, 'logps/chosen': -61.076446533203125, 'logits/rejected': -35.7431755065918, 'logits/chosen': -35.82997131347656, 'epoch': 0.01}\r\n",
      "{'loss': 0.693, 'grad_norm': 1.1193320751190186, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 8.777929906500503e-05, 'rewards/rejected': -0.0001330984669039026, 'rewards/accuracies': 0.5699936151504517, 'rewards/margins': 0.00022087775869295, 'logps/rejected': -60.39039993286133, 'logps/chosen': -60.497310638427734, 'logits/rejected': -37.19394302368164, 'logits/chosen': -36.574363708496094, 'epoch': 0.27}\r\n",
      "{'loss': 0.6924, 'grad_norm': 1.1124919652938843, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0004275160899851471, 'rewards/rejected': -0.001180182909592986, 'rewards/accuracies': 0.589062511920929, 'rewards/margins': 0.0016076990868896246, 'logps/rejected': -60.35344696044922, 'logps/chosen': -60.2781982421875, 'logits/rejected': -37.14052963256836, 'logits/chosen': -36.68940353393555, 'epoch': 0.54}\r\n",
      "{'loss': 0.6909, 'grad_norm': 1.1370117664337158, 'learning_rate': 2e-06, 'rewards/chosen': 0.00023611349752172828, 'rewards/rejected': -0.004449209664016962, 'rewards/accuracies': 0.5954687595367432, 'rewards/margins': 0.0046853236854076385, 'logps/rejected': -60.54841613769531, 'logps/chosen': -60.245811462402344, 'logits/rejected': -36.760257720947266, 'logits/chosen': -36.50764846801758, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:05<12:08,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.68it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.72it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.75it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.77it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.79it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6904634237289429, 'eval_runtime': 5.7092, 'eval_samples_per_second': 218.068, 'eval_steps_per_second': 1.752, 'eval_rewards/chosen': -0.00036195488064549863, 'eval_rewards/rejected': -0.005943876691162586, 'eval_rewards/accuracies': 0.5901965498924255, 'eval_rewards/margins': 0.005581921432167292, 'eval_logps/rejected': -61.27458572387695, 'eval_logps/chosen': -60.15874099731445, 'eval_logits/rejected': -36.461204528808594, 'eval_logits/chosen': -36.151084899902344, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:11<12:08,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.98it/s]\u001b[A\r\n",
      "{'loss': 0.6876, 'grad_norm': 1.1717548370361328, 'learning_rate': 2e-06, 'rewards/chosen': -0.002307647606357932, 'rewards/rejected': -0.013823393732309341, 'rewards/accuracies': 0.6209694147109985, 'rewards/margins': 0.011515747755765915, 'logps/rejected': -60.81452178955078, 'logps/chosen': -59.99259948730469, 'logits/rejected': -37.14898681640625, 'logits/chosen': -36.539485931396484, 'epoch': 1.08}\r\n",
      "{'loss': 0.6833, 'grad_norm': 1.102686882019043, 'learning_rate': 2e-06, 'rewards/chosen': -0.0066218595020473, 'rewards/rejected': -0.027209101244807243, 'rewards/accuracies': 0.6434375047683716, 'rewards/margins': 0.020587243139743805, 'logps/rejected': -60.82051086425781, 'logps/chosen': -60.229942321777344, 'logits/rejected': -36.659305572509766, 'logits/chosen': -36.368446350097656, 'epoch': 1.35}\r\n",
      "{'loss': 0.6811, 'grad_norm': 1.141733169555664, 'learning_rate': 2e-06, 'rewards/chosen': -0.01683034934103489, 'rewards/rejected': -0.04268630966544151, 'rewards/accuracies': 0.6396874785423279, 'rewards/margins': 0.02585596591234207, 'logps/rejected': -60.98873519897461, 'logps/chosen': -60.63347625732422, 'logits/rejected': -36.494136810302734, 'logits/chosen': -36.12180709838867, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:18<09:10,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.73it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.76it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6858007907867432, 'eval_runtime': 5.6579, 'eval_samples_per_second': 220.047, 'eval_steps_per_second': 1.767, 'eval_rewards/chosen': -0.02437722496688366, 'eval_rewards/rejected': -0.04101530835032463, 'eval_rewards/accuracies': 0.5821908712387085, 'eval_rewards/margins': 0.01663808338344097, 'eval_logps/rejected': -61.97601318359375, 'eval_logps/chosen': -60.6390495300293, 'eval_logits/rejected': -35.97025680541992, 'eval_logits/chosen': -35.69813919067383, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:24<09:10,  1.25s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6799, 'grad_norm': 1.0696616172790527, 'learning_rate': 2e-06, 'rewards/chosen': -0.028768278658390045, 'rewards/rejected': -0.05787740275263786, 'rewards/accuracies': 0.6212499737739563, 'rewards/margins': 0.029109125956892967, 'logps/rejected': -61.40028381347656, 'logps/chosen': -61.1119499206543, 'logits/rejected': -36.383480072021484, 'logits/chosen': -35.828983306884766, 'epoch': 1.89}\r\n",
      "{'loss': 0.6735, 'grad_norm': 1.0752800703048706, 'learning_rate': 2e-06, 'rewards/chosen': -0.03997403383255005, 'rewards/rejected': -0.08294728398323059, 'rewards/accuracies': 0.6497927308082581, 'rewards/margins': 0.042973242700099945, 'logps/rejected': -62.16080093383789, 'logps/chosen': -60.95460891723633, 'logits/rejected': -36.466514587402344, 'logits/chosen': -36.09221649169922, 'epoch': 2.16}\r\n",
      "{'loss': 0.6693, 'grad_norm': 1.1195330619812012, 'learning_rate': 2e-06, 'rewards/chosen': -0.05483291670680046, 'rewards/rejected': -0.10801210254430771, 'rewards/accuracies': 0.6618750095367432, 'rewards/margins': 0.053179193288087845, 'logps/rejected': -62.736820220947266, 'logps/chosen': -61.2927360534668, 'logits/rejected': -36.360774993896484, 'logits/chosen': -36.168121337890625, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:31<05:59,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.53it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.73it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.75it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6815610527992249, 'eval_runtime': 5.6703, 'eval_samples_per_second': 219.564, 'eval_steps_per_second': 1.764, 'eval_rewards/chosen': -0.07446056604385376, 'eval_rewards/rejected': -0.10333251953125, 'eval_rewards/accuracies': 0.5761340856552124, 'eval_rewards/margins': 0.028871949762105942, 'eval_logps/rejected': -63.22235870361328, 'eval_logps/chosen': -61.640716552734375, 'eval_logits/rejected': -35.96866226196289, 'eval_logits/chosen': -35.74689865112305, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:36<05:59,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6679, 'grad_norm': 1.0711697340011597, 'learning_rate': 2e-06, 'rewards/chosen': -0.07572430372238159, 'rewards/rejected': -0.1333540678024292, 'rewards/accuracies': 0.6490625143051147, 'rewards/margins': 0.057629767805337906, 'logps/rejected': -63.07658767700195, 'logps/chosen': -61.996788024902344, 'logits/rejected': -36.33100509643555, 'logits/chosen': -35.989192962646484, 'epoch': 2.7}\r\n",
      "{'loss': 0.665, 'grad_norm': 1.0984997749328613, 'learning_rate': 2e-06, 'rewards/chosen': -0.09493768960237503, 'rewards/rejected': -0.1602075695991516, 'rewards/accuracies': 0.65234375, 'rewards/margins': 0.06526989489793777, 'logps/rejected': -63.57583999633789, 'logps/chosen': -62.06938552856445, 'logits/rejected': -36.22309494018555, 'logits/chosen': -35.84836959838867, 'epoch': 2.97}\r\n",
      "{'loss': 0.6575, 'grad_norm': 1.1983219385147095, 'learning_rate': 2e-06, 'rewards/chosen': -0.11551600694656372, 'rewards/rejected': -0.1986757516860962, 'rewards/accuracies': 0.668373703956604, 'rewards/margins': 0.08315972983837128, 'logps/rejected': -64.3022232055664, 'logps/chosen': -62.48257827758789, 'logits/rejected': -36.194210052490234, 'logits/chosen': -35.913700103759766, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:45<02:57,  1.27s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.53it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.00it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.93it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.85it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.85it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6774890422821045, 'eval_runtime': 5.454, 'eval_samples_per_second': 228.273, 'eval_steps_per_second': 1.834, 'eval_rewards/chosen': -0.14482608437538147, 'eval_rewards/rejected': -0.18806229531764984, 'eval_rewards/accuracies': 0.5753527879714966, 'eval_rewards/margins': 0.043236203491687775, 'eval_logps/rejected': -64.91694641113281, 'eval_logps/chosen': -63.04802322387695, 'eval_logits/rejected': -36.05485534667969, 'eval_logits/chosen': -35.901832580566406, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:51<02:57,  1.27s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.03it/s]\u001b[A\r\n",
      "{'loss': 0.6565, 'grad_norm': 1.0702500343322754, 'learning_rate': 2e-06, 'rewards/chosen': -0.14650820195674896, 'rewards/rejected': -0.23415972292423248, 'rewards/accuracies': 0.663281261920929, 'rewards/margins': 0.08765149116516113, 'logps/rejected': -65.33351135253906, 'logps/chosen': -63.26426315307617, 'logits/rejected': -36.306209564208984, 'logits/chosen': -36.155906677246094, 'epoch': 3.51}\r\n",
      "{'loss': 0.6554, 'grad_norm': 1.1196730136871338, 'learning_rate': 2e-06, 'rewards/chosen': -0.1730557233095169, 'rewards/rejected': -0.2661430239677429, 'rewards/accuracies': 0.645312488079071, 'rewards/margins': 0.0930873230099678, 'logps/rejected': -65.51123809814453, 'logps/chosen': -63.88509750366211, 'logits/rejected': -36.642311096191406, 'logits/chosen': -36.34637451171875, 'epoch': 3.78}\r\n",
      "{'train_runtime': 964.7755, 'train_samples_per_second': 98.054, 'train_steps_per_second': 0.767, 'train_loss': 0.6740352160221822, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:48<00:00,  1.28s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:01, 30.24s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:30, 30.12s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:01, 30.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:31, 30.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:02, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:32, 30.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:34<01:01, 30.72s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:04<00:30, 30.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:35<00:00, 30.64s/it]\r\n",
      "mean test reward 0.8323854191126985 +/- 0.007682842753015513 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9986743330955505 from 0.00922983093187213\r\n",
      "mean KL 2.3355117028501504 +/- 0.08699101325819884 full 4.884572190698236 +/- 0.03693577123344111\r\n",
      "median KL 2.792699933052063 full 4.647141456604004\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:07, 30.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:36, 30.93s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:32<03:05, 30.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:03<02:33, 30.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:33<02:02, 30.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:04<01:32, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:35<01:01, 30.70s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:06<00:30, 30.79s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:37<00:00, 30.78s/it]\r\n",
      "mean test reward 0.7918243391096667 +/- 0.008365372361790501 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9984817504882812 from 0.00922983093187213\r\n",
      "mean KL 1.702906084667322 +/- 0.06277122920028542 full 3.0400270262940063 +/- 0.025688297199469735\r\n",
      "median KL 1.9263628721237183 full 2.8609451055526733\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:05, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:33, 30.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:32<03:04, 30.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:03<02:34, 30.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:33<02:03, 30.78s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:31, 30.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:33<01:00, 30.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:03<00:30, 30.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:35<00:00, 30.58s/it]\r\n",
      "mean test reward 0.6903473589366563 +/- 0.009538753630871963 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9974463284015656 from 0.00922983093187213\r\n",
      "mean KL 0.7916509396099072 +/- 0.038938354975661275 full 1.2954014742992714 +/- 0.012831740651771497\r\n",
      "median KL 0.9325133562088013 full 1.1996330618858337\r\n",
      "dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15,0.05,epoch 4,0.8323854191126985,0.007682842753015513,4.884572190698236,0.03693577123344111\r\n",
      "dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15,0.05,epoch 3,0.7918243391096667,0.008365372361790501,3.0400270262940063,0.025688297199469735\r\n",
      "dpo_uniform_flipped_2c2_0_25-0_05-2024.06.01.14.15,0.05,epoch 2,0.6903473589366563,0.009538753630871963,1.2954014742992714,0.012831740651771497\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_uniform_flipped_2c2_0_25-0_05 --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_uniform_flipped_0_25_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ee5b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T14:45:35.301016Z",
     "iopub.status.busy": "2024-06-01T14:45:35.300606Z",
     "iopub.status.idle": "2024-06-01T15:15:00.220265Z",
     "shell.execute_reply": "2024-06-01T15:15:00.219071Z"
    },
    "papermill": {
     "duration": 1765.437495,
     "end_time": "2024-06-01T15:15:00.222692",
     "exception": false,
     "start_time": "2024-06-01T14:45:34.785197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 14:45:42.970020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 14:45:42.970084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 14:45:42.971809: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_144550-3trjnlyu\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_uniform_flipped_2c2_0_5-0_05\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/3trjnlyu\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.1230782270431519, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -61.850318908691406, 'logps/chosen': -61.747108459472656, 'logits/rejected': -36.069252014160156, 'logits/chosen': -35.5038948059082, 'epoch': 0.01}\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.130582332611084, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 9.497288147031213e-07, 'rewards/rejected': -1.0555293101788266e-06, 'rewards/accuracies': 0.484375, 'rewards/margins': 2.0052571017004084e-06, 'logps/rejected': -60.40513229370117, 'logps/chosen': -60.48167037963867, 'logits/rejected': -36.888004302978516, 'logits/chosen': -36.90485763549805, 'epoch': 0.27}\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.1071358919143677, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': -2.6827990950550884e-05, 'rewards/rejected': -3.313469642307609e-05, 'rewards/accuracies': 0.5004687309265137, 'rewards/margins': 6.306710474746069e-06, 'logps/rejected': -60.539268493652344, 'logps/chosen': -60.078521728515625, 'logits/rejected': -36.91038131713867, 'logits/chosen': -37.07219314575195, 'epoch': 0.54}\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.1534441709518433, 'learning_rate': 2e-06, 'rewards/chosen': -0.00011718453606590629, 'rewards/rejected': -0.00021547447249758989, 'rewards/accuracies': 0.5079687237739563, 'rewards/margins': 9.828989277593791e-05, 'logps/rejected': -60.26580047607422, 'logps/chosen': -60.45082092285156, 'logits/rejected': -36.880069732666016, 'logits/chosen': -36.754024505615234, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:05<12:06,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.70it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6932632327079773, 'eval_runtime': 5.6753, 'eval_samples_per_second': 219.372, 'eval_steps_per_second': 1.762, 'eval_rewards/chosen': -0.000398074189433828, 'eval_rewards/rejected': -0.000192986277397722, 'eval_rewards/accuracies': 0.49380040168762207, 'eval_rewards/margins': -0.00020508789748419076, 'eval_logps/rejected': -61.22026824951172, 'eval_logps/chosen': -60.0987663269043, 'eval_logits/rejected': -36.56425857543945, 'eval_logits/chosen': -36.556453704833984, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:10<12:06,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.692, 'grad_norm': 1.1589933633804321, 'learning_rate': 2e-06, 'rewards/chosen': 0.0008554145460948348, 'rewards/rejected': -0.001504665706306696, 'rewards/accuracies': 0.5699872374534607, 'rewards/margins': 0.0023600803688168526, 'logps/rejected': -60.231178283691406, 'logps/chosen': -60.26631164550781, 'logits/rejected': -36.88215637207031, 'logits/chosen': -37.31401062011719, 'epoch': 1.08}\r\n",
      "{'loss': 0.6893, 'grad_norm': 1.1485719680786133, 'learning_rate': 2e-06, 'rewards/chosen': 0.002869429998099804, 'rewards/rejected': -0.004803122486919165, 'rewards/accuracies': 0.6943749785423279, 'rewards/margins': 0.007672552019357681, 'logps/rejected': -60.304283142089844, 'logps/chosen': -60.108219146728516, 'logits/rejected': -36.932132720947266, 'logits/chosen': -36.879337310791016, 'epoch': 1.35}\r\n",
      "{'loss': 0.6895, 'grad_norm': 1.0839980840682983, 'learning_rate': 2e-06, 'rewards/chosen': 0.002317124977707863, 'rewards/rejected': -0.00513411033898592, 'rewards/accuracies': 0.6534374952316284, 'rewards/margins': 0.0074512348510324955, 'logps/rejected': -60.230838775634766, 'logps/chosen': -60.25737762451172, 'logits/rejected': -36.943603515625, 'logits/chosen': -37.010711669921875, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:18<09:09,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.77it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.78it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.79it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.80it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6933286786079407, 'eval_runtime': 5.6024, 'eval_samples_per_second': 222.227, 'eval_steps_per_second': 1.785, 'eval_rewards/chosen': -0.0019364053150638938, 'eval_rewards/rejected': -0.001601282856427133, 'eval_rewards/accuracies': 0.49467405676841736, 'eval_rewards/margins': -0.00033512269146740437, 'eval_logps/rejected': -61.24843215942383, 'eval_logps/chosen': -60.1295280456543, 'eval_logits/rejected': -36.44482421875, 'eval_logits/chosen': -36.436805725097656, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:23<09:09,  1.25s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.6894, 'grad_norm': 1.122021198272705, 'learning_rate': 2e-06, 'rewards/chosen': 0.0017975601367652416, 'rewards/rejected': -0.00575334532186389, 'rewards/accuracies': 0.6399999856948853, 'rewards/margins': 0.007550905458629131, 'logps/rejected': -60.5927734375, 'logps/chosen': -60.26565933227539, 'logits/rejected': -36.58952713012695, 'logits/chosen': -36.509849548339844, 'epoch': 1.89}\r\n",
      "{'loss': 0.6858, 'grad_norm': 1.135152816772461, 'learning_rate': 2e-06, 'rewards/chosen': 0.00499937916174531, 'rewards/rejected': -0.00988202728331089, 'rewards/accuracies': 0.7382652759552002, 'rewards/margins': 0.014881405979394913, 'logps/rejected': -60.475833892822266, 'logps/chosen': -60.2787971496582, 'logits/rejected': -36.494606018066406, 'logits/chosen': -36.7056999206543, 'epoch': 2.16}\r\n",
      "{'loss': 0.6831, 'grad_norm': 1.1155519485473633, 'learning_rate': 2e-06, 'rewards/chosen': 0.0060718003660440445, 'rewards/rejected': -0.014265713281929493, 'rewards/accuracies': 0.7579687237739563, 'rewards/margins': 0.020337514579296112, 'logps/rejected': -60.668540954589844, 'logps/chosen': -60.268001556396484, 'logits/rejected': -36.573551177978516, 'logits/chosen': -36.63386535644531, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:29<06:00,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.61it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.71it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.77it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6934559345245361, 'eval_runtime': 5.6784, 'eval_samples_per_second': 219.251, 'eval_steps_per_second': 1.761, 'eval_rewards/chosen': -0.004521901253610849, 'eval_rewards/rejected': -0.004093885887414217, 'eval_rewards/accuracies': 0.495850145816803, 'eval_rewards/margins': -0.00042801498784683645, 'eval_logps/rejected': -61.29829025268555, 'eval_logps/chosen': -60.181236267089844, 'eval_logits/rejected': -36.11909103393555, 'eval_logits/chosen': -36.1136360168457, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:35<06:00,  1.24s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6833, 'grad_norm': 1.0528250932693481, 'learning_rate': 2e-06, 'rewards/chosen': 0.005143166985362768, 'rewards/rejected': -0.014904890209436417, 'rewards/accuracies': 0.7306249737739563, 'rewards/margins': 0.020048055797815323, 'logps/rejected': -60.88321304321289, 'logps/chosen': -60.20382308959961, 'logits/rejected': -36.40556716918945, 'logits/chosen': -36.473262786865234, 'epoch': 2.7}\r\n",
      "{'loss': 0.6831, 'grad_norm': 1.0785959959030151, 'learning_rate': 2e-06, 'rewards/chosen': 0.004424865357577801, 'rewards/rejected': -0.016188746318221092, 'rewards/accuracies': 0.7109375, 'rewards/margins': 0.02061360888183117, 'logps/rejected': -60.544010162353516, 'logps/chosen': -60.23359298706055, 'logits/rejected': -36.041961669921875, 'logits/chosen': -36.142311096191406, 'epoch': 2.97}\r\n",
      "{'loss': 0.6772, 'grad_norm': 1.113823413848877, 'learning_rate': 2e-06, 'rewards/chosen': 0.00864706002175808, 'rewards/rejected': -0.023958496749401093, 'rewards/accuracies': 0.7831218242645264, 'rewards/margins': 0.03260555863380432, 'logps/rejected': -60.92660140991211, 'logps/chosen': -59.880592346191406, 'logits/rejected': -36.2254524230957, 'logits/chosen': -36.3544921875, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:44<02:55,  1.26s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.01it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.93it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.89it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.87it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6935931444168091, 'eval_runtime': 5.4253, 'eval_samples_per_second': 229.48, 'eval_steps_per_second': 1.843, 'eval_rewards/chosen': -0.009458905085921288, 'eval_rewards/rejected': -0.008978625759482384, 'eval_rewards/accuracies': 0.49448084831237793, 'eval_rewards/margins': -0.0004802783369086683, 'eval_logps/rejected': -61.3959846496582, 'eval_logps/chosen': -60.27997589111328, 'eval_logits/rejected': -36.157630920410156, 'eval_logits/chosen': -36.16488265991211, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:49<02:55,  1.26s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\r\n",
      "{'loss': 0.6767, 'grad_norm': 1.138504981994629, 'learning_rate': 2e-06, 'rewards/chosen': 0.006227557081729174, 'rewards/rejected': -0.02753322198987007, 'rewards/accuracies': 0.7606250047683716, 'rewards/margins': 0.03376077860593796, 'logps/rejected': -61.177886962890625, 'logps/chosen': -60.232643127441406, 'logits/rejected': -36.18151092529297, 'logits/chosen': -36.332550048828125, 'epoch': 3.51}\r\n",
      "{'loss': 0.6761, 'grad_norm': 1.157120704650879, 'learning_rate': 2e-06, 'rewards/chosen': 0.004290608689188957, 'rewards/rejected': -0.0308670811355114, 'rewards/accuracies': 0.7396875023841858, 'rewards/margins': 0.0351576954126358, 'logps/rejected': -60.713871002197266, 'logps/chosen': -60.430023193359375, 'logits/rejected': -36.41194152832031, 'logits/chosen': -36.384849548339844, 'epoch': 3.78}\r\n",
      "{'train_runtime': 962.2603, 'train_samples_per_second': 98.31, 'train_steps_per_second': 0.769, 'train_loss': 0.6855627001942814, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:45<00:00,  1.28s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:54, 29.26s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:57<03:20, 28.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:26<02:51, 28.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:54<02:23, 28.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:23<01:54, 28.60s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:51<01:25, 28.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:20<00:57, 28.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:48<00:28, 28.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:17<00:00, 28.57s/it]\r\n",
      "mean test reward 0.4275610003701584 +/- 0.010197283639371342 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.007617847993969917 from 0.00922983093187213\r\n",
      "mean KL -0.20699197731705177 +/- 0.02788235166791483 full 0.4073311913913737 +/- 0.0031045273513624866\r\n",
      "median KL -0.06363202631473541 full 0.375994473695755\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:46, 28.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:56<03:17, 28.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:24<02:49, 28.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:53<02:21, 28.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:21<01:52, 28.21s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:49<01:24, 28.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:17<00:56, 28.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:46<00:28, 28.26s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:14<00:00, 28.26s/it]\r\n",
      "mean test reward 0.42599436592667594 +/- 0.010188707244878492 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.007820036262273788 from 0.00922983093187213\r\n",
      "mean KL -0.028795662707933743 +/- 0.015056987837278308 full 0.16722787059875877 +/- 0.0011531576217862695\r\n",
      "median KL 0.03568969666957855 full 0.15842162817716599\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:46, 28.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:55<03:14, 27.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:23<02:47, 27.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:51<02:19, 27.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:19<01:51, 27.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:46<01:23, 27.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:14<00:55, 27.75s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:42<00:27, 27.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:10<00:00, 27.85s/it]\r\n",
      "mean test reward 0.42478450335561824 +/- 0.010179780728964983 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.007905714213848114 from 0.00922983093187213\r\n",
      "mean KL 0.07303795667798517 +/- 0.007597152242062713 full 0.05861799552642171 +/- 0.0004332960216487472\r\n",
      "median KL 0.07498137652873993 full 0.05549303814768791\r\n",
      "dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45,0.05,epoch 4,0.4275610003701584,0.010197283639371342,0.4073311913913737,0.0031045273513624866\r\n",
      "dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45,0.05,epoch 3,0.42599436592667594,0.010188707244878492,0.16722787059875877,0.0011531576217862695\r\n",
      "dpo_uniform_flipped_2c2_0_5-0_05-2024.06.01.14.45,0.05,epoch 2,0.42478450335561824,0.010179780728964983,0.05861799552642171,0.0004332960216487472\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_uniform_flipped_2c2_0_5-0_05 --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_uniform_flipped_0_5_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37663c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T15:15:01.264446Z",
     "iopub.status.busy": "2024-06-01T15:15:01.264058Z",
     "iopub.status.idle": "2024-06-01T15:44:08.722167Z",
     "shell.execute_reply": "2024-06-01T15:44:08.720986Z"
    },
    "papermill": {
     "duration": 1747.963444,
     "end_time": "2024-06-01T15:44:08.724766",
     "exception": false,
     "start_time": "2024-06-01T15:15:00.761322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-01 15:15:08.430906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-01 15:15:08.430965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-01 15:15:08.432643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_151515-08an62my\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_uniform_flipped_2c2_0_5-0_2\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/08an62my\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.492312908172607, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -61.850318908691406, 'logps/chosen': -61.747108459472656, 'logits/rejected': -36.069252014160156, 'logits/chosen': -35.5038948059082, 'epoch': 0.01}\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.521970748901367, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 3.873308742186055e-06, 'rewards/rejected': -4.288383934181184e-06, 'rewards/accuracies': 0.48405611515045166, 'rewards/margins': 8.161691766872536e-06, 'logps/rejected': -60.40513229370117, 'logps/chosen': -60.48166275024414, 'logits/rejected': -36.888038635253906, 'logits/chosen': -36.9048957824707, 'epoch': 0.27}\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.42412805557251, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': -0.00010532527812756598, 'rewards/rejected': -0.0001277559931622818, 'rewards/accuracies': 0.5010937452316284, 'rewards/margins': 2.2430731405620463e-05, 'logps/rejected': -60.539249420166016, 'logps/chosen': -60.078514099121094, 'logits/rejected': -36.911346435546875, 'logits/chosen': -37.07315444946289, 'epoch': 0.54}\r\n",
      "{'loss': 0.693, 'grad_norm': 4.615036964416504, 'learning_rate': 2e-06, 'rewards/chosen': -0.0004214564396534115, 'rewards/rejected': -0.0008017171057872474, 'rewards/accuracies': 0.5079687237739563, 'rewards/margins': 0.0003802607243414968, 'logps/rejected': -60.2655029296875, 'logps/chosen': -60.450584411621094, 'logits/rejected': -36.88169479370117, 'logits/chosen': -36.755680084228516, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:04<12:06,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.69it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.73it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.77it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6936317682266235, 'eval_runtime': 5.6687, 'eval_samples_per_second': 219.627, 'eval_steps_per_second': 1.764, 'eval_rewards/chosen': -0.0013964356621727347, 'eval_rewards/rejected': -0.0006688031135126948, 'eval_rewards/accuracies': 0.49926915764808655, 'eval_rewards/margins': -0.0007276328979060054, 'eval_logps/rejected': -61.219757080078125, 'eval_logps/chosen': -60.09778594970703, 'eval_logits/rejected': -36.56300354003906, 'eval_logits/chosen': -36.55536651611328, 'epoch': 0.81}\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 150/740 [03:10<12:06,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6887, 'grad_norm': 4.582692623138428, 'learning_rate': 2e-06, 'rewards/chosen': 0.003609162289649248, 'rewards/rejected': -0.005692262668162584, 'rewards/accuracies': 0.5707685351371765, 'rewards/margins': 0.009301424957811832, 'logps/rejected': -60.22954559326172, 'logps/chosen': -60.2653694152832, 'logits/rejected': -36.87229919433594, 'logits/chosen': -37.3044319152832, 'epoch': 1.08}\r\n",
      "{'loss': 0.6785, 'grad_norm': 4.54762601852417, 'learning_rate': 2e-06, 'rewards/chosen': 0.01219905260950327, 'rewards/rejected': -0.01809530332684517, 'rewards/accuracies': 0.7159374952316284, 'rewards/margins': 0.030294353142380714, 'logps/rejected': -60.2986946105957, 'logps/chosen': -60.104618072509766, 'logits/rejected': -36.903343200683594, 'logits/chosen': -36.85142517089844, 'epoch': 1.35}\r\n",
      "{'loss': 0.679, 'grad_norm': 4.276611804962158, 'learning_rate': 2e-06, 'rewards/chosen': 0.010955111123621464, 'rewards/rejected': -0.018496891483664513, 'rewards/accuracies': 0.6815624833106995, 'rewards/margins': 0.029452001675963402, 'logps/rejected': -60.22063446044922, 'logps/chosen': -60.24894714355469, 'logits/rejected': -36.903953552246094, 'logits/chosen': -36.97160720825195, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:15<09:02,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.75it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.77it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.79it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.80it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6937942504882812, 'eval_runtime': 5.6095, 'eval_samples_per_second': 221.946, 'eval_steps_per_second': 1.783, 'eval_rewards/chosen': -0.004818403162062168, 'eval_rewards/rejected': -0.004130621440708637, 'eval_rewards/accuracies': 0.49154067039489746, 'eval_rewards/margins': -0.0006877820705994964, 'eval_logps/rejected': -61.237060546875, 'eval_logps/chosen': -60.11488723754883, 'eval_logits/rejected': -36.399559020996094, 'eval_logits/chosen': -36.39413833618164, 'epoch': 1.62}\r\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [06:21<09:02,  1.23s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.679, 'grad_norm': 4.4248433113098145, 'learning_rate': 2e-06, 'rewards/chosen': 0.009679190814495087, 'rewards/rejected': -0.019833413884043694, 'rewards/accuracies': 0.6753125190734863, 'rewards/margins': 0.02951260656118393, 'logps/rejected': -60.576881408691406, 'logps/chosen': -60.25322341918945, 'logits/rejected': -36.5471076965332, 'logits/chosen': -36.46658706665039, 'epoch': 1.89}\r\n",
      "{'loss': 0.6652, 'grad_norm': 4.409360408782959, 'learning_rate': 2e-06, 'rewards/chosen': 0.023100918158888817, 'rewards/rejected': -0.035142045468091965, 'rewards/accuracies': 0.7874362468719482, 'rewards/margins': 0.05824296548962593, 'logps/rejected': -60.45390701293945, 'logps/chosen': -60.263282775878906, 'logits/rejected': -36.46206283569336, 'logits/chosen': -36.671512603759766, 'epoch': 2.16}\r\n",
      "{'loss': 0.6554, 'grad_norm': 4.324149131774902, 'learning_rate': 2e-06, 'rewards/chosen': 0.02955794520676136, 'rewards/rejected': -0.04961823299527168, 'rewards/accuracies': 0.8121874928474426, 'rewards/margins': 0.0791761726140976, 'logps/rejected': -60.631317138671875, 'logps/chosen': -60.241641998291016, 'logits/rejected': -36.5572624206543, 'logits/chosen': -36.61598205566406, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:27<06:03,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  1.75it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:03<00:02,  1.77it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.78it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:04<00:01,  1.79it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6944709420204163, 'eval_runtime': 5.6135, 'eval_samples_per_second': 221.786, 'eval_steps_per_second': 1.781, 'eval_rewards/chosen': -0.009700831957161427, 'eval_rewards/rejected': -0.008945122361183167, 'eval_rewards/accuracies': 0.5017052888870239, 'eval_rewards/margins': -0.0007557104108855128, 'eval_logps/rejected': -61.261131286621094, 'eval_logps/chosen': -60.13930130004883, 'eval_logits/rejected': -36.10767364501953, 'eval_logits/chosen': -36.10468292236328, 'epoch': 2.43}\r\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 450/740 [09:33<06:03,  1.25s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.656, 'grad_norm': 4.0663042068481445, 'learning_rate': 2e-06, 'rewards/chosen': 0.02812560833990574, 'rewards/rejected': -0.049938470125198364, 'rewards/accuracies': 0.7946875095367432, 'rewards/margins': 0.07806407660245895, 'logps/rejected': -60.83480453491211, 'logps/chosen': -60.16606140136719, 'logits/rejected': -36.40769958496094, 'logits/chosen': -36.47509765625, 'epoch': 2.7}\r\n",
      "{'loss': 0.6556, 'grad_norm': 4.207059383392334, 'learning_rate': 2e-06, 'rewards/chosen': 0.027056284248828888, 'rewards/rejected': -0.05248672515153885, 'rewards/accuracies': 0.7714062333106995, 'rewards/margins': 0.07954301685094833, 'logps/rejected': -60.482662200927734, 'logps/chosen': -60.18681335449219, 'logits/rejected': -36.07790756225586, 'logits/chosen': -36.18017578125, 'epoch': 2.97}\r\n",
      "{'loss': 0.6348, 'grad_norm': 4.194408416748047, 'learning_rate': 2e-06, 'rewards/chosen': 0.04621625319123268, 'rewards/rejected': -0.07812920957803726, 'rewards/accuracies': 0.8527837991714478, 'rewards/margins': 0.12434547394514084, 'logps/rejected': -60.83808135986328, 'logps/chosen': -59.822452545166016, 'logits/rejected': -36.308021545410156, 'logits/chosen': -36.433692932128906, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:40<02:55,  1.26s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.02it/s]\u001b[A\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\r\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6954330205917358, 'eval_runtime': 5.3853, 'eval_samples_per_second': 231.183, 'eval_steps_per_second': 1.857, 'eval_rewards/chosen': -0.01903550513088703, 'eval_rewards/rejected': -0.017986882477998734, 'eval_rewards/accuracies': 0.5009240508079529, 'eval_rewards/margins': -0.0010486228857189417, 'eval_logps/rejected': -61.30634689331055, 'eval_logps/chosen': -60.18597412109375, 'eval_logits/rejected': -36.24336624145508, 'eval_logits/chosen': -36.25136947631836, 'epoch': 3.24}\r\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [12:45<02:55,  1.26s/it]\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\r\n",
      "{'loss': 0.6329, 'grad_norm': 4.323297023773193, 'learning_rate': 2e-06, 'rewards/chosen': 0.042656272649765015, 'rewards/rejected': -0.08660957962274551, 'rewards/accuracies': 0.8379687666893005, 'rewards/margins': 0.12926585972309113, 'logps/rejected': -61.060264587402344, 'logps/chosen': -60.14390563964844, 'logits/rejected': -36.29838943481445, 'logits/chosen': -36.442928314208984, 'epoch': 3.51}\r\n",
      "{'loss': 0.6319, 'grad_norm': 4.381690979003906, 'learning_rate': 2e-06, 'rewards/chosen': 0.04015728458762169, 'rewards/rejected': -0.09236571192741394, 'rewards/accuracies': 0.8167187571525574, 'rewards/margins': 0.13252300024032593, 'logps/rejected': -60.5583610534668, 'logps/chosen': -60.31504821777344, 'logits/rejected': -36.572330474853516, 'logits/chosen': -36.54911804199219, 'epoch': 3.78}\r\n",
      "{'train_runtime': 956.2503, 'train_samples_per_second': 98.928, 'train_steps_per_second': 0.774, 'train_loss': 0.665115239813521, 'epoch': 4.0}\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [15:39<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:27<03:41, 27.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:55<03:13, 27.66s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:22<02:45, 27.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:50<02:18, 27.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:17<01:50, 27.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:45<01:22, 27.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:13<00:55, 27.66s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:41<00:27, 27.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:09<00:00, 27.77s/it]\r\n",
      "mean test reward 0.42252520411988015 +/- 0.010182103851501602 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.007220318308100104 from 0.00922983093187213\r\n",
      "mean KL -0.1932492253747316 +/- 0.01800675935132413 full 0.1848323845058783 +/- 0.0013190084284346413\r\n",
      "median KL -0.1029205247759819 full 0.17257652431726456\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:27<03:42, 27.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:55<03:14, 27.72s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:23<02:46, 27.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:51<02:19, 27.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:18<01:51, 27.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:47<01:23, 27.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:14<00:55, 27.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:42<00:27, 27.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:11<00:00, 27.90s/it]\r\n",
      "mean test reward 0.4237849044314872 +/- 0.01018116683315423 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.007712522754445672 from 0.00922983093187213\r\n",
      "mean KL -0.042827709686308585 +/- 0.010693238148784147 full 0.08961368063157554 +/- 0.0006042514529957093\r\n",
      "median KL -0.008354008197784424 full 0.08542115986347198\r\n",
      "Loading from dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:27<03:43, 27.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:55<03:15, 27.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:23<02:48, 28.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:51<02:19, 27.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:19<01:51, 28.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:47<01:23, 27.93s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:15<00:56, 28.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:44<00:28, 28.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:12<00:00, 28.01s/it]\r\n",
      "mean test reward 0.431332689479935 +/- 0.010193391397916973 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.00840333104133606 from 0.00922983093187213\r\n",
      "mean KL 0.03288167275119728 +/- 0.005944424452013177 full 0.035842142984620295 +/- 0.00024398162293824363\r\n",
      "median KL 0.036804139614105225 full 0.03419411554932594\r\n",
      "dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15,0.2,epoch 4,0.42252520411988015,0.010182103851501602,0.1848323845058783,0.0013190084284346413\r\n",
      "dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15,0.2,epoch 3,0.4237849044314872,0.01018116683315423,0.08961368063157554,0.0006042514529957093\r\n",
      "dpo_uniform_flipped_2c2_0_5-0_2-2024.06.01.15.15,0.2,epoch 2,0.431332689479935,0.010193391397916973,0.035842142984620295,0.00024398162293824363\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_uniform_flipped_2c2_0_5-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_uniform_flipped_0_5_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12792.02816,
   "end_time": "2024-06-01T15:44:11.926351",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-01T12:10:59.898191",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
