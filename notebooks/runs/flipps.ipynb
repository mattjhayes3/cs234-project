{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !conda install -y gdown\n# print('hi')\n!conda install aiohttp -y\n!git clone https://github.com/mattjhayes3/cs234-project.git\n%cd /kaggle/working/cs234-project\n!git pull --rebase\n!pip install wandb\nimport wandb\n\nwandb.login(key=\"KEY\")\n# wandb.init()\n\nfrom huggingface_hub import notebook_login,login\n\n# notebook_login(\"KEY\")\nlogin(\"KEY\")\n\n# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n!mkdir results\n!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n!pip install git+https://github.com/mattjhayes3/trl.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-01T10:27:28.077600Z","iopub.execute_input":"2024-06-01T10:27:28.078395Z","iopub.status.idle":"2024-06-01T10:30:02.524799Z","shell.execute_reply.started":"2024-06-01T10:27:28.078345Z","shell.execute_reply":"2024-06-01T10:30:02.523677Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - rapidsai\n - nvidia\n - conda-forge\n - defaults\n - pytorch\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - aiohttp\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    aiohttp-3.9.5              |  py310h2372a71_0         682 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         682 KB\n\nThe following packages will be UPDATED:\n\n  aiohttp                             3.9.1-py310h2372a71_0 --> 3.9.5-py310h2372a71_0 \n\n\n\nDownloading and Extracting Packages:\n                                                                                \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nCloning into 'cs234-project'...\nremote: Enumerating objects: 255, done.\u001b[K\nremote: Counting objects: 100% (35/35), done.\u001b[K\nremote: Compressing objects: 100% (24/24), done.\u001b[K\nremote: Total 255 (delta 17), reused 29 (delta 11), pack-reused 220\u001b[K\nReceiving objects: 100% (255/255), 56.43 MiB | 18.98 MiB/s, done.\nResolving deltas: 100% (124/124), done.\nUpdating files: 100% (109/109), done.\n/kaggle/working/cs234-project\nAlready up to date.\nCurrent branch main is up to date.\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.2)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nCollecting git+https://github.com/mattjhayes3/trl.git\n  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-ce68x8hu\n  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-ce68x8hu\n  Resolved https://github.com/mattjhayes3/trl.git to commit 4c8e35e0e0a6b90b4b7c506cce7e31eb51b170b9\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.41.1)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.30.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.19.1)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.3.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.23.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.8.7.dev0-py3-none-any.whl size=209532 sha256=ebf4813cfada60782191985d117d1217af823f443402fbed76f6d128b6a6afb4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ca14ksw5/wheels/b5/f5/74/f982e27bdb1c23b205f8bfcaaed174cf3d2c06bd1a5f5926cc\nSuccessfully built trl\nInstalling collected packages: trl\nSuccessfully installed trl-0.8.7.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf */optimizer.pt && python dpo.py --output_dir=dpo_filtered_2c2_0_3-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_filtered_0_3_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 100     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=100 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=6","metadata":{"execution":{"iopub.status.busy":"2024-06-01T10:36:56.449131Z","iopub.execute_input":"2024-06-01T10:36:56.449546Z","iopub.status.idle":"2024-06-01T11:03:59.767536Z","shell.execute_reply.started":"2024-06-01T10:36:56.449498Z","shell.execute_reply":"2024-06-01T11:03:59.766330Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2024-06-01 10:37:02.499233: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-01 10:37:02.499301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-01 10:37:02.500815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 10022\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_103709-a5505our\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_filtered_2c2_0_3-0_2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/a5505our\u001b[0m\n  0%|                                                   | 0/450 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 4.733633995056152, 'learning_rate': 2e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -61.769798278808594, 'logps/chosen': -60.97594451904297, 'logits/rejected': -36.121891021728516, 'logits/chosen': -36.62195587158203, 'epoch': 0.01}\n{'loss': 0.6879, 'grad_norm': 4.8341474533081055, 'learning_rate': 1e-06, 'rewards/chosen': 0.004366671666502953, 'rewards/rejected': -0.006343964021652937, 'rewards/accuracies': 0.7624362111091614, 'rewards/margins': 0.010710636153817177, 'logps/rejected': -61.347320556640625, 'logps/chosen': -60.410858154296875, 'logits/rejected': -37.92112731933594, 'logits/chosen': -36.455772399902344, 'epoch': 0.67}\n{'loss': 0.6516, 'grad_norm': 4.111318588256836, 'learning_rate': 2e-06, 'rewards/chosen': 0.023831436410546303, 'rewards/rejected': -0.06429753452539444, 'rewards/accuracies': 0.8052604794502258, 'rewards/margins': 0.08812897652387619, 'logps/rejected': -61.92388153076172, 'logps/chosen': -60.48045349121094, 'logits/rejected': -37.197139739990234, 'logits/chosen': -35.76232147216797, 'epoch': 1.33}\n 22%|█████████                                | 100/450 [02:04<07:04,  1.21s/it]evaluation_loop\n\n  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.66it/s]\u001b[A\n 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.59it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6300869584083557, 'eval_runtime': 2.1589, 'eval_samples_per_second': 232.526, 'eval_steps_per_second': 1.853, 'eval_rewards/chosen': 0.018200097605586052, 'eval_rewards/rejected': -0.12020523101091385, 'eval_rewards/accuracies': 0.8153800368309021, 'eval_rewards/margins': 0.13840532302856445, 'eval_logps/rejected': -61.90637969970703, 'eval_logps/chosen': -59.700706481933594, 'eval_logits/rejected': -37.413387298583984, 'eval_logits/chosen': -35.76129913330078, 'epoch': 1.33}\n 22%|█████████                                | 100/450 [02:06<07:04,  1.21s/it]\n100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]\u001b[A\n{'loss': 0.5981, 'grad_norm': 6.594691276550293, 'learning_rate': 2e-06, 'rewards/chosen': 0.009791375137865543, 'rewards/rejected': -0.20890438556671143, 'rewards/accuracies': 0.8080729842185974, 'rewards/margins': 0.218695729970932, 'logps/rejected': -62.804141998291016, 'logps/chosen': -60.873565673828125, 'logits/rejected': -37.02384567260742, 'logits/chosen': -35.72268295288086, 'epoch': 2.0}\n{'loss': 0.5375, 'grad_norm': 3.5258870124816895, 'learning_rate': 2e-06, 'rewards/chosen': -0.043115466833114624, 'rewards/rejected': -0.43275296688079834, 'rewards/accuracies': 0.8362500071525574, 'rewards/margins': 0.38963741064071655, 'logps/rejected': -63.703617095947266, 'logps/chosen': -60.871795654296875, 'logits/rejected': -36.416160583496094, 'logits/chosen': -35.3201904296875, 'epoch': 2.67}\n 44%|██████████████████▏                      | 200/450 [04:11<05:16,  1.27s/it]evaluation_loop\n\n  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.64it/s]\u001b[A\n 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.58it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5381031632423401, 'eval_runtime': 2.16, 'eval_samples_per_second': 232.412, 'eval_steps_per_second': 1.852, 'eval_rewards/chosen': -0.11837294697761536, 'eval_rewards/rejected': -0.5232871174812317, 'eval_rewards/accuracies': 0.8111427426338196, 'eval_rewards/margins': 0.40491414070129395, 'eval_logps/rejected': -63.92178726196289, 'eval_logps/chosen': -60.38357162475586, 'eval_logits/rejected': -36.53960418701172, 'eval_logits/chosen': -35.00480270385742, 'epoch': 2.67}\n 44%|██████████████████▏                      | 200/450 [04:13<05:16,  1.27s/it]\n100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]\u001b[A\n{'loss': 0.493, 'grad_norm': 3.663849115371704, 'learning_rate': 2e-06, 'rewards/chosen': -0.1394355446100235, 'rewards/rejected': -0.6847968101501465, 'rewards/accuracies': 0.8394270539283752, 'rewards/margins': 0.545361340045929, 'logps/rejected': -64.88976287841797, 'logps/chosen': -60.973960876464844, 'logits/rejected': -36.20097351074219, 'logits/chosen': -35.132389068603516, 'epoch': 3.33}\n{'loss': 0.4566, 'grad_norm': 5.956601619720459, 'learning_rate': 2e-06, 'rewards/chosen': -0.2452520728111267, 'rewards/rejected': -0.9406946301460266, 'rewards/accuracies': 0.8439583778381348, 'rewards/margins': 0.6954425573348999, 'logps/rejected': -66.42498779296875, 'logps/chosen': -62.12311935424805, 'logits/rejected': -35.76375961303711, 'logits/chosen': -34.67682647705078, 'epoch': 4.0}\n 67%|███████████████████████████▎             | 300/450 [06:17<02:33,  1.02s/it]evaluation_loop\n\n  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.66it/s]\u001b[A\n 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.57it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4803301692008972, 'eval_runtime': 2.1607, 'eval_samples_per_second': 232.337, 'eval_steps_per_second': 1.851, 'eval_rewards/chosen': -0.35887327790260315, 'eval_rewards/rejected': -1.0151405334472656, 'eval_rewards/accuracies': 0.8129303455352783, 'eval_rewards/margins': 0.6562672257423401, 'eval_logps/rejected': -66.38105773925781, 'eval_logps/chosen': -61.5860710144043, 'eval_logits/rejected': -36.09148406982422, 'eval_logits/chosen': -34.653526306152344, 'epoch': 4.0}\n 67%|███████████████████████████▎             | 300/450 [06:19<02:33,  1.02s/it]\n100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.36it/s]\u001b[A\n{'loss': 0.4185, 'grad_norm': 3.20605206489563, 'learning_rate': 2e-06, 'rewards/chosen': -0.3460314869880676, 'rewards/rejected': -1.1999603509902954, 'rewards/accuracies': 0.8620312213897705, 'rewards/margins': 0.853928804397583, 'logps/rejected': -67.70857238769531, 'logps/chosen': -62.321990966796875, 'logits/rejected': -35.63064956665039, 'logits/chosen': -34.63776397705078, 'epoch': 4.67}\n{'loss': 0.3955, 'grad_norm': 3.0061373710632324, 'learning_rate': 2e-06, 'rewards/chosen': -0.44855132699012756, 'rewards/rejected': -1.4356253147125244, 'rewards/accuracies': 0.8680729866027832, 'rewards/margins': 0.987074077129364, 'logps/rejected': -68.55139923095703, 'logps/chosen': -62.94196319580078, 'logits/rejected': -35.4864501953125, 'logits/chosen': -34.63745880126953, 'epoch': 5.33}\n 89%|████████████████████████████████████▍    | 400/450 [08:26<01:01,  1.23s/it]evaluation_loop\n\n  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.60it/s]\u001b[A\n 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.57it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4493570625782013, 'eval_runtime': 2.1703, 'eval_samples_per_second': 231.309, 'eval_steps_per_second': 1.843, 'eval_rewards/chosen': -0.5785771608352661, 'eval_rewards/rejected': -1.4264914989471436, 'eval_rewards/accuracies': 0.8171676397323608, 'eval_rewards/margins': 0.8479143381118774, 'eval_logps/rejected': -68.43781280517578, 'eval_logps/chosen': -62.684593200683594, 'eval_logits/rejected': -35.78302764892578, 'eval_logits/chosen': -34.39310073852539, 'epoch': 5.33}\n 89%|████████████████████████████████████▍    | 400/450 [08:28<01:01,  1.23s/it]\n100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.36it/s]\u001b[A\n{'loss': 0.373, 'grad_norm': 4.7620015144348145, 'learning_rate': 2e-06, 'rewards/chosen': -0.5319710969924927, 'rewards/rejected': -1.6304311752319336, 'rewards/accuracies': 0.877291738986969, 'rewards/margins': 1.098460078239441, 'logps/rejected': -69.78273010253906, 'logps/chosen': -63.280757904052734, 'logits/rejected': -35.51423263549805, 'logits/chosen': -34.808738708496094, 'epoch': 6.0}\n{'train_runtime': 588.7216, 'train_samples_per_second': 97.024, 'train_steps_per_second': 0.764, 'train_loss': 0.5124135918087429, 'epoch': 6.0}\n100%|█████████████████████████████████████████| 450/450 [09:31<00:00,  1.27s/it]\nEvaling epochs [6, 5, 4, 3]\nLoading from dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-450\nDownloading readme: 100%|██████████████████| 7.81k/7.81k [00:00<00:00, 14.3MB/s]\nDownloading data: 100%|████████████████████| 21.0M/21.0M [00:00<00:00, 63.1MB/s]\nDownloading data: 100%|████████████████████| 20.5M/20.5M [00:00<00:00, 80.1MB/s]\nDownloading data: 100%|█████████████████████| 42.0M/42.0M [00:00<00:00, 135MB/s]\nGenerating train split: 100%|██| 25000/25000 [00:00<00:00, 140615.77 examples/s]\nGenerating test split: 100%|███| 25000/25000 [00:00<00:00, 191060.09 examples/s]\nGenerating unsupervised split: 100%|█| 50000/50000 [00:00<00:00, 184818.49 examp\nFilter: 100%|███████████████████████| 250/250 [00:00<00:00, 17032.57 examples/s]\nMap:   0%|                                       | 0/250 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|████████████████████████████| 250/250 [00:00<00:00, 822.64 examples/s]\nload ref model lvwerra/gpt2-imdb\nload train model dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-450\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 2.09MB/s]\npytorch_model.bin: 100%|████████████████████| 1.42G/1.42G [00:03<00:00, 378MB/s]\ntokenizer_config.json: 100%|████████████████████| 256/256 [00:00<00:00, 772kB/s]\nvocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 2.43MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.88MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 150/150 [00:00<00:00, 661kB/s]\neval batch size 256\nFilter: 100%|████████████████████| 2500/2500 [00:00<00:00, 132179.00 examples/s]\nMap:   3%|▉                           | 83/2487 [00:00<00:02, 812.66 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|██████████████████████████| 2487/2487 [00:04<00:00, 590.76 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:33, 26.70s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:53<03:05, 26.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:39, 26.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:12<01:46, 26.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n 67%|██████████████████████████████               | 6/9 [02:39<01:19, 26.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:06<00:53, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:32<00:26, 26.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [03:59<00:00, 26.56s/it]\nmean test reward 0.856337828077042 +/- 0.007195213361890524 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.998697817325592 from 0.00922983093187213\nmean KL 2.074786619438479 +/- 0.09323982380386457 full 5.102978884604656 +/- 0.03809978076530731\nmedian KL 2.665057897567749 full 4.84456205368042\nLoading from dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-375\nload ref model lvwerra/gpt2-imdb\nload train model dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-375\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:30, 26.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:52<03:04, 26.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:38, 26.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:12<01:46, 26.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 67%|██████████████████████████████               | 6/9 [02:39<01:19, 26.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:05<00:53, 26.60s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:32<00:26, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [03:59<00:00, 26.59s/it]\nmean test reward 0.8301084394966615 +/- 0.007719157132597479 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9986384510993958 from 0.00922983093187213\nmean KL 2.1930691867471777 +/- 0.08339395628566287 full 4.285773716090868 +/- 0.03425208006429828\nmedian KL 2.603592276573181 full 4.042970418930054\nLoading from dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-300\nload ref model lvwerra/gpt2-imdb\nload train model dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-300\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:30, 26.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:52<03:04, 26.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:38, 26.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:12<01:46, 26.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 67%|██████████████████████████████               | 6/9 [02:38<01:19, 26.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:05<00:52, 26.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:31<00:26, 26.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [03:58<00:00, 26.47s/it]\nmean test reward 0.795813927594016 +/- 0.00829535151459212 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9984965324401855 from 0.00922983093187213\nmean KL 1.9904728444331947 +/- 0.06755979294520294 full 3.280058358097449 +/- 0.02806323810149199\nmedian KL 2.3222235441207886 full 3.056312680244446\nLoading from dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-225\nload ref model lvwerra/gpt2-imdb\nload train model dpo_filtered_2c2_0_3-0_2-2024.06.01.10.37/checkpoint-225\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:31, 26.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:52<03:04, 26.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:38, 26.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:12<01:46, 26.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 67%|██████████████████████████████               | 6/9 [02:39<01:19, 26.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:05<00:53, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:32<00:26, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [03:58<00:00, 26.53s/it]\nmean test reward 0.7525595069132325 +/- 0.008889865606914255 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9981070756912231 from 0.00922983093187213\nmean KL 1.412628876853786 +/- 0.05093463993716973 full 2.0608412671459115 +/- 0.018621356808975552\nmedian KL 1.6372949481010437 full 1.906242847442627\ndpo_filtered_2c2_0_3-0_2-2024.06.01.10.37,0.2,epoch 6,0.856337828077042,0.007195213361890524,5.102978884604656,0.03809978076530731\ndpo_filtered_2c2_0_3-0_2-2024.06.01.10.37,0.2,epoch 5,0.8301084394966615,0.007719157132597479,4.285773716090868,0.03425208006429828\ndpo_filtered_2c2_0_3-0_2-2024.06.01.10.37,0.2,epoch 4,0.795813927594016,0.00829535151459212,3.280058358097449,0.02806323810149199\ndpo_filtered_2c2_0_3-0_2-2024.06.01.10.37,0.2,epoch 3,0.7525595069132325,0.008889865606914255,2.0608412671459115,0.018621356808975552\n","output_type":"stream"}]},{"cell_type":"code","source":"!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_flipped_2c2_0_2-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_flipped_0_2_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4","metadata":{"execution":{"iopub.status.busy":"2024-06-01T11:15:42.505814Z","iopub.execute_input":"2024-06-01T11:15:42.506345Z","iopub.status.idle":"2024-06-01T11:48:59.835655Z","shell.execute_reply.started":"2024-06-01T11:15:42.506299Z","shell.execute_reply":"2024-06-01T11:48:59.834526Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"No local changes to save\nAlready up to date.\nCurrent branch main is up to date.\n2024-06-01 11:15:49.587410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-01 11:15:49.587477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-01 11:15:49.589014: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 24895\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240601_111556-nco1xoa1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_flipped_2c2_0_2-0_2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/nco1xoa1\u001b[0m\n  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 4.703707695007324, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -64.33436584472656, 'logps/chosen': -59.26305389404297, 'logits/rejected': -35.94441604614258, 'logits/chosen': -35.62873077392578, 'epoch': 0.01}\n{'loss': 0.6928, 'grad_norm': 4.363811492919922, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.00032509095035493374, 'rewards/rejected': -0.00047224288573488593, 'rewards/accuracies': 0.545918345451355, 'rewards/margins': 0.0007973338942974806, 'logps/rejected': -60.521278381347656, 'logps/chosen': -60.36626434326172, 'logits/rejected': -37.11806869506836, 'logits/chosen': -36.64082717895508, 'epoch': 0.27}\n{'loss': 0.6906, 'grad_norm': 4.560818672180176, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0012234342284500599, 'rewards/rejected': -0.003997547551989555, 'rewards/accuracies': 0.569531261920929, 'rewards/margins': 0.005220981314778328, 'logps/rejected': -60.706153869628906, 'logps/chosen': -59.9243049621582, 'logits/rejected': -37.105194091796875, 'logits/chosen': -36.76346206665039, 'epoch': 0.54}\n{'loss': 0.6873, 'grad_norm': 4.621930122375488, 'learning_rate': 2e-06, 'rewards/chosen': 0.0004915273166261613, 'rewards/rejected': -0.012381959706544876, 'rewards/accuracies': 0.5724999904632568, 'rewards/margins': 0.01287348847836256, 'logps/rejected': -60.5232048034668, 'logps/chosen': -60.2462158203125, 'logits/rejected': -36.983558654785156, 'logits/chosen': -36.48396301269531, 'epoch': 0.81}\n 20%|████████▎                                | 150/740 [03:04<11:59,  1.22s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.71it/s]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6843000054359436, 'eval_runtime': 5.6479, 'eval_samples_per_second': 220.435, 'eval_steps_per_second': 1.771, 'eval_rewards/chosen': 0.002762965392321348, 'eval_rewards/rejected': -0.017503628507256508, 'eval_rewards/accuracies': 0.5937163829803467, 'eval_rewards/margins': 0.020266592502593994, 'eval_logps/rejected': -60.93873977661133, 'eval_logps/chosen': -60.442169189453125, 'eval_logits/rejected': -36.717403411865234, 'eval_logits/chosen': -36.20252227783203, 'epoch': 0.81}\n 20%|████████▎                                | 150/740 [03:10<11:59,  1.22s/it]\n100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\n{'loss': 0.6799, 'grad_norm': 4.354125499725342, 'learning_rate': 2e-06, 'rewards/chosen': -0.0023699095472693443, 'rewards/rejected': -0.03246177360415459, 'rewards/accuracies': 0.5895663499832153, 'rewards/margins': 0.030091863125562668, 'logps/rejected': -60.554168701171875, 'logps/chosen': -60.104488372802734, 'logits/rejected': -36.973201751708984, 'logits/chosen': -36.79351043701172, 'epoch': 1.08}\n{'loss': 0.6655, 'grad_norm': 4.394620895385742, 'learning_rate': 2e-06, 'rewards/chosen': 0.0005062227719463408, 'rewards/rejected': -0.06171892583370209, 'rewards/accuracies': 0.6534374952316284, 'rewards/margins': 0.06222515180706978, 'logps/rejected': -60.658668518066406, 'logps/chosen': -60.02122497558594, 'logits/rejected': -36.78510665893555, 'logits/chosen': -36.459529876708984, 'epoch': 1.35}\n{'loss': 0.6635, 'grad_norm': 4.38765811920166, 'learning_rate': 2e-06, 'rewards/chosen': -0.011800502426922321, 'rewards/rejected': -0.08194126188755035, 'rewards/accuracies': 0.6385937333106995, 'rewards/margins': 0.07014074921607971, 'logps/rejected': -60.79818344116211, 'logps/chosen': -60.102394104003906, 'logits/rejected': -36.79824447631836, 'logits/chosen': -36.29594039916992, 'epoch': 1.62}\n 41%|████████████████▌                        | 300/740 [06:15<09:06,  1.24s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.83it/s]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.81it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.81it/s]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6762643456459045, 'eval_runtime': 5.5496, 'eval_samples_per_second': 224.339, 'eval_steps_per_second': 1.802, 'eval_rewards/chosen': -0.02440045401453972, 'eval_rewards/rejected': -0.06955770403146744, 'eval_rewards/accuracies': 0.5767220854759216, 'eval_rewards/margins': 0.04515724629163742, 'eval_logps/rejected': -61.199012756347656, 'eval_logps/chosen': -60.57798385620117, 'eval_logits/rejected': -36.224159240722656, 'eval_logits/chosen': -35.723140716552734, 'epoch': 1.62}\n 41%|████████████████▌                        | 300/740 [06:21<09:06,  1.24s/it]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\n{'loss': 0.6632, 'grad_norm': 4.207848072052002, 'learning_rate': 2e-06, 'rewards/chosen': -0.025378653779625893, 'rewards/rejected': -0.09917651116847992, 'rewards/accuracies': 0.6226562261581421, 'rewards/margins': 0.07379785180091858, 'logps/rejected': -61.183475494384766, 'logps/chosen': -60.218624114990234, 'logits/rejected': -36.329673767089844, 'logits/chosen': -35.96303176879883, 'epoch': 1.89}\n{'loss': 0.6452, 'grad_norm': 3.975512742996216, 'learning_rate': 2e-06, 'rewards/chosen': -0.022613931447267532, 'rewards/rejected': -0.13727813959121704, 'rewards/accuracies': 0.671065092086792, 'rewards/margins': 0.11466421186923981, 'logps/rejected': -61.16728591918945, 'logps/chosen': -60.289161682128906, 'logits/rejected': -36.4532356262207, 'logits/chosen': -35.927738189697266, 'epoch': 2.16}\n{'loss': 0.6341, 'grad_norm': 4.318473815917969, 'learning_rate': 2e-06, 'rewards/chosen': -0.0206484105437994, 'rewards/rejected': -0.16286388039588928, 'rewards/accuracies': 0.6970312595367432, 'rewards/margins': 0.14221544563770294, 'logps/rejected': -61.294090270996094, 'logps/chosen': -60.39613342285156, 'logits/rejected': -36.3321647644043, 'logits/chosen': -36.048988342285156, 'epoch': 2.43}\n 61%|████████████████████████▉                | 450/740 [09:26<06:00,  1.24s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.81it/s]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.81it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.81it/s]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.81it/s]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6718258857727051, 'eval_runtime': 5.5583, 'eval_samples_per_second': 223.987, 'eval_steps_per_second': 1.799, 'eval_rewards/chosen': -0.05825827270746231, 'eval_rewards/rejected': -0.12233857810497284, 'eval_rewards/accuracies': 0.5833669900894165, 'eval_rewards/margins': 0.06408031284809113, 'eval_logps/rejected': -61.462913513183594, 'eval_logps/chosen': -60.747276306152344, 'eval_logits/rejected': -36.03330612182617, 'eval_logits/chosen': -35.54090881347656, 'epoch': 2.43}\n 61%|████████████████████████▉                | 450/740 [09:32<06:00,  1.24s/it]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\n{'loss': 0.6363, 'grad_norm': 4.036654949188232, 'learning_rate': 2e-06, 'rewards/chosen': -0.03676002100110054, 'rewards/rejected': -0.17778882384300232, 'rewards/accuracies': 0.6725000143051147, 'rewards/margins': 0.14102880656719208, 'logps/rejected': -61.47830581665039, 'logps/chosen': -60.48624038696289, 'logits/rejected': -36.348880767822266, 'logits/chosen': -35.85807418823242, 'epoch': 2.7}\n{'loss': 0.6353, 'grad_norm': 4.5055108070373535, 'learning_rate': 2e-06, 'rewards/chosen': -0.0432661734521389, 'rewards/rejected': -0.18829181790351868, 'rewards/accuracies': 0.6776562333106995, 'rewards/margins': 0.14502565562725067, 'logps/rejected': -61.35700988769531, 'logps/chosen': -60.34310531616211, 'logits/rejected': -36.032257080078125, 'logits/chosen': -35.602657318115234, 'epoch': 2.97}\n{'loss': 0.6147, 'grad_norm': 4.150722026824951, 'learning_rate': 2e-06, 'rewards/chosen': -0.032855819910764694, 'rewards/rejected': -0.22535760700702667, 'rewards/accuracies': 0.7155134081840515, 'rewards/margins': 0.19250179827213287, 'logps/rejected': -61.612548828125, 'logps/chosen': -60.17948913574219, 'logits/rejected': -35.983726501464844, 'logits/chosen': -35.487796783447266, 'epoch': 3.24}\n 81%|█████████████████████████████████▏       | 600/740 [12:38<02:54,  1.25s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6704995632171631, 'eval_runtime': 5.3955, 'eval_samples_per_second': 230.749, 'eval_steps_per_second': 1.853, 'eval_rewards/chosen': -0.08477069437503815, 'eval_rewards/rejected': -0.15899226069450378, 'eval_rewards/accuracies': 0.5816028118133545, 'eval_rewards/margins': 0.07422155141830444, 'eval_logps/rejected': -61.64618682861328, 'eval_logps/chosen': -60.87984085083008, 'eval_logits/rejected': -35.791683197021484, 'eval_logits/chosen': -35.31986618041992, 'epoch': 3.24}\n 81%|█████████████████████████████████▏       | 600/740 [12:44<02:54,  1.25s/it]\n100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.6101, 'grad_norm': 4.023021697998047, 'learning_rate': 2e-06, 'rewards/chosen': -0.03952985629439354, 'rewards/rejected': -0.2439718097448349, 'rewards/accuracies': 0.7303125262260437, 'rewards/margins': 0.20444194972515106, 'logps/rejected': -61.68549728393555, 'logps/chosen': -60.7164192199707, 'logits/rejected': -35.956817626953125, 'logits/chosen': -35.606998443603516, 'epoch': 3.51}\n{'loss': 0.6096, 'grad_norm': 4.126832962036133, 'learning_rate': 2e-06, 'rewards/chosen': -0.04773921146988869, 'rewards/rejected': -0.25709980726242065, 'rewards/accuracies': 0.7176562547683716, 'rewards/margins': 0.20936059951782227, 'logps/rejected': -61.88922882080078, 'logps/chosen': -60.247344970703125, 'logits/rejected': -36.04386901855469, 'logits/chosen': -35.668724060058594, 'epoch': 3.78}\n{'train_runtime': 954.8125, 'train_samples_per_second': 99.077, 'train_steps_per_second': 0.775, 'train_loss': 0.649648420875137, 'epoch': 4.0}\n100%|█████████████████████████████████████████| 740/740 [15:37<00:00,  1.27s/it]\nEvaling epochs [4, 3, 2, 1]\nLoading from dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-740\nload ref model lvwerra/gpt2-imdb\nload train model dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-740\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:33, 26.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:53<03:05, 26.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:39, 26.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:13, 26.69s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:13<01:46, 26.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:06<00:53, 26.75s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:33<00:26, 26.72s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [04:00<00:00, 26.72s/it]\nmean test reward 0.6595852784186036 +/- 0.009744839968932375 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9963377714157104 from 0.00922983093187213\nmean KL 0.3860770042778717 +/- 0.03351895019034439 full 0.9217889571575344 +/- 0.007880677727947118\nmedian KL 0.5718885362148285 full 0.8425825834274292\nLoading from dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-555\nload ref model lvwerra/gpt2-imdb\nload train model dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-555\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:32, 26.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:53<03:06, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:40, 26.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:13, 26.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:13<01:46, 26.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.75s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:33<00:26, 26.75s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [04:00<00:00, 26.77s/it]\nmean test reward 0.6352944832068993 +/- 0.009905568728247152 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9955103397369385 from 0.00922983093187213\nmean KL 0.46295528424606247 +/- 0.02771722100343255 full 0.7009950936513228 +/- 0.006520612314275733\nmedian KL 0.541321337223053 full 0.6340689361095428\nLoading from dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-370\nload ref model lvwerra/gpt2-imdb\nload train model dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-370\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:32, 26.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:53<03:05, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:19<02:39, 26.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:46<02:13, 26.79s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:13<01:47, 26.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:34<00:26, 26.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [04:00<00:00, 26.77s/it]\nmean test reward 0.5942674961958372 +/- 0.01010989865047246 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9935219287872314 from 0.00922983093187213\nmean KL 0.45370092274000245 +/- 0.021141145013987648 full 0.43239188926397926 +/- 0.0043217007864764815\nmedian KL 0.47785887122154236 full 0.3912496268749237\nLoading from dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-185\nload ref model lvwerra/gpt2-imdb\nload train model dpo_flipped_2c2_0_2-0_2-2024.06.01.11.15/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|█████                                        | 1/9 [00:26<03:32, 26.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|██████████                                   | 2/9 [00:53<03:06, 26.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|███████████████                              | 3/9 [01:20<02:40, 26.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|████████████████████                         | 4/9 [01:47<02:14, 26.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|█████████████████████████                    | 5/9 [02:13<01:47, 26.79s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|████████████████████████████████████████     | 8/9 [03:34<00:26, 26.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|█████████████████████████████████████████████| 9/9 [04:01<00:00, 26.83s/it]\nmean test reward 0.5084433197194786 +/- 0.010297633241556919 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9584429860115051 from 0.00922983093187213\nmean KL 0.21154487554869977 +/- 0.010590103258266835 full 0.11326032105150968 +/- 0.001295991414768945\nmedian KL 0.20342065393924713 full 0.09956524148583412\ndpo_flipped_2c2_0_2-0_2-2024.06.01.11.15,0.2,epoch 4,0.6595852784186036,0.009744839968932375,0.9217889571575344,0.007880677727947118\ndpo_flipped_2c2_0_2-0_2-2024.06.01.11.15,0.2,epoch 3,0.6352944832068993,0.009905568728247152,0.7009950936513228,0.006520612314275733\ndpo_flipped_2c2_0_2-0_2-2024.06.01.11.15,0.2,epoch 2,0.5942674961958372,0.01010989865047246,0.43239188926397926,0.0043217007864764815\ndpo_flipped_2c2_0_2-0_2-2024.06.01.11.15,0.2,epoch 1,0.5084433197194786,0.010297633241556919,0.11326032105150968,0.001295991414768945\n","output_type":"stream"}]}]}