{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !conda install -y gdown\n# print('hi')\n!git clone https://github.com/mattjhayes3/cs234-project.git\n%cd /kaggle/working/cs234-project\n!pip install wandb\nimport wandb\n\nwandb.login(key=\"KEY\")\n# wandb.init()\n\nfrom huggingface_hub import notebook_login,login\n\n# notebook_login(\"KEY\")\nlogin(\"KEY\")\n\n# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n!mkdir results\n!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n!pip install git+https://github.com/mattjhayes3/trl.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T09:39:39.081315Z","iopub.execute_input":"2024-05-27T09:39:39.081667Z","iopub.status.idle":"2024-05-27T09:40:40.379293Z","shell.execute_reply.started":"2024-05-27T09:39:39.081637Z","shell.execute_reply":"2024-05-27T09:40:40.378235Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'cs234-project' already exists and is not an empty directory.\n/kaggle/working/cs234-project\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nmkdir: cannot create directory 'results': File exists\nCollecting git+https://github.com/mattjhayes3/trl.git\n  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-3z7z3wpd\n  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-3z7z3wpd\n  Resolved https://github.com/mattjhayes3/trl.git to commit 0f327a2feb6a1bba0c9494205875bea5c868ed75\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.39.3)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.29.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.18.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.1)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.8.7.dev0-py3-none-any.whl size=209518 sha256=7c4543df12a2c6fa5880f0260223daa4bc9f097f7dbc100e320593e97483cc88\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ghrha1al/wheels/b5/f5/74/f982e27bdb1c23b205f8bfcaaed174cf3d2c06bd1a5f5926cc\nSuccessfully built trl\nInstalling collected packages: trl\nSuccessfully installed trl-0.8.7.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=test_dpo_0_6_lr1e-3  --beta=0.6 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_tokenized_split     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 1e-3     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=500 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch'","metadata":{"execution":{"iopub.status.busy":"2024-05-27T02:33:13.105369Z","iopub.execute_input":"2024-05-27T02:33:13.105632Z","iopub.status.idle":"2024-05-27T03:37:16.319660Z","shell.execute_reply.started":"2024-05-27T02:33:13.105604Z","shell.execute_reply":"2024-05-27T03:37:16.318615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2024-05-27 02:33:23.396038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 02:33:23.396191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 02:33:23.519661: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|█████████████████████████████| 577/577 [00:00<00:00, 3.89MB/s]\npytorch_model.bin: 100%|█████████████████████| 548M/548M [00:13<00:00, 40.9MB/s]\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\ntokenizer_config.json: 100%|██████████████████| 17.0/17.0 [00:00<00:00, 110kB/s]\nvocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 13.0MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 9.24MB/s]\nspecial_tokens_map.json: 100%|████████████████| 90.0/90.0 [00:00<00:00, 521kB/s]\nds len 149370\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_023350-6bet3pct\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest_dpo_0_6_lr1e-3\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/6bet3pct\u001b[0m\n  0%|                                                  | 0/3327 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 13.237253189086914, 'learning_rate': 6.6666666666666675e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -58.112220764160156, 'logps/chosen': -60.65317153930664, 'logits/rejected': -39.777442932128906, 'logits/chosen': -37.3552131652832, 'epoch': 0.0}\n{'loss': 0.7286, 'grad_norm': 10.923274993896484, 'learning_rate': 0.0003333333333333333, 'rewards/chosen': -1.363308072090149, 'rewards/rejected': -2.2158474922180176, 'rewards/accuracies': 0.6521046161651611, 'rewards/margins': 0.852539598941803, 'logps/rejected': -64.4424819946289, 'logps/chosen': -62.769325256347656, 'logits/rejected': -33.725276947021484, 'logits/chosen': -33.073822021484375, 'epoch': 0.05}\n{'loss': 1.4765, 'grad_norm': 8.61414623260498, 'learning_rate': 0.0006666666666666666, 'rewards/chosen': -8.018301010131836, 'rewards/rejected': -9.71589183807373, 'rewards/accuracies': 0.6251562237739563, 'rewards/margins': 1.6975895166397095, 'logps/rejected': -76.5928726196289, 'logps/chosen': -73.4997787475586, 'logits/rejected': -11.093091011047363, 'logits/chosen': -10.97346305847168, 'epoch': 0.09}\n{'loss': 2.0511, 'grad_norm': 5.955862998962402, 'learning_rate': 0.001, 'rewards/chosen': -17.415258407592773, 'rewards/rejected': -20.022701263427734, 'rewards/accuracies': 0.6317187547683716, 'rewards/margins': 2.6074423789978027, 'logps/rejected': -94.41931915283203, 'logps/chosen': -89.06204223632812, 'logits/rejected': -18.071256637573242, 'logits/chosen': -18.212387084960938, 'epoch': 0.14}\n{'loss': 2.4565, 'grad_norm': 8.939168930053711, 'learning_rate': 0.001, 'rewards/chosen': -25.849193572998047, 'rewards/rejected': -28.866165161132812, 'rewards/accuracies': 0.6278125047683716, 'rewards/margins': 3.016969680786133, 'logps/rejected': -108.303955078125, 'logps/chosen': -103.2724380493164, 'logits/rejected': -7.346400260925293, 'logits/chosen': -7.505399703979492, 'epoch': 0.18}\n{'loss': 3.0326, 'grad_norm': 3.792712926864624, 'learning_rate': 0.001, 'rewards/chosen': -34.10637664794922, 'rewards/rejected': -36.22922897338867, 'rewards/accuracies': 0.5914062261581421, 'rewards/margins': 2.122853994369507, 'logps/rejected': -120.6994400024414, 'logps/chosen': -116.86697387695312, 'logits/rejected': -10.524086952209473, 'logits/chosen': -10.644303321838379, 'epoch': 0.23}\n{'loss': 3.0885, 'grad_norm': 11.444540977478027, 'learning_rate': 0.001, 'rewards/chosen': -36.43047332763672, 'rewards/rejected': -38.37213134765625, 'rewards/accuracies': 0.5771874785423279, 'rewards/margins': 1.9416590929031372, 'logps/rejected': -124.53104400634766, 'logps/chosen': -120.75899505615234, 'logits/rejected': -8.282849311828613, 'logits/chosen': -8.31926441192627, 'epoch': 0.27}\n{'loss': 2.9791, 'grad_norm': 8.770522117614746, 'learning_rate': 0.001, 'rewards/chosen': -36.36014175415039, 'rewards/rejected': -38.527408599853516, 'rewards/accuracies': 0.5918750166893005, 'rewards/margins': 2.1672699451446533, 'logps/rejected': -124.66438293457031, 'logps/chosen': -120.47534942626953, 'logits/rejected': -7.320728302001953, 'logits/chosen': -7.3467302322387695, 'epoch': 0.32}\n{'loss': 2.9321, 'grad_norm': 5.208885192871094, 'learning_rate': 0.001, 'rewards/chosen': -40.170570373535156, 'rewards/rejected': -42.5223503112793, 'rewards/accuracies': 0.5942187309265137, 'rewards/margins': 2.35178279876709, 'logps/rejected': -131.2322998046875, 'logps/chosen': -127.18028259277344, 'logits/rejected': -5.6303558349609375, 'logits/chosen': -5.543367862701416, 'epoch': 0.36}\n{'loss': 2.751, 'grad_norm': 4.790102005004883, 'learning_rate': 0.001, 'rewards/chosen': -38.760414123535156, 'rewards/rejected': -41.6583366394043, 'rewards/accuracies': 0.6159374713897705, 'rewards/margins': 2.8979129791259766, 'logps/rejected': -129.96600341796875, 'logps/chosen': -124.81903076171875, 'logits/rejected': -6.613058567047119, 'logits/chosen': -6.644672870635986, 'epoch': 0.41}\n{'loss': 2.5994, 'grad_norm': 3.6330008506774902, 'learning_rate': 0.001, 'rewards/chosen': -36.899940490722656, 'rewards/rejected': -40.133811950683594, 'rewards/accuracies': 0.62890625, 'rewards/margins': 3.233874559402466, 'logps/rejected': -127.21741485595703, 'logps/chosen': -121.82415771484375, 'logits/rejected': -5.884768009185791, 'logits/chosen': -5.8736748695373535, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [08:59<50:52,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.64it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.57it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.18it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.04it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.96it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.85it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.84it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.83it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.80it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.81it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.81it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.80it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.81it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:29<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.76it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.77it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.626530170440674, 'eval_runtime': 32.5119, 'eval_samples_per_second': 229.731, 'eval_steps_per_second': 1.815, 'eval_rewards/chosen': -37.131290435791016, 'eval_rewards/rejected': -40.195865631103516, 'eval_rewards/accuracies': 0.6321445107460022, 'eval_rewards/margins': 3.0645785331726074, 'eval_logps/rejected': -127.25997161865234, 'eval_logps/chosen': -121.85511016845703, 'eval_logits/rejected': -5.155966758728027, 'eval_logits/chosen': -5.123490333557129, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:31<50:52,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.20it/s]\u001b[A\n{'loss': 2.624, 'grad_norm': 5.451295852661133, 'learning_rate': 0.001, 'rewards/chosen': -37.735511779785156, 'rewards/rejected': -41.03172302246094, 'rewards/accuracies': 0.6285937428474426, 'rewards/margins': 3.296215534210205, 'logps/rejected': -128.94113159179688, 'logps/chosen': -123.26844787597656, 'logits/rejected': -5.660406112670898, 'logits/chosen': -5.543316841125488, 'epoch': 0.5}\n{'loss': 2.4885, 'grad_norm': 4.578028678894043, 'learning_rate': 0.001, 'rewards/chosen': -38.24623489379883, 'rewards/rejected': -41.67330551147461, 'rewards/accuracies': 0.6351562738418579, 'rewards/margins': 3.4270753860473633, 'logps/rejected': -130.15650939941406, 'logps/chosen': -123.90025329589844, 'logits/rejected': -5.499856948852539, 'logits/chosen': -5.722431659698486, 'epoch': 0.54}\n{'loss': 2.5639, 'grad_norm': 3.3022124767303467, 'learning_rate': 0.001, 'rewards/chosen': -38.07017517089844, 'rewards/rejected': -41.39362335205078, 'rewards/accuracies': 0.6332812309265137, 'rewards/margins': 3.3234500885009766, 'logps/rejected': -129.82992553710938, 'logps/chosen': -123.4373550415039, 'logits/rejected': -5.919851779937744, 'logits/chosen': -6.045426845550537, 'epoch': 0.59}\n{'loss': 2.4176, 'grad_norm': 3.924232006072998, 'learning_rate': 0.001, 'rewards/chosen': -38.414695739746094, 'rewards/rejected': -42.136627197265625, 'rewards/accuracies': 0.6485937237739563, 'rewards/margins': 3.7219295501708984, 'logps/rejected': -130.52931213378906, 'logps/chosen': -124.38471984863281, 'logits/rejected': -4.677895545959473, 'logits/chosen': -4.860055923461914, 'epoch': 0.63}\n{'loss': 2.2673, 'grad_norm': 3.7519350051879883, 'learning_rate': 0.001, 'rewards/chosen': -38.158077239990234, 'rewards/rejected': -42.14521789550781, 'rewards/accuracies': 0.6607812643051147, 'rewards/margins': 3.9871413707733154, 'logps/rejected': -130.24412536621094, 'logps/chosen': -123.85079956054688, 'logits/rejected': -6.911339282989502, 'logits/chosen': -6.922461032867432, 'epoch': 0.68}\n{'loss': 2.3971, 'grad_norm': 3.1695926189422607, 'learning_rate': 0.001, 'rewards/chosen': -37.58619689941406, 'rewards/rejected': -41.22748565673828, 'rewards/accuracies': 0.645312488079071, 'rewards/margins': 3.641282320022583, 'logps/rejected': -128.82940673828125, 'logps/chosen': -122.60311889648438, 'logits/rejected': -4.29320764541626, 'logits/chosen': -4.3503289222717285, 'epoch': 0.72}\n{'loss': 2.3192, 'grad_norm': 3.5258536338806152, 'learning_rate': 0.001, 'rewards/chosen': -39.343631744384766, 'rewards/rejected': -43.40037536621094, 'rewards/accuracies': 0.6568750143051147, 'rewards/margins': 4.056748390197754, 'logps/rejected': -132.8638153076172, 'logps/chosen': -126.0182113647461, 'logits/rejected': -5.616447925567627, 'logits/chosen': -5.6923909187316895, 'epoch': 0.77}\n{'loss': 2.2949, 'grad_norm': 4.923637866973877, 'learning_rate': 0.001, 'rewards/chosen': -36.267127990722656, 'rewards/rejected': -39.82022476196289, 'rewards/accuracies': 0.6459375023841858, 'rewards/margins': 3.553097724914551, 'logps/rejected': -126.71585845947266, 'logps/chosen': -120.939208984375, 'logits/rejected': -4.821224689483643, 'logits/chosen': -4.900295257568359, 'epoch': 0.81}\n{'loss': 2.1324, 'grad_norm': 2.9515137672424316, 'learning_rate': 0.001, 'rewards/chosen': -38.09010314941406, 'rewards/rejected': -42.29143142700195, 'rewards/accuracies': 0.6645312309265137, 'rewards/margins': 4.201333045959473, 'logps/rejected': -130.85182189941406, 'logps/chosen': -123.4538345336914, 'logits/rejected': -5.156492710113525, 'logits/chosen': -5.2963714599609375, 'epoch': 0.86}\n{'loss': 2.1256, 'grad_norm': 3.900725841522217, 'learning_rate': 0.001, 'rewards/chosen': -38.64373016357422, 'rewards/rejected': -43.07957458496094, 'rewards/accuracies': 0.6742187738418579, 'rewards/margins': 4.435845375061035, 'logps/rejected': -132.2371826171875, 'logps/chosen': -124.2754135131836, 'logits/rejected': -5.41811990737915, 'logits/chosen': -5.686873912811279, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [18:30<41:33,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.85it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.81it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.82it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.81it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.76it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.2004308700561523, 'eval_runtime': 32.5465, 'eval_samples_per_second': 229.487, 'eval_steps_per_second': 1.813, 'eval_rewards/chosen': -38.949432373046875, 'eval_rewards/rejected': -43.26127243041992, 'eval_rewards/accuracies': 0.6668667793273926, 'eval_rewards/margins': 4.311839580535889, 'eval_logps/rejected': -132.3689727783203, 'eval_logps/chosen': -124.88534545898438, 'eval_logits/rejected': -8.618050575256348, 'eval_logits/chosen': -8.976847648620605, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [19:03<41:33,  1.07s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 2.054, 'grad_norm': 4.096019744873047, 'learning_rate': 0.001, 'rewards/chosen': -37.137847900390625, 'rewards/rejected': -41.371036529541016, 'rewards/accuracies': 0.6664062738418579, 'rewards/margins': 4.233191967010498, 'logps/rejected': -129.63070678710938, 'logps/chosen': -121.97530364990234, 'logits/rejected': -4.913091659545898, 'logits/chosen': -5.165886402130127, 'epoch': 0.95}\n{'loss': 1.9672, 'grad_norm': 5.124836444854736, 'learning_rate': 0.001, 'rewards/chosen': -37.52214050292969, 'rewards/rejected': -42.44491195678711, 'rewards/accuracies': 0.6887500286102295, 'rewards/margins': 4.922770977020264, 'logps/rejected': -131.16717529296875, 'logps/chosen': -122.92913055419922, 'logits/rejected': -3.2868871688842773, 'logits/chosen': -3.492014169692993, 'epoch': 0.99}\n{'loss': 1.4222, 'grad_norm': 3.047811269760132, 'learning_rate': 0.001, 'rewards/chosen': -37.487674713134766, 'rewards/rejected': -43.590126037597656, 'rewards/accuracies': 0.7501258254051208, 'rewards/margins': 6.102451801300049, 'logps/rejected': -132.6121063232422, 'logps/chosen': -122.07221984863281, 'logits/rejected': -6.324069023132324, 'logits/chosen': -6.646094799041748, 'epoch': 1.04}\n{'loss': 1.3236, 'grad_norm': 2.6757891178131104, 'learning_rate': 0.001, 'rewards/chosen': -38.30769348144531, 'rewards/rejected': -44.765132904052734, 'rewards/accuracies': 0.7653124928474426, 'rewards/margins': 6.457437038421631, 'logps/rejected': -135.3678741455078, 'logps/chosen': -124.08670806884766, 'logits/rejected': -6.109961032867432, 'logits/chosen': -6.183845043182373, 'epoch': 1.08}\n{'loss': 1.2379, 'grad_norm': 2.868166923522949, 'learning_rate': 0.001, 'rewards/chosen': -39.31125259399414, 'rewards/rejected': -45.93305206298828, 'rewards/accuracies': 0.7709375023841858, 'rewards/margins': 6.621795654296875, 'logps/rejected': -136.5593719482422, 'logps/chosen': -125.57111358642578, 'logits/rejected': -5.971700668334961, 'logits/chosen': -6.056766510009766, 'epoch': 1.13}\n{'loss': 1.3316, 'grad_norm': 3.5024044513702393, 'learning_rate': 0.001, 'rewards/chosen': -39.273006439208984, 'rewards/rejected': -45.6696662902832, 'rewards/accuracies': 0.7565624713897705, 'rewards/margins': 6.396660804748535, 'logps/rejected': -136.593017578125, 'logps/chosen': -125.54434204101562, 'logits/rejected': -5.438560962677002, 'logits/chosen': -5.657340049743652, 'epoch': 1.17}\n{'loss': 1.3393, 'grad_norm': 2.4449400901794434, 'learning_rate': 0.001, 'rewards/chosen': -39.52397918701172, 'rewards/rejected': -45.98308563232422, 'rewards/accuracies': 0.7598437666893005, 'rewards/margins': 6.459106922149658, 'logps/rejected': -137.46627807617188, 'logps/chosen': -125.92910766601562, 'logits/rejected': -6.3449602127075195, 'logits/chosen': -6.511631965637207, 'epoch': 1.22}\n{'loss': 1.3204, 'grad_norm': 3.166276693344116, 'learning_rate': 0.001, 'rewards/chosen': -38.48561096191406, 'rewards/rejected': -44.74073791503906, 'rewards/accuracies': 0.7548437714576721, 'rewards/margins': 6.255121231079102, 'logps/rejected': -135.06004333496094, 'logps/chosen': -124.77949523925781, 'logits/rejected': -3.347470760345459, 'logits/chosen': -3.466709852218628, 'epoch': 1.26}\n{'loss': 1.2657, 'grad_norm': 2.8028111457824707, 'learning_rate': 0.001, 'rewards/chosen': -38.674034118652344, 'rewards/rejected': -45.10881423950195, 'rewards/accuracies': 0.7637500166893005, 'rewards/margins': 6.434774875640869, 'logps/rejected': -135.7299346923828, 'logps/chosen': -124.483154296875, 'logits/rejected': -4.753523349761963, 'logits/chosen': -4.939445972442627, 'epoch': 1.31}\n{'loss': 1.317, 'grad_norm': 2.617023468017578, 'learning_rate': 0.001, 'rewards/chosen': -39.57310104370117, 'rewards/rejected': -45.92906188964844, 'rewards/accuracies': 0.7589062452316284, 'rewards/margins': 6.355961799621582, 'logps/rejected': -136.85775756835938, 'logps/chosen': -126.19027709960938, 'logits/rejected': -5.504896640777588, 'logits/chosen': -5.893303394317627, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:03<32:37,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.96it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.81it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.80it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.81it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.79it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.81it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.76it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.8885663747787476, 'eval_runtime': 32.5364, 'eval_samples_per_second': 229.558, 'eval_steps_per_second': 1.813, 'eval_rewards/chosen': -41.07737350463867, 'eval_rewards/rejected': -46.45658493041992, 'eval_rewards/accuracies': 0.7031073570251465, 'eval_rewards/margins': 5.379209518432617, 'eval_logps/rejected': -137.69448852539062, 'eval_logps/chosen': -128.43191528320312, 'eval_logits/rejected': -4.492290496826172, 'eval_logits/chosen': -4.92561149597168, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:36<32:37,  1.07s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 1.2816, 'grad_norm': 3.3107900619506836, 'learning_rate': 0.001, 'rewards/chosen': -39.78704071044922, 'rewards/rejected': -46.282386779785156, 'rewards/accuracies': 0.7640625238418579, 'rewards/margins': 6.495345592498779, 'logps/rejected': -137.2641143798828, 'logps/chosen': -126.42831420898438, 'logits/rejected': -3.897350549697876, 'logits/chosen': -4.399683952331543, 'epoch': 1.4}\n{'loss': 1.3241, 'grad_norm': 2.8898746967315674, 'learning_rate': 0.001, 'rewards/chosen': -38.902130126953125, 'rewards/rejected': -45.40004348754883, 'rewards/accuracies': 0.7592187523841858, 'rewards/margins': 6.497913837432861, 'logps/rejected': -136.0546112060547, 'logps/chosen': -124.69721984863281, 'logits/rejected': -6.296672344207764, 'logits/chosen': -6.738650321960449, 'epoch': 1.44}\n{'loss': 1.2889, 'grad_norm': 2.468177318572998, 'learning_rate': 0.001, 'rewards/chosen': -38.519989013671875, 'rewards/rejected': -44.8394889831543, 'rewards/accuracies': 0.7626562714576721, 'rewards/margins': 6.3195013999938965, 'logps/rejected': -135.1571502685547, 'logps/chosen': -124.29132843017578, 'logits/rejected': -4.322150707244873, 'logits/chosen': -4.5996174812316895, 'epoch': 1.49}\n{'loss': 1.3293, 'grad_norm': 2.148864507675171, 'learning_rate': 0.001, 'rewards/chosen': -39.53885269165039, 'rewards/rejected': -45.91022872924805, 'rewards/accuracies': 0.7582812309265137, 'rewards/margins': 6.371375560760498, 'logps/rejected': -136.92941284179688, 'logps/chosen': -125.75782012939453, 'logits/rejected': -4.839343070983887, 'logits/chosen': -4.952086925506592, 'epoch': 1.53}\n{'loss': 1.2586, 'grad_norm': 2.2418880462646484, 'learning_rate': 0.001, 'rewards/chosen': -39.74385452270508, 'rewards/rejected': -46.3486213684082, 'rewards/accuracies': 0.76171875, 'rewards/margins': 6.604767322540283, 'logps/rejected': -137.74415588378906, 'logps/chosen': -126.4257583618164, 'logits/rejected': -5.0229716300964355, 'logits/chosen': -5.032254695892334, 'epoch': 1.58}\n{'loss': 1.3125, 'grad_norm': 2.549635887145996, 'learning_rate': 0.001, 'rewards/chosen': -38.60869216918945, 'rewards/rejected': -44.89348602294922, 'rewards/accuracies': 0.7562500238418579, 'rewards/margins': 6.284792423248291, 'logps/rejected': -135.2996368408203, 'logps/chosen': -124.85478210449219, 'logits/rejected': -5.278273105621338, 'logits/chosen': -5.236749649047852, 'epoch': 1.62}\n{'loss': 1.2462, 'grad_norm': 2.8715224266052246, 'learning_rate': 0.001, 'rewards/chosen': -39.71932601928711, 'rewards/rejected': -46.233360290527344, 'rewards/accuracies': 0.7628124952316284, 'rewards/margins': 6.514031887054443, 'logps/rejected': -137.802978515625, 'logps/chosen': -126.32101440429688, 'logits/rejected': -3.633107900619507, 'logits/chosen': -3.7037322521209717, 'epoch': 1.67}\n{'loss': 1.2928, 'grad_norm': 2.922724723815918, 'learning_rate': 0.001, 'rewards/chosen': -39.95521545410156, 'rewards/rejected': -46.4625244140625, 'rewards/accuracies': 0.7645312547683716, 'rewards/margins': 6.507309436798096, 'logps/rejected': -138.01365661621094, 'logps/chosen': -126.86380767822266, 'logits/rejected': -4.088380813598633, 'logits/chosen': -4.045665740966797, 'epoch': 1.71}\n{'loss': 1.2634, 'grad_norm': 2.5497281551361084, 'learning_rate': 0.001, 'rewards/chosen': -39.058780670166016, 'rewards/rejected': -45.58176803588867, 'rewards/accuracies': 0.7657812237739563, 'rewards/margins': 6.522984027862549, 'logps/rejected': -136.47093200683594, 'logps/chosen': -125.30644226074219, 'logits/rejected': -5.755499362945557, 'logits/chosen': -5.84986686706543, 'epoch': 1.76}\n{'loss': 1.2379, 'grad_norm': 2.4082107543945312, 'learning_rate': 0.001, 'rewards/chosen': -39.2131233215332, 'rewards/rejected': -45.865875244140625, 'rewards/accuracies': 0.7646874785423279, 'rewards/margins': 6.652746677398682, 'logps/rejected': -137.33139038085938, 'logps/chosen': -125.54171752929688, 'logits/rejected': -6.776431083679199, 'logits/chosen': -6.852993965148926, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [37:35<25:09,  1.14s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.57it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.80it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.6893366575241089, 'eval_runtime': 32.5573, 'eval_samples_per_second': 229.411, 'eval_steps_per_second': 1.812, 'eval_rewards/chosen': -41.16350555419922, 'eval_rewards/rejected': -47.61582565307617, 'eval_rewards/accuracies': 0.7257503867149353, 'eval_rewards/margins': 6.452322959899902, 'eval_logps/rejected': -139.62655639648438, 'eval_logps/chosen': -128.57546997070312, 'eval_logits/rejected': -4.609124183654785, 'eval_logits/chosen': -4.745906829833984, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [38:08<25:09,  1.14s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 1.2417, 'grad_norm': 2.5833699703216553, 'learning_rate': 0.001, 'rewards/chosen': -38.981231689453125, 'rewards/rejected': -45.32343673706055, 'rewards/accuracies': 0.7651562690734863, 'rewards/margins': 6.342199325561523, 'logps/rejected': -135.52691650390625, 'logps/chosen': -125.02865600585938, 'logits/rejected': -4.347536087036133, 'logits/chosen': -4.508875846862793, 'epoch': 1.85}\n{'loss': 1.2671, 'grad_norm': 2.5418238639831543, 'learning_rate': 0.001, 'rewards/chosen': -39.558258056640625, 'rewards/rejected': -45.89689636230469, 'rewards/accuracies': 0.7571874856948853, 'rewards/margins': 6.338635444641113, 'logps/rejected': -137.23748779296875, 'logps/chosen': -126.1906509399414, 'logits/rejected': -3.9763593673706055, 'logits/chosen': -4.2789692878723145, 'epoch': 1.89}\n{'loss': 1.2432, 'grad_norm': 2.221404552459717, 'learning_rate': 0.001, 'rewards/chosen': -38.754425048828125, 'rewards/rejected': -45.20294952392578, 'rewards/accuracies': 0.766406238079071, 'rewards/margins': 6.44852352142334, 'logps/rejected': -135.91207885742188, 'logps/chosen': -124.97186279296875, 'logits/rejected': -4.888024806976318, 'logits/chosen': -5.35549783706665, 'epoch': 1.94}\n{'loss': 1.1735, 'grad_norm': 2.6129214763641357, 'learning_rate': 0.001, 'rewards/chosen': -40.574676513671875, 'rewards/rejected': -47.50258255004883, 'rewards/accuracies': 0.7681249976158142, 'rewards/margins': 6.927910327911377, 'logps/rejected': -139.51010131835938, 'logps/chosen': -127.97913360595703, 'logits/rejected': -3.5222344398498535, 'logits/chosen': -3.989262104034424, 'epoch': 1.98}\n{'loss': 0.7974, 'grad_norm': 1.9272069931030273, 'learning_rate': 0.001, 'rewards/chosen': -39.221858978271484, 'rewards/rejected': -47.16505432128906, 'rewards/accuracies': 0.8217836618423462, 'rewards/margins': 7.943198204040527, 'logps/rejected': -138.92196655273438, 'logps/chosen': -125.66036987304688, 'logits/rejected': -3.730556011199951, 'logits/chosen': -4.101137638092041, 'epoch': 2.03}\n{'loss': 0.5362, 'grad_norm': 1.9926942586898804, 'learning_rate': 0.001, 'rewards/chosen': -39.64057922363281, 'rewards/rejected': -48.79312515258789, 'rewards/accuracies': 0.8712499737739563, 'rewards/margins': 9.152541160583496, 'logps/rejected': -141.56236267089844, 'logps/chosen': -126.6950454711914, 'logits/rejected': -5.3841423988342285, 'logits/chosen': -5.543816089630127, 'epoch': 2.07}\n{'loss': 0.5729, 'grad_norm': 2.223182201385498, 'learning_rate': 0.001, 'rewards/chosen': -39.79536819458008, 'rewards/rejected': -48.90949249267578, 'rewards/accuracies': 0.8626562356948853, 'rewards/margins': 9.114121437072754, 'logps/rejected': -141.83018493652344, 'logps/chosen': -126.56846618652344, 'logits/rejected': -5.5255022048950195, 'logits/chosen': -5.620789051055908, 'epoch': 2.12}\n{'loss': 0.5481, 'grad_norm': 2.0189435482025146, 'learning_rate': 0.001, 'rewards/chosen': -40.499420166015625, 'rewards/rejected': -49.45915222167969, 'rewards/accuracies': 0.8653125166893005, 'rewards/margins': 8.95973014831543, 'logps/rejected': -143.004150390625, 'logps/chosen': -127.7337417602539, 'logits/rejected': -4.937169075012207, 'logits/chosen': -5.3192901611328125, 'epoch': 2.16}\n{'loss': 0.5961, 'grad_norm': 2.4976789951324463, 'learning_rate': 0.001, 'rewards/chosen': -39.964111328125, 'rewards/rejected': -48.91146469116211, 'rewards/accuracies': 0.860156238079071, 'rewards/margins': 8.94735050201416, 'logps/rejected': -142.12957763671875, 'logps/chosen': -126.5967788696289, 'logits/rejected': -3.728518009185791, 'logits/chosen': -4.263339042663574, 'epoch': 2.21}\n{'loss': 0.6069, 'grad_norm': 2.462787628173828, 'learning_rate': 0.001, 'rewards/chosen': -40.5630989074707, 'rewards/rejected': -49.52964782714844, 'rewards/accuracies': 0.8575000166893005, 'rewards/margins': 8.966548919677734, 'logps/rejected': -143.43902587890625, 'logps/chosen': -127.98887634277344, 'logits/rejected': -4.272692680358887, 'logits/chosen': -4.659285068511963, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:08<14:48,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.61it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.81it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:15,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.81it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.79it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.81it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.80it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.4911893606185913, 'eval_runtime': 32.5367, 'eval_samples_per_second': 229.556, 'eval_steps_per_second': 1.813, 'eval_rewards/chosen': -42.58185577392578, 'eval_rewards/rejected': -49.523841857910156, 'eval_rewards/accuracies': 0.7421286702156067, 'eval_rewards/margins': 6.9419846534729, 'eval_logps/rejected': -142.80657958984375, 'eval_logps/chosen': -130.9394073486328, 'eval_logits/rejected': -2.43125057220459, 'eval_logits/chosen': -2.7983100414276123, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:41<14:48,  1.07s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 0.5812, 'grad_norm': 2.0675272941589355, 'learning_rate': 0.001, 'rewards/chosen': -39.92913818359375, 'rewards/rejected': -49.0731086730957, 'rewards/accuracies': 0.8639062643051147, 'rewards/margins': 9.143970489501953, 'logps/rejected': -142.12791442871094, 'logps/chosen': -127.01361083984375, 'logits/rejected': -4.006024360656738, 'logits/chosen': -4.382545471191406, 'epoch': 2.3}\n{'loss': 0.6266, 'grad_norm': 1.8503642082214355, 'learning_rate': 0.001, 'rewards/chosen': -39.18834686279297, 'rewards/rejected': -48.198158264160156, 'rewards/accuracies': 0.8509374856948853, 'rewards/margins': 9.009811401367188, 'logps/rejected': -141.09910583496094, 'logps/chosen': -125.32192993164062, 'logits/rejected': -5.880792140960693, 'logits/chosen': -6.109534740447998, 'epoch': 2.34}\n{'loss': 0.704, 'grad_norm': 2.0141372680664062, 'learning_rate': 0.001, 'rewards/chosen': -40.22637176513672, 'rewards/rejected': -48.945186614990234, 'rewards/accuracies': 0.8396875262260437, 'rewards/margins': 8.718810081481934, 'logps/rejected': -142.2348175048828, 'logps/chosen': -127.2307357788086, 'logits/rejected': -3.687023639678955, 'logits/chosen': -3.855170965194702, 'epoch': 2.39}\n{'loss': 0.6803, 'grad_norm': 1.779288411140442, 'learning_rate': 0.001, 'rewards/chosen': -39.76020812988281, 'rewards/rejected': -48.45793914794922, 'rewards/accuracies': 0.8450000286102295, 'rewards/margins': 8.697726249694824, 'logps/rejected': -141.02871704101562, 'logps/chosen': -126.05908203125, 'logits/rejected': -4.222546577453613, 'logits/chosen': -4.3959832191467285, 'epoch': 2.43}\n{'loss': 0.6494, 'grad_norm': 1.8298805952072144, 'learning_rate': 0.001, 'rewards/chosen': -40.273529052734375, 'rewards/rejected': -49.0992431640625, 'rewards/accuracies': 0.8475000262260437, 'rewards/margins': 8.825715065002441, 'logps/rejected': -142.24090576171875, 'logps/chosen': -126.90229797363281, 'logits/rejected': -3.741969108581543, 'logits/chosen': -3.9324347972869873, 'epoch': 2.48}\n{'loss': 0.7008, 'grad_norm': 2.083967685699463, 'learning_rate': 0.001, 'rewards/chosen': -40.68817901611328, 'rewards/rejected': -49.062652587890625, 'rewards/accuracies': 0.8354687690734863, 'rewards/margins': 8.374471664428711, 'logps/rejected': -142.12725830078125, 'logps/chosen': -128.1981964111328, 'logits/rejected': -4.2176642417907715, 'logits/chosen': -4.459802627563477, 'epoch': 2.52}\n{'loss': 0.7297, 'grad_norm': 2.7088940143585205, 'learning_rate': 0.001, 'rewards/chosen': -40.28166580200195, 'rewards/rejected': -48.83698654174805, 'rewards/accuracies': 0.8334375023841858, 'rewards/margins': 8.555317878723145, 'logps/rejected': -142.0338897705078, 'logps/chosen': -127.27295684814453, 'logits/rejected': -4.472193717956543, 'logits/chosen': -4.715816974639893, 'epoch': 2.57}\n{'loss': 0.6588, 'grad_norm': 3.415311574935913, 'learning_rate': 0.001, 'rewards/chosen': -40.19743728637695, 'rewards/rejected': -49.13396453857422, 'rewards/accuracies': 0.8485937714576721, 'rewards/margins': 8.936529159545898, 'logps/rejected': -142.20571899414062, 'logps/chosen': -127.12478637695312, 'logits/rejected': -4.126662731170654, 'logits/chosen': -4.366382122039795, 'epoch': 2.61}\n{'loss': 0.6735, 'grad_norm': 2.0555508136749268, 'learning_rate': 0.001, 'rewards/chosen': -40.43547439575195, 'rewards/rejected': -49.157012939453125, 'rewards/accuracies': 0.840624988079071, 'rewards/margins': 8.721539497375488, 'logps/rejected': -142.45147705078125, 'logps/chosen': -127.54993438720703, 'logits/rejected': -4.230712413787842, 'logits/chosen': -4.5095438957214355, 'epoch': 2.66}\n{'loss': 0.662, 'grad_norm': 2.2781198024749756, 'learning_rate': 0.001, 'rewards/chosen': -40.98788070678711, 'rewards/rejected': -49.7273063659668, 'rewards/accuracies': 0.8370312452316284, 'rewards/margins': 8.739421844482422, 'logps/rejected': -143.53961181640625, 'logps/chosen': -128.41493225097656, 'logits/rejected': -4.528903961181641, 'logits/chosen': -4.905339241027832, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [56:39<05:51,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.81it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.80it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.80it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.79it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.80it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.81it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.76it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.4211608171463013, 'eval_runtime': 32.55, 'eval_samples_per_second': 229.462, 'eval_steps_per_second': 1.813, 'eval_rewards/chosen': -42.46735382080078, 'eval_rewards/rejected': -49.90047073364258, 'eval_rewards/accuracies': 0.7495850920677185, 'eval_rewards/margins': 7.433117389678955, 'eval_logps/rejected': -143.43431091308594, 'eval_logps/chosen': -130.74855041503906, 'eval_logits/rejected': -4.103960037231445, 'eval_logits/chosen': -4.431049823760986, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [57:12<05:51,  1.07s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 0.6988, 'grad_norm': 1.793443202972412, 'learning_rate': 0.001, 'rewards/chosen': -41.08984375, 'rewards/rejected': -49.898284912109375, 'rewards/accuracies': 0.8339062333106995, 'rewards/margins': 8.808442115783691, 'logps/rejected': -143.57952880859375, 'logps/chosen': -128.41343688964844, 'logits/rejected': -4.3040971755981445, 'logits/chosen': -4.624762535095215, 'epoch': 2.75}\n{'loss': 0.7075, 'grad_norm': 2.0601987838745117, 'learning_rate': 0.001, 'rewards/chosen': -40.786678314208984, 'rewards/rejected': -49.516212463378906, 'rewards/accuracies': 0.8374999761581421, 'rewards/margins': 8.729534149169922, 'logps/rejected': -142.91741943359375, 'logps/chosen': -128.2595672607422, 'logits/rejected': -3.40615177154541, 'logits/chosen': -3.672980308532715, 'epoch': 2.8}\n{'loss': 0.6775, 'grad_norm': 2.5375611782073975, 'learning_rate': 0.001, 'rewards/chosen': -41.122108459472656, 'rewards/rejected': -49.89719772338867, 'rewards/accuracies': 0.8401562571525574, 'rewards/margins': 8.7750825881958, 'logps/rejected': -143.75677490234375, 'logps/chosen': -128.923583984375, 'logits/rejected': -4.3538289070129395, 'logits/chosen': -4.715333938598633, 'epoch': 2.84}\n{'loss': 0.7281, 'grad_norm': 2.392019271850586, 'learning_rate': 0.001, 'rewards/chosen': -41.49540710449219, 'rewards/rejected': -50.2452392578125, 'rewards/accuracies': 0.8360937237739563, 'rewards/margins': 8.749839782714844, 'logps/rejected': -143.99916076660156, 'logps/chosen': -129.57713317871094, 'logits/rejected': -4.928309440612793, 'logits/chosen': -5.2527947425842285, 'epoch': 2.89}\n{'loss': 0.7384, 'grad_norm': 1.6954532861709595, 'learning_rate': 0.001, 'rewards/chosen': -41.167781829833984, 'rewards/rejected': -49.737430572509766, 'rewards/accuracies': 0.8317187428474426, 'rewards/margins': 8.569647789001465, 'logps/rejected': -142.8827667236328, 'logps/chosen': -128.81451416015625, 'logits/rejected': -4.462515354156494, 'logits/chosen': -4.836927890777588, 'epoch': 2.93}\n{'loss': 0.7307, 'grad_norm': 2.1814448833465576, 'learning_rate': 0.001, 'rewards/chosen': -40.73749923706055, 'rewards/rejected': -49.241905212402344, 'rewards/accuracies': 0.8335937261581421, 'rewards/margins': 8.504405975341797, 'logps/rejected': -142.57029724121094, 'logps/chosen': -127.91451263427734, 'logits/rejected': -4.681119441986084, 'logits/chosen': -5.0778350830078125, 'epoch': 2.98}\n{'train_runtime': 3802.9842, 'train_samples_per_second': 111.939, 'train_steps_per_second': 0.875, 'train_loss': 1.4285561193529823, 'epoch': 3.0}\n100%|█████████████████████████████████████| 3327/3327 [1:03:06<00:00,  1.14s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-3327\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-2218\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-1109","metadata":{"execution":{"iopub.status.busy":"2024-05-27T03:44:17.795110Z","iopub.execute_input":"2024-05-27T03:44:17.795550Z","iopub.status.idle":"2024-05-27T03:58:50.321325Z","shell.execute_reply.started":"2024-05-27T03:44:17.795517Z","shell.execute_reply":"2024-05-27T03:58:50.320088Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2024-05-27 03:44:24.218330: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 03:44:24.218388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 03:44:24.219801: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-3327', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.27.03.44'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nDownloading readme: 100%|██████████████████| 7.81k/7.81k [00:00<00:00, 24.1MB/s]\nDownloading data: 100%|████████████████████| 21.0M/21.0M [00:00<00:00, 66.7MB/s]\nDownloading data: 100%|█████████████████████| 20.5M/20.5M [00:00<00:00, 110MB/s]\nDownloading data: 100%|█████████████████████| 42.0M/42.0M [00:00<00:00, 113MB/s]\nGenerating train split: 100%|██| 25000/25000 [00:00<00:00, 162187.71 examples/s]\nGenerating test split: 100%|███| 25000/25000 [00:00<00:00, 197259.45 examples/s]\nGenerating unsupervised split: 100%|█| 50000/50000 [00:00<00:00, 200703.99 examp\nFilter: 100%|██████████████████| 25000/25000 [00:00<00:00, 162703.36 examples/s]\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|████████████████████████| 24895/24895 [00:28<00:00, 867.43 examples/s]\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-3327\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_034503-1gxy378a\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.27.03.44\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/1gxy378a\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 2.04MB/s]\npytorch_model.bin: 100%|████████████████████| 1.42G/1.42G [00:04<00:00, 292MB/s]\ntokenizer_config.json: 100%|████████████████████| 256/256 [00:00<00:00, 669kB/s]\nvocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 16.8MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 20.5MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 150/150 [00:00<00:00, 529kB/s]\neval batch size 256\nFilter: 100%|█████████████████████| 2500/2500 [00:00<00:00, 88793.14 examples/s]\nMap:   2%|▋                           | 60/2487 [00:00<00:04, 583.07 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|██████████████████████████| 2487/2487 [00:04<00:00, 564.48 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:28<03:49, 28.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:54<03:11, 27.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:21<02:41, 26.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:47<02:13, 26.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:14<01:46, 26.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:40<01:19, 26.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:33<00:26, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:00<00:00, 26.74s/it]\nmean test reward 0.8164737734246046 +/- 0.007958856240041536 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.99825519323349 from 0.00922983093187213\nmean KL 5.683440565235085 +/- 0.5785263978869969 full 84.66163535912831 +/- 0.19215233274331953\nmedian KL 9.395160675048828 full 84.0092887878418\n2024-05-27 03:49:44.611599: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 03:49:44.611654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 03:49:44.613074: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-2218', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.27.03.49'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-2218\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_034953-zmr97y21\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.27.03.49\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/zmr97y21\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:32, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:52<03:04, 26.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:37, 26.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:45<02:11, 26.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:11<01:45, 26.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:38<01:19, 26.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:04<00:52, 26.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:30<00:26, 26.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:57<00:00, 26.37s/it]\nmean test reward 0.7993676942293304 +/- 0.008222832383973769 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9980008006095886 from 0.00922983093187213\nmean KL -0.44574629329144955 +/- 0.6311798659685102 full 87.46127213206556 +/- 0.18939311387538926\nmedian KL 2.4450883865356445 full 86.65761947631836\n2024-05-27 03:54:20.424790: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 03:54:20.424848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 03:54:20.426448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-1109', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.27.03.54'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_0_6_lr1e-3-2024.05.27.02.33/checkpoint-1109\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_035429-ury38b5z\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.27.03.54\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/ury38b5z\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:32, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:52<03:03, 26.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:18<02:37, 26.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:45<02:11, 26.26s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:11<01:45, 26.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:37<01:18, 26.27s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:04<00:52, 26.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:31<00:26, 26.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:57<00:00, 26.42s/it]\nmean test reward 0.7989253848305048 +/- 0.008227290806265612 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.997838705778122 from 0.00922983093187213\nmean KL 5.597673446250458 +/- 0.5185372411527659 full 79.36952357656426 +/- 0.217772285638375\nmedian KL 8.447534561157227 full 78.28573989868164\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=test_dpo_0_6_lr1e-3  --loss_type=ipo --beta=0.6 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_tokenized_split     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 1e-3     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=500 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch'","metadata":{"execution":{"iopub.status.busy":"2024-05-27T04:06:53.532696Z","iopub.execute_input":"2024-05-27T04:06:53.533543Z","iopub.status.idle":"2024-05-27T05:10:46.565296Z","shell.execute_reply.started":"2024-05-27T04:06:53.533504Z","shell.execute_reply":"2024-05-27T05:10:46.564013Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2024-05-27 04:06:59.501516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 04:06:59.501575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 04:06:59.503083: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 149370\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_040705-rliqi3w5\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest_dpo_0_6_lr1e-3\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/rliqi3w5\u001b[0m\n  0%|                                                  | 0/3327 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6944, 'grad_norm': 4.818739891052246, 'learning_rate': 6.6666666666666675e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -3.736757755279541, 'logps/chosen': -3.8562333583831787, 'logits/rejected': -39.777442932128906, 'logits/chosen': -37.3552131652832, 'epoch': 0.0}\n{'loss': 0.6025, 'grad_norm': 4.04394006729126, 'learning_rate': 0.0003333333333333333, 'rewards/chosen': -0.18892350792884827, 'rewards/rejected': -0.2793239951133728, 'rewards/accuracies': 0.6567283272743225, 'rewards/margins': 0.09040050208568573, 'logps/rejected': -4.3362016677856445, 'logps/chosen': -4.160398960113525, 'logits/rejected': -34.75062942504883, 'logits/chosen': -34.230953216552734, 'epoch': 0.05}\n{'loss': 0.7684, 'grad_norm': 3.028377056121826, 'learning_rate': 0.0006666666666666666, 'rewards/chosen': -0.5560494065284729, 'rewards/rejected': -0.6740739345550537, 'rewards/accuracies': 0.6346874833106995, 'rewards/margins': 0.1180245503783226, 'logps/rejected': -4.965661525726318, 'logps/chosen': -4.756692409515381, 'logits/rejected': -11.81482219696045, 'logits/chosen': -11.658773422241211, 'epoch': 0.09}\n{'loss': 1.0021, 'grad_norm': 2.5558347702026367, 'learning_rate': 0.001, 'rewards/chosen': -1.150099277496338, 'rewards/rejected': -1.2795937061309814, 'rewards/accuracies': 0.6187499761581421, 'rewards/margins': 0.12949447333812714, 'logps/rejected': -6.008272647857666, 'logps/chosen': -5.737055778503418, 'logits/rejected': -17.8169002532959, 'logits/chosen': -17.937257766723633, 'epoch': 0.14}\n{'loss': 1.1381, 'grad_norm': 1.9422073364257812, 'learning_rate': 0.001, 'rewards/chosen': -1.5395241975784302, 'rewards/rejected': -1.6696124076843262, 'rewards/accuracies': 0.6009374856948853, 'rewards/margins': 0.13008837401866913, 'logps/rejected': -6.619600772857666, 'logps/chosen': -6.40049934387207, 'logits/rejected': -9.500960350036621, 'logits/chosen': -9.61499309539795, 'epoch': 0.18}\n{'loss': 1.1366, 'grad_norm': 1.7377314567565918, 'learning_rate': 0.001, 'rewards/chosen': -1.6801843643188477, 'rewards/rejected': -1.8071845769882202, 'rewards/accuracies': 0.6010937690734863, 'rewards/margins': 0.12700045108795166, 'logps/rejected': -6.850539684295654, 'logps/chosen': -6.6166672706604, 'logits/rejected': -3.859800338745117, 'logits/chosen': -4.265954971313477, 'epoch': 0.23}\n{'loss': 1.1311, 'grad_norm': 1.499982476234436, 'learning_rate': 0.001, 'rewards/chosen': -1.755143404006958, 'rewards/rejected': -1.8736263513565063, 'rewards/accuracies': 0.5903124809265137, 'rewards/margins': 0.11848285794258118, 'logps/rejected': -6.979712963104248, 'logps/chosen': -6.745581150054932, 'logits/rejected': -4.324687957763672, 'logits/chosen': -4.349027633666992, 'epoch': 0.27}\n{'loss': 1.0683, 'grad_norm': 1.4659841060638428, 'learning_rate': 0.001, 'rewards/chosen': -1.855790138244629, 'rewards/rejected': -1.9990040063858032, 'rewards/accuracies': 0.6092187762260437, 'rewards/margins': 0.14321376383304596, 'logps/rejected': -7.178144454956055, 'logps/chosen': -6.907717704772949, 'logits/rejected': -6.275196075439453, 'logits/chosen': -6.383439540863037, 'epoch': 0.32}\n{'loss': 1.0559, 'grad_norm': 1.2585185766220093, 'learning_rate': 0.001, 'rewards/chosen': -1.850679874420166, 'rewards/rejected': -2.0015225410461426, 'rewards/accuracies': 0.6215624809265137, 'rewards/margins': 0.15084262192249298, 'logps/rejected': -7.176095008850098, 'logps/chosen': -6.90877628326416, 'logits/rejected': -3.5548489093780518, 'logits/chosen': -3.6009137630462646, 'epoch': 0.36}\n{'loss': 1.0096, 'grad_norm': 2.116466999053955, 'learning_rate': 0.001, 'rewards/chosen': -1.8411567211151123, 'rewards/rejected': -1.9959402084350586, 'rewards/accuracies': 0.6223437786102295, 'rewards/margins': 0.15478339791297913, 'logps/rejected': -7.171087741851807, 'logps/chosen': -6.8928327560424805, 'logits/rejected': -5.767096996307373, 'logits/chosen': -5.937700271606445, 'epoch': 0.41}\n{'loss': 0.9798, 'grad_norm': 1.0953437089920044, 'learning_rate': 0.001, 'rewards/chosen': -1.8150135278701782, 'rewards/rejected': -1.9804518222808838, 'rewards/accuracies': 0.6364062428474426, 'rewards/margins': 0.16543790698051453, 'logps/rejected': -7.139342784881592, 'logps/chosen': -6.859555721282959, 'logits/rejected': -3.63191556930542, 'logits/chosen': -3.6449224948883057, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:01<51:07,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.80it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.80it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.79it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.79it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.0190141201019287, 'eval_runtime': 32.6558, 'eval_samples_per_second': 228.719, 'eval_steps_per_second': 1.807, 'eval_rewards/chosen': -1.784837007522583, 'eval_rewards/rejected': -1.9355932474136353, 'eval_rewards/accuracies': 0.6284575462341309, 'eval_rewards/margins': 0.15075629949569702, 'eval_logps/rejected': -7.06876277923584, 'eval_logps/chosen': -6.795840740203857, 'eval_logits/rejected': -7.278273105621338, 'eval_logits/chosen': -7.6958909034729, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:34<51:07,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 0.9876, 'grad_norm': 1.0801926851272583, 'learning_rate': 0.001, 'rewards/chosen': -1.85799241065979, 'rewards/rejected': -2.020721673965454, 'rewards/accuracies': 0.6340625286102295, 'rewards/margins': 0.1627293825149536, 'logps/rejected': -7.2210845947265625, 'logps/chosen': -6.9358110427856445, 'logits/rejected': -4.294049263000488, 'logits/chosen': -4.352651596069336, 'epoch': 0.5}\n{'loss': 0.9367, 'grad_norm': 1.0938866138458252, 'learning_rate': 0.001, 'rewards/chosen': -1.8582898378372192, 'rewards/rejected': -2.0301623344421387, 'rewards/accuracies': 0.6401562690734863, 'rewards/margins': 0.17187240719795227, 'logps/rejected': -7.245762348175049, 'logps/chosen': -6.923621654510498, 'logits/rejected': -4.506816864013672, 'logits/chosen': -4.623519420623779, 'epoch': 0.54}\n{'loss': 0.9134, 'grad_norm': 1.050818681716919, 'learning_rate': 0.001, 'rewards/chosen': -1.817838191986084, 'rewards/rejected': -1.9914946556091309, 'rewards/accuracies': 0.6496875286102295, 'rewards/margins': 0.1736564040184021, 'logps/rejected': -7.189192295074463, 'logps/chosen': -6.849659442901611, 'logits/rejected': -4.393133640289307, 'logits/chosen': -4.674933910369873, 'epoch': 0.59}\n{'loss': 0.875, 'grad_norm': 1.2096937894821167, 'learning_rate': 0.001, 'rewards/chosen': -1.8547765016555786, 'rewards/rejected': -2.043853521347046, 'rewards/accuracies': 0.6603124737739563, 'rewards/margins': 0.18907712399959564, 'logps/rejected': -7.244211196899414, 'logps/chosen': -6.928669452667236, 'logits/rejected': -2.933095932006836, 'logits/chosen': -3.1536426544189453, 'epoch': 0.63}\n{'loss': 0.8661, 'grad_norm': 0.9194351434707642, 'learning_rate': 0.001, 'rewards/chosen': -1.838474154472351, 'rewards/rejected': -2.029209852218628, 'rewards/accuracies': 0.6650000214576721, 'rewards/margins': 0.19073586165905, 'logps/rejected': -7.203624248504639, 'logps/chosen': -6.898680210113525, 'logits/rejected': -5.169659614562988, 'logits/chosen': -5.207777500152588, 'epoch': 0.68}\n{'loss': 0.8952, 'grad_norm': 1.159669280052185, 'learning_rate': 0.001, 'rewards/chosen': -1.8466659784317017, 'rewards/rejected': -2.020845651626587, 'rewards/accuracies': 0.6539062261581421, 'rewards/margins': 0.17417968809604645, 'logps/rejected': -7.204544544219971, 'logps/chosen': -6.893863677978516, 'logits/rejected': -4.506798267364502, 'logits/chosen': -4.685708522796631, 'epoch': 0.72}\n{'loss': 0.847, 'grad_norm': 1.0885460376739502, 'learning_rate': 0.001, 'rewards/chosen': -1.8798617124557495, 'rewards/rejected': -2.0729453563690186, 'rewards/accuracies': 0.6693750023841858, 'rewards/margins': 0.19308367371559143, 'logps/rejected': -7.309890270233154, 'logps/chosen': -6.97075080871582, 'logits/rejected': -4.333876609802246, 'logits/chosen': -4.407349586486816, 'epoch': 0.77}\n{'loss': 0.8504, 'grad_norm': 1.025112271308899, 'learning_rate': 0.001, 'rewards/chosen': -1.8625696897506714, 'rewards/rejected': -2.0448687076568604, 'rewards/accuracies': 0.6542187333106995, 'rewards/margins': 0.18229886889457703, 'logps/rejected': -7.251889705657959, 'logps/chosen': -6.9439263343811035, 'logits/rejected': -3.6378448009490967, 'logits/chosen': -3.852287530899048, 'epoch': 0.81}\n{'loss': 0.8015, 'grad_norm': 0.8914923071861267, 'learning_rate': 0.001, 'rewards/chosen': -1.8445101976394653, 'rewards/rejected': -2.043367862701416, 'rewards/accuracies': 0.6670312285423279, 'rewards/margins': 0.198857381939888, 'logps/rejected': -7.244454383850098, 'logps/chosen': -6.887210845947266, 'logits/rejected': -3.9456212520599365, 'logits/chosen': -4.018416881561279, 'epoch': 0.86}\n{'loss': 0.8027, 'grad_norm': 1.1693202257156372, 'learning_rate': 0.001, 'rewards/chosen': -1.8505932092666626, 'rewards/rejected': -2.0499885082244873, 'rewards/accuracies': 0.6745312213897705, 'rewards/margins': 0.19939523935317993, 'logps/rejected': -7.257265090942383, 'logps/chosen': -6.897356033325195, 'logits/rejected': -5.090245723724365, 'logits/chosen': -5.2121100425720215, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [18:35<41:51,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.61it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.94it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.89it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.86it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.84it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.81it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.81it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.79it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.79it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.7994838356971741, 'eval_runtime': 32.6654, 'eval_samples_per_second': 228.652, 'eval_steps_per_second': 1.806, 'eval_rewards/chosen': -1.8156144618988037, 'eval_rewards/rejected': -2.009734630584717, 'eval_rewards/accuracies': 0.6700447201728821, 'eval_rewards/margins': 0.19412018358707428, 'eval_logps/rejected': -7.192331790924072, 'eval_logps/chosen': -6.847136974334717, 'eval_logits/rejected': -4.735436916351318, 'eval_logits/chosen': -4.868200778961182, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [19:08<41:51,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 0.793, 'grad_norm': 1.163270354270935, 'learning_rate': 0.001, 'rewards/chosen': -1.8175616264343262, 'rewards/rejected': -2.0094377994537354, 'rewards/accuracies': 0.6659374833106995, 'rewards/margins': 0.19187623262405396, 'logps/rejected': -7.213627815246582, 'logps/chosen': -6.850261688232422, 'logits/rejected': -2.6910085678100586, 'logits/chosen': -2.751610517501831, 'epoch': 0.95}\n{'loss': 0.7516, 'grad_norm': 1.176750659942627, 'learning_rate': 0.001, 'rewards/chosen': -1.8302406072616577, 'rewards/rejected': -2.04826283454895, 'rewards/accuracies': 0.6885937452316284, 'rewards/margins': 0.2180224359035492, 'logps/rejected': -7.25799560546875, 'logps/chosen': -6.88746452331543, 'logits/rejected': -3.546729803085327, 'logits/chosen': -3.5849969387054443, 'epoch': 0.99}\n{'loss': 0.5356, 'grad_norm': 0.8409452438354492, 'learning_rate': 0.001, 'rewards/chosen': -1.770004153251648, 'rewards/rejected': -2.0572516918182373, 'rewards/accuracies': 0.7790300250053406, 'rewards/margins': 0.28724750876426697, 'logps/rejected': -7.254889965057373, 'logps/chosen': -6.743541717529297, 'logits/rejected': -3.105902671813965, 'logits/chosen': -3.2861664295196533, 'epoch': 1.04}\n{'loss': 0.469, 'grad_norm': 0.787722647190094, 'learning_rate': 0.001, 'rewards/chosen': -1.7234059572219849, 'rewards/rejected': -2.0296056270599365, 'rewards/accuracies': 0.8073437213897705, 'rewards/margins': 0.3061998784542084, 'logps/rejected': -7.242832183837891, 'logps/chosen': -6.699664115905762, 'logits/rejected': -3.7736544609069824, 'logits/chosen': -4.0316596031188965, 'epoch': 1.08}\n{'loss': 0.481, 'grad_norm': 0.7422354221343994, 'learning_rate': 0.001, 'rewards/chosen': -1.7799618244171143, 'rewards/rejected': -2.081286668777466, 'rewards/accuracies': 0.800000011920929, 'rewards/margins': 0.3013250529766083, 'logps/rejected': -7.2995476722717285, 'logps/chosen': -6.7857842445373535, 'logits/rejected': -4.4083251953125, 'logits/chosen': -4.728686809539795, 'epoch': 1.13}\n{'loss': 0.4922, 'grad_norm': 0.8801537156105042, 'learning_rate': 0.001, 'rewards/chosen': -1.7437553405761719, 'rewards/rejected': -2.037014961242676, 'rewards/accuracies': 0.7917187213897705, 'rewards/margins': 0.29325979948043823, 'logps/rejected': -7.236550331115723, 'logps/chosen': -6.7288947105407715, 'logits/rejected': -3.560983180999756, 'logits/chosen': -3.7938005924224854, 'epoch': 1.17}\n{'loss': 0.503, 'grad_norm': 0.7679752707481384, 'learning_rate': 0.001, 'rewards/chosen': -1.755983829498291, 'rewards/rejected': -2.048424482345581, 'rewards/accuracies': 0.7892187237739563, 'rewards/margins': 0.2924404442310333, 'logps/rejected': -7.283290863037109, 'logps/chosen': -6.746269702911377, 'logits/rejected': -3.3271541595458984, 'logits/chosen': -3.4714300632476807, 'epoch': 1.22}\n{'loss': 0.4934, 'grad_norm': 0.8113707900047302, 'learning_rate': 0.001, 'rewards/chosen': -1.757956862449646, 'rewards/rejected': -2.0505452156066895, 'rewards/accuracies': 0.7890625, 'rewards/margins': 0.2925882637500763, 'logps/rejected': -7.2747602462768555, 'logps/chosen': -6.779207706451416, 'logits/rejected': -4.127618312835693, 'logits/chosen': -4.282507419586182, 'epoch': 1.26}\n{'loss': 0.4956, 'grad_norm': 0.7680554389953613, 'learning_rate': 0.001, 'rewards/chosen': -1.7344470024108887, 'rewards/rejected': -2.026881217956543, 'rewards/accuracies': 0.7834374904632568, 'rewards/margins': 0.2924339473247528, 'logps/rejected': -7.227380275726318, 'logps/chosen': -6.708265781402588, 'logits/rejected': -3.445671796798706, 'logits/chosen': -3.57930064201355, 'epoch': 1.31}\n{'loss': 0.5011, 'grad_norm': 0.6862382888793945, 'learning_rate': 0.001, 'rewards/chosen': -1.7856736183166504, 'rewards/rejected': -2.0772652626037598, 'rewards/accuracies': 0.7854687571525574, 'rewards/margins': 0.2915918231010437, 'logps/rejected': -7.304892063140869, 'logps/chosen': -6.80726432800293, 'logits/rejected': -3.472890615463257, 'logits/chosen': -3.5746283531188965, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:11<32:48,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.80it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.80it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.80it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.79it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.79it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.73it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.75it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6886090040206909, 'eval_runtime': 32.6618, 'eval_samples_per_second': 228.677, 'eval_steps_per_second': 1.806, 'eval_rewards/chosen': -1.8388134241104126, 'eval_rewards/rejected': -2.058197498321533, 'eval_rewards/accuracies': 0.7016007304191589, 'eval_rewards/margins': 0.2193838655948639, 'eval_logps/rejected': -7.2731032371521, 'eval_logps/chosen': -6.885801792144775, 'eval_logits/rejected': -2.353869676589966, 'eval_logits/chosen': -2.4801430702209473, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:43<32:48,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.18it/s]\u001b[A\n{'loss': 0.4952, 'grad_norm': 0.8138155341148376, 'learning_rate': 0.001, 'rewards/chosen': -1.8003915548324585, 'rewards/rejected': -2.0903890132904053, 'rewards/accuracies': 0.7835937738418579, 'rewards/margins': 0.2899974584579468, 'logps/rejected': -7.314779281616211, 'logps/chosen': -6.822874069213867, 'logits/rejected': -3.4804258346557617, 'logits/chosen': -3.662497043609619, 'epoch': 1.4}\n{'loss': 0.5161, 'grad_norm': 0.8473292589187622, 'learning_rate': 0.001, 'rewards/chosen': -1.7519155740737915, 'rewards/rejected': -2.0348289012908936, 'rewards/accuracies': 0.7768750190734863, 'rewards/margins': 0.2829134464263916, 'logps/rejected': -7.2387919425964355, 'logps/chosen': -6.726844310760498, 'logits/rejected': -3.77085018157959, 'logits/chosen': -4.008518218994141, 'epoch': 1.44}\n{'loss': 0.5138, 'grad_norm': 0.713275134563446, 'learning_rate': 0.001, 'rewards/chosen': -1.7662739753723145, 'rewards/rejected': -2.0479815006256104, 'rewards/accuracies': 0.7707812786102295, 'rewards/margins': 0.2817073464393616, 'logps/rejected': -7.260141372680664, 'logps/chosen': -6.770470142364502, 'logits/rejected': -3.5039424896240234, 'logits/chosen': -3.5417673587799072, 'epoch': 1.49}\n{'loss': 0.5121, 'grad_norm': 0.7392369508743286, 'learning_rate': 0.001, 'rewards/chosen': -1.7633038759231567, 'rewards/rejected': -2.0481345653533936, 'rewards/accuracies': 0.7734375, 'rewards/margins': 0.28483065962791443, 'logps/rejected': -7.257002353668213, 'logps/chosen': -6.747735977172852, 'logits/rejected': -3.680307626724243, 'logits/chosen': -3.7580718994140625, 'epoch': 1.53}\n{'loss': 0.5045, 'grad_norm': 0.6804996728897095, 'learning_rate': 0.001, 'rewards/chosen': -1.8222403526306152, 'rewards/rejected': -2.106755256652832, 'rewards/accuracies': 0.7704687714576721, 'rewards/margins': 0.28451475501060486, 'logps/rejected': -7.361352443695068, 'logps/chosen': -6.866817474365234, 'logits/rejected': -3.7871460914611816, 'logits/chosen': -3.9254422187805176, 'epoch': 1.58}\n{'loss': 0.5242, 'grad_norm': 1.048427939414978, 'learning_rate': 0.001, 'rewards/chosen': -1.8106459379196167, 'rewards/rejected': -2.088118076324463, 'rewards/accuracies': 0.7699999809265137, 'rewards/margins': 0.2774721384048462, 'logps/rejected': -7.3258771896362305, 'logps/chosen': -6.863180160522461, 'logits/rejected': -3.0906734466552734, 'logits/chosen': -3.2936859130859375, 'epoch': 1.62}\n{'loss': 0.503, 'grad_norm': 0.8002029657363892, 'learning_rate': 0.001, 'rewards/chosen': -1.7920514345169067, 'rewards/rejected': -2.071537971496582, 'rewards/accuracies': 0.7710937261581421, 'rewards/margins': 0.27948683500289917, 'logps/rejected': -7.313047409057617, 'logps/chosen': -6.803633213043213, 'logits/rejected': -3.2707102298736572, 'logits/chosen': -3.5607030391693115, 'epoch': 1.67}\n{'loss': 0.5077, 'grad_norm': 0.8390942215919495, 'learning_rate': 0.001, 'rewards/chosen': -1.78228759765625, 'rewards/rejected': -2.065786838531494, 'rewards/accuracies': 0.7785937786102295, 'rewards/margins': 0.2834993600845337, 'logps/rejected': -7.292116641998291, 'logps/chosen': -6.810603141784668, 'logits/rejected': -3.6730945110321045, 'logits/chosen': -3.861722946166992, 'epoch': 1.71}\n{'loss': 0.5216, 'grad_norm': 0.8159002661705017, 'learning_rate': 0.001, 'rewards/chosen': -1.8145092725753784, 'rewards/rejected': -2.0880331993103027, 'rewards/accuracies': 0.7696874737739563, 'rewards/margins': 0.2735237181186676, 'logps/rejected': -7.326621055603027, 'logps/chosen': -6.8577189445495605, 'logits/rejected': -3.511791706085205, 'logits/chosen': -3.6540465354919434, 'epoch': 1.76}\n{'loss': 0.51, 'grad_norm': 0.7005115747451782, 'learning_rate': 0.001, 'rewards/chosen': -1.776810884475708, 'rewards/rejected': -2.056236505508423, 'rewards/accuracies': 0.7723437547683716, 'rewards/margins': 0.27942556142807007, 'logps/rejected': -7.298768520355225, 'logps/chosen': -6.784067153930664, 'logits/rejected': -4.319227695465088, 'logits/chosen': -4.519869804382324, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [37:45<24:04,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.94it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.89it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:31,  1.61it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:29,  1.66it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:28,  1.71it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:27,  1.74it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:26,  1.76it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.74it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:08<00:24,  1.76it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:24,  1.78it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:09<00:23,  1.79it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.79it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:13<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:14<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.80it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:18<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.78it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:19<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:23<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:24<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:28<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:29<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:32<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6267457604408264, 'eval_runtime': 32.897, 'eval_samples_per_second': 227.042, 'eval_steps_per_second': 1.793, 'eval_rewards/chosen': -1.7928998470306396, 'eval_rewards/rejected': -2.046373128890991, 'eval_rewards/accuracies': 0.7220839262008667, 'eval_rewards/margins': 0.2534732520580292, 'eval_logps/rejected': -7.2533955574035645, 'eval_logps/chosen': -6.809279441833496, 'eval_logits/rejected': -5.007301330566406, 'eval_logits/chosen': -5.234168529510498, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [38:18<24:04,  1.09s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 0.5035, 'grad_norm': 1.159275770187378, 'learning_rate': 0.001, 'rewards/chosen': -1.8147101402282715, 'rewards/rejected': -2.0953845977783203, 'rewards/accuracies': 0.7718750238418579, 'rewards/margins': 0.28067445755004883, 'logps/rejected': -7.318748950958252, 'logps/chosen': -6.846803188323975, 'logits/rejected': -4.183996677398682, 'logits/chosen': -4.314203262329102, 'epoch': 1.85}\n{'loss': 0.5171, 'grad_norm': 0.9054221510887146, 'learning_rate': 0.001, 'rewards/chosen': -1.868052363395691, 'rewards/rejected': -2.1448144912719727, 'rewards/accuracies': 0.7659375071525574, 'rewards/margins': 0.27676188945770264, 'logps/rejected': -7.434679985046387, 'logps/chosen': -6.947727680206299, 'logits/rejected': -3.7728209495544434, 'logits/chosen': -3.882920503616333, 'epoch': 1.89}\n{'loss': 0.5199, 'grad_norm': 0.6290156245231628, 'learning_rate': 0.001, 'rewards/chosen': -1.8338043689727783, 'rewards/rejected': -2.1074864864349365, 'rewards/accuracies': 0.7634375095367432, 'rewards/margins': 0.27368202805519104, 'logps/rejected': -7.35942268371582, 'logps/chosen': -6.896735668182373, 'logits/rejected': -3.3594887256622314, 'logits/chosen': -3.4422643184661865, 'epoch': 1.94}\n{'loss': 0.5042, 'grad_norm': 0.6905130743980408, 'learning_rate': 0.001, 'rewards/chosen': -1.864478588104248, 'rewards/rejected': -2.1436715126037598, 'rewards/accuracies': 0.76953125, 'rewards/margins': 0.2791929543018341, 'logps/rejected': -7.413408279418945, 'logps/chosen': -6.942366123199463, 'logits/rejected': -3.3998916149139404, 'logits/chosen': -3.5375287532806396, 'epoch': 1.98}\n{'loss': 0.3894, 'grad_norm': 0.5682637691497803, 'learning_rate': 0.001, 'rewards/chosen': -1.8158916234970093, 'rewards/rejected': -2.142880916595459, 'rewards/accuracies': 0.8407426476478577, 'rewards/margins': 0.32698920369148254, 'logps/rejected': -7.416691780090332, 'logps/chosen': -6.861321926116943, 'logits/rejected': -3.910310983657837, 'logits/chosen': -4.120177745819092, 'epoch': 2.03}\n{'loss': 0.2982, 'grad_norm': 0.5705609917640686, 'learning_rate': 0.001, 'rewards/chosen': -1.7723040580749512, 'rewards/rejected': -2.1376655101776123, 'rewards/accuracies': 0.8948437571525574, 'rewards/margins': 0.3653613030910492, 'logps/rejected': -7.391021251678467, 'logps/chosen': -6.797879695892334, 'logits/rejected': -4.245851993560791, 'logits/chosen': -4.448175430297852, 'epoch': 2.07}\n{'loss': 0.3189, 'grad_norm': 0.5313532948493958, 'learning_rate': 0.001, 'rewards/chosen': -1.7830021381378174, 'rewards/rejected': -2.127981185913086, 'rewards/accuracies': 0.8762500286102295, 'rewards/margins': 0.34497883915901184, 'logps/rejected': -7.385395526885986, 'logps/chosen': -6.8041462898254395, 'logits/rejected': -3.131854295730591, 'logits/chosen': -3.253316640853882, 'epoch': 2.12}\n{'loss': 0.3075, 'grad_norm': 0.5086668729782104, 'learning_rate': 0.001, 'rewards/chosen': -1.7832750082015991, 'rewards/rejected': -2.136348009109497, 'rewards/accuracies': 0.883593738079071, 'rewards/margins': 0.3530733585357666, 'logps/rejected': -7.407555103302002, 'logps/chosen': -6.805851459503174, 'logits/rejected': -3.7934658527374268, 'logits/chosen': -3.888493537902832, 'epoch': 2.16}\n{'loss': 0.3239, 'grad_norm': 0.5100292563438416, 'learning_rate': 0.001, 'rewards/chosen': -1.8032476902008057, 'rewards/rejected': -2.151693105697632, 'rewards/accuracies': 0.8767187595367432, 'rewards/margins': 0.34844544529914856, 'logps/rejected': -7.443376541137695, 'logps/chosen': -6.825340747833252, 'logits/rejected': -3.0887186527252197, 'logits/chosen': -3.2084455490112305, 'epoch': 2.21}\n{'loss': 0.3399, 'grad_norm': 0.6542589664459229, 'learning_rate': 0.001, 'rewards/chosen': -1.793373465538025, 'rewards/rejected': -2.135895252227783, 'rewards/accuracies': 0.8660937547683716, 'rewards/margins': 0.34252169728279114, 'logps/rejected': -7.431224346160889, 'logps/chosen': -6.83051872253418, 'logits/rejected': -3.011225700378418, 'logits/chosen': -3.158992052078247, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:20<14:52,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.61it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.80it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.78it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.78it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.78it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.76it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.78it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.78it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.79it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.79it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5856570601463318, 'eval_runtime': 32.6885, 'eval_samples_per_second': 228.49, 'eval_steps_per_second': 1.805, 'eval_rewards/chosen': -1.8390518426895142, 'eval_rewards/rejected': -2.106074810028076, 'eval_rewards/accuracies': 0.7331862449645996, 'eval_rewards/margins': 0.2670230567455292, 'eval_logps/rejected': -7.352898597717285, 'eval_logps/chosen': -6.886199474334717, 'eval_logits/rejected': -2.988658905029297, 'eval_logits/chosen': -3.172569751739502, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:53<14:52,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 0.3339, 'grad_norm': 0.5125100016593933, 'learning_rate': 0.001, 'rewards/chosen': -1.796726107597351, 'rewards/rejected': -2.1403141021728516, 'rewards/accuracies': 0.8692187666893005, 'rewards/margins': 0.3435879051685333, 'logps/rejected': -7.410412788391113, 'logps/chosen': -6.831991672515869, 'logits/rejected': -2.807419538497925, 'logits/chosen': -2.9549689292907715, 'epoch': 2.3}\n{'loss': 0.3489, 'grad_norm': 0.5525304079055786, 'learning_rate': 0.001, 'rewards/chosen': -1.7871997356414795, 'rewards/rejected': -2.1236464977264404, 'rewards/accuracies': 0.8604687452316284, 'rewards/margins': 0.3364466726779938, 'logps/rejected': -7.404372692108154, 'logps/chosen': -6.7990546226501465, 'logits/rejected': -3.5198676586151123, 'logits/chosen': -3.68587589263916, 'epoch': 2.34}\n{'loss': 0.3535, 'grad_norm': 0.6204448342323303, 'learning_rate': 0.001, 'rewards/chosen': -1.7576185464859009, 'rewards/rejected': -2.091557025909424, 'rewards/accuracies': 0.8556249737739563, 'rewards/margins': 0.33393847942352295, 'logps/rejected': -7.352926731109619, 'logps/chosen': -6.755961894989014, 'logits/rejected': -3.294309139251709, 'logits/chosen': -3.456732988357544, 'epoch': 2.39}\n{'loss': 0.3727, 'grad_norm': 0.5293018221855164, 'learning_rate': 0.001, 'rewards/chosen': -1.8123022317886353, 'rewards/rejected': -2.134330987930298, 'rewards/accuracies': 0.8443750143051147, 'rewards/margins': 0.3220287561416626, 'logps/rejected': -7.3892107009887695, 'logps/chosen': -6.826740741729736, 'logits/rejected': -2.892472505569458, 'logits/chosen': -3.0404627323150635, 'epoch': 2.43}\n{'loss': 0.3754, 'grad_norm': 0.5563784241676331, 'learning_rate': 0.001, 'rewards/chosen': -1.8307998180389404, 'rewards/rejected': -2.154773235321045, 'rewards/accuracies': 0.839062511920929, 'rewards/margins': 0.32397353649139404, 'logps/rejected': -7.43002462387085, 'logps/chosen': -6.865472793579102, 'logits/rejected': -3.3560476303100586, 'logits/chosen': -3.435213327407837, 'epoch': 2.48}\n{'loss': 0.3844, 'grad_norm': 0.5352764129638672, 'learning_rate': 0.001, 'rewards/chosen': -1.8165419101715088, 'rewards/rejected': -2.1376655101776123, 'rewards/accuracies': 0.8382812738418579, 'rewards/margins': 0.32112371921539307, 'logps/rejected': -7.406470775604248, 'logps/chosen': -6.862583160400391, 'logits/rejected': -2.9883642196655273, 'logits/chosen': -3.0448741912841797, 'epoch': 2.52}\n{'loss': 0.3884, 'grad_norm': 0.7876192331314087, 'learning_rate': 0.001, 'rewards/chosen': -1.8328254222869873, 'rewards/rejected': -2.1555533409118652, 'rewards/accuracies': 0.8343750238418579, 'rewards/margins': 0.32272762060165405, 'logps/rejected': -7.44607400894165, 'logps/chosen': -6.880815505981445, 'logits/rejected': -3.2943224906921387, 'logits/chosen': -3.341554641723633, 'epoch': 2.57}\n{'loss': 0.378, 'grad_norm': 0.7280998826026917, 'learning_rate': 0.001, 'rewards/chosen': -1.857102632522583, 'rewards/rejected': -2.1828298568725586, 'rewards/accuracies': 0.8389062285423279, 'rewards/margins': 0.3257271647453308, 'logps/rejected': -7.4743523597717285, 'logps/chosen': -6.917442798614502, 'logits/rejected': -3.4977903366088867, 'logits/chosen': -3.5365188121795654, 'epoch': 2.61}\n{'loss': 0.3973, 'grad_norm': 0.6715676188468933, 'learning_rate': 0.001, 'rewards/chosen': -1.8657505512237549, 'rewards/rejected': -2.1819286346435547, 'rewards/accuracies': 0.8264062404632568, 'rewards/margins': 0.3161781132221222, 'logps/rejected': -7.489800453186035, 'logps/chosen': -6.93882942199707, 'logits/rejected': -2.7152926921844482, 'logits/chosen': -2.881714105606079, 'epoch': 2.66}\n{'loss': 0.3989, 'grad_norm': 0.6153255105018616, 'learning_rate': 0.001, 'rewards/chosen': -1.8311740159988403, 'rewards/rejected': -2.151577949523926, 'rewards/accuracies': 0.8274999856948853, 'rewards/margins': 0.32040396332740784, 'logps/rejected': -7.446674823760986, 'logps/chosen': -6.870075702667236, 'logits/rejected': -2.8585638999938965, 'logits/chosen': -3.026399612426758, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [56:54<05:52,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.86it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.84it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.82it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.78it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.76it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.79it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5647662281990051, 'eval_runtime': 32.661, 'eval_samples_per_second': 228.683, 'eval_steps_per_second': 1.806, 'eval_rewards/chosen': -1.8999431133270264, 'eval_rewards/rejected': -2.145160675048828, 'eval_rewards/accuracies': 0.7337659001350403, 'eval_rewards/margins': 0.24521739780902863, 'eval_logps/rejected': -7.418041706085205, 'eval_logps/chosen': -6.9876837730407715, 'eval_logits/rejected': -3.4188802242279053, 'eval_logits/chosen': -3.5532164573669434, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [57:26<05:52,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 0.4126, 'grad_norm': 0.9374139308929443, 'learning_rate': 0.001, 'rewards/chosen': -1.836951494216919, 'rewards/rejected': -2.147165298461914, 'rewards/accuracies': 0.8218749761581421, 'rewards/margins': 0.3102141320705414, 'logps/rejected': -7.4242048263549805, 'logps/chosen': -6.871545314788818, 'logits/rejected': -3.0514304637908936, 'logits/chosen': -3.263551950454712, 'epoch': 2.75}\n{'loss': 0.4, 'grad_norm': 0.6523117423057556, 'learning_rate': 0.001, 'rewards/chosen': -1.8741028308868408, 'rewards/rejected': -2.1909189224243164, 'rewards/accuracies': 0.8224999904632568, 'rewards/margins': 0.3168163001537323, 'logps/rejected': -7.496332168579102, 'logps/chosen': -6.953022003173828, 'logits/rejected': -2.7827703952789307, 'logits/chosen': -2.9122042655944824, 'epoch': 2.8}\n{'loss': 0.4147, 'grad_norm': 0.7251567244529724, 'learning_rate': 0.001, 'rewards/chosen': -1.880194902420044, 'rewards/rejected': -2.1943953037261963, 'rewards/accuracies': 0.8185937404632568, 'rewards/margins': 0.31420060992240906, 'logps/rejected': -7.51718807220459, 'logps/chosen': -6.972750663757324, 'logits/rejected': -3.7600772380828857, 'logits/chosen': -3.819627046585083, 'epoch': 2.84}\n{'loss': 0.4182, 'grad_norm': 0.7247282862663269, 'learning_rate': 0.001, 'rewards/chosen': -1.8690247535705566, 'rewards/rejected': -2.175586462020874, 'rewards/accuracies': 0.8181250095367432, 'rewards/margins': 0.30656176805496216, 'logps/rejected': -7.46307373046875, 'logps/chosen': -6.954039096832275, 'logits/rejected': -3.155334711074829, 'logits/chosen': -3.2615320682525635, 'epoch': 2.89}\n{'loss': 0.4236, 'grad_norm': 0.6587578058242798, 'learning_rate': 0.001, 'rewards/chosen': -1.9102561473846436, 'rewards/rejected': -2.216794967651367, 'rewards/accuracies': 0.8145312666893005, 'rewards/margins': 0.3065389096736908, 'logps/rejected': -7.516983509063721, 'logps/chosen': -7.010168552398682, 'logits/rejected': -3.5668060779571533, 'logits/chosen': -3.657693386077881, 'epoch': 2.93}\n{'loss': 0.427, 'grad_norm': 0.4811348021030426, 'learning_rate': 0.001, 'rewards/chosen': -1.9002488851547241, 'rewards/rejected': -2.200279951095581, 'rewards/accuracies': 0.8121874928474426, 'rewards/margins': 0.30003124475479126, 'logps/rejected': -7.523561954498291, 'logps/chosen': -6.994018077850342, 'logits/rejected': -3.2437899112701416, 'logits/chosen': -3.3503894805908203, 'epoch': 2.98}\n{'train_runtime': 3818.7162, 'train_samples_per_second': 111.478, 'train_steps_per_second': 0.871, 'train_loss': 0.5976862767669292, 'epoch': 3.0}\n100%|█████████████████████████████████████| 3327/3327 [1:03:22<00:00,  1.14s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-3327\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-2218\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-1109","metadata":{"execution":{"iopub.status.busy":"2024-05-27T05:24:36.504307Z","iopub.execute_input":"2024-05-27T05:24:36.504739Z","iopub.status.idle":"2024-05-27T05:38:35.552438Z","shell.execute_reply.started":"2024-05-27T05:24:36.504706Z","shell.execute_reply":"2024-05-27T05:38:35.551238Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2024-05-27 05:24:43.318699: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 05:24:43.318759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 05:24:43.320398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-3327', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.27.05.24'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-3327\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_052456-e37s602h\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.27.05.24\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/e37s602h\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:35, 26.97s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:53<03:06, 26.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:39, 26.60s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:46<02:13, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:13<01:46, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:39<01:19, 26.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:06<00:53, 26.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:32<00:26, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:59<00:00, 26.63s/it]\nmean test reward 0.6361075505675318 +/- 0.009870788744827925 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9937576353549957 from 0.00922983093187213\nmean KL 32.88393407625457 +/- 0.23946677494135984 full 70.87160015271769 +/- 0.18539575423169793\nmedian KL 32.85153579711914 full 70.18509674072266\n2024-05-27 05:29:26.780618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 05:29:26.780683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 05:29:26.782345: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-2218', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.27.05.29'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-2218\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_052936-vfn6aj52\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.27.05.29\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/vfn6aj52\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:34, 26.79s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:53<03:05, 26.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:39, 26.60s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:12<01:45, 26.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:39<01:19, 26.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:05<00:52, 26.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:32<00:26, 26.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:58<00:00, 26.49s/it]\nmean test reward 0.6425838347336644 +/- 0.009835797675802432 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9945091903209686 from 0.00922983093187213\nmean KL 31.021924138896996 +/- 0.23384434277014843 full 68.31061257918675 +/- 0.17584584332646921\nmedian KL 30.767044067382812 full 67.55088806152344\n2024-05-27 05:34:04.277899: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 05:34:04.277967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 05:34:04.279510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-1109', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.27.05.34'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_0_6_lr1e-3-2024.05.27.04.07/checkpoint-1109\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_053413-8oowexdz\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.27.05.34\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/8oowexdz\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:34, 26.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:53<03:05, 26.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:38, 26.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:12<01:46, 26.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:39<01:19, 26.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:05<00:53, 26.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:32<00:26, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:58<00:00, 26.55s/it]\nmean test reward 0.7468324235301225 +/- 0.008915789238129523 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9973801076412201 from 0.00922983093187213\nmean KL 29.17729499703273 +/- 0.23985574634360732 full 65.84842653241422 +/- 0.17519817421575065\nmedian KL 28.800021171569824 full 65.01482772827148\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=roberta16-256_0_02-1epoch --log_with=wandb --init_kl_coef=0.02 --ppo_epochs=1","metadata":{"execution":{"iopub.status.busy":"2024-05-27T05:45:06.048114Z","iopub.execute_input":"2024-05-27T05:45:06.048806Z","iopub.status.idle":"2024-05-27T06:35:36.202364Z","shell.execute_reply.started":"2024-05-27T05:45:06.048773Z","shell.execute_reply":"2024-05-27T06:35:36.201191Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"2024-05-27 05:45:12.434822: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 05:45:12.434885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 05:45:12.436463: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='roberta16-256_0_02-1epoch', dry_run=False, eval_model=None, seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'roberta16-256_0_02-1epoch-2024.05.27.05.45'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.02, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=1, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model lvwerra/gpt2-imdb\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_054522-ys7x8sqj\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroberta16-256_0_02-1epoch-2024.05.27.05.45\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/ys7x8sqj\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\n  0%|                                                    | 0/97 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n  5%|██▎                                         | 5/97 [02:19<42:46, 27.89s/it]--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 204, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n100%|███████████████████████████████████████████| 97/97 [45:31<00:00, 28.16s/it]\nmodel.safetensors: 100%|█████████████████████| 498M/498M [00:10<00:00, 48.3MB/s]\nTraining done!  Start eval\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:27<03:42, 27.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:54<03:10, 27.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:21<02:42, 27.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:48<02:15, 27.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:15<01:47, 26.99s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:09<00:53, 26.96s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:35<00:26, 26.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:02<00:00, 26.97s/it]\nmean test reward 0.8698471887144277 +/- 0.006908722611464011 from 0.447150354645929 +/- 0.010230744800409515\nmedian test reward 0.9987620711326599 from 0.010067821945995092\nmean KL 12.93547330227577 +/- 0.13883159522338212 full 14.423730991004655 +/- 0.1055999166051829\nmedian KL 12.68832540512085 full 13.55678939819336\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=roberta16-256_0_2-full --log_with=wandb --init_kl_coef=0.2 --ppo_epochs=1 --kl_penalty=full --mini_batch_size=128 --gradient_accumulation_steps=2","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:48:32.005525Z","iopub.execute_input":"2024-05-27T06:48:32.006339Z","iopub.status.idle":"2024-05-27T07:29:28.188167Z","shell.execute_reply.started":"2024-05-27T06:48:32.006301Z","shell.execute_reply":"2024-05-27T07:29:28.186872Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2024-05-27 06:48:38.423851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 06:48:38.423909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 06:48:38.425417: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='roberta16-256_0_2-full', dry_run=False, eval_model=None, seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'roberta16-256_0_2-full-2024.05.27.06.48'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='full', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=2, world_size=None, ppo_epochs=1, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model lvwerra/gpt2-imdb\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_064848-os0c6dhn\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroberta16-256_0_2-full-2024.05.27.06.48\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/os0c6dhn\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\n  0%|                                                    | 0/97 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n  5%|██▎                                         | 5/97 [02:20<43:15, 28.21s/it]--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 204, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n100%|███████████████████████████████████████████| 97/97 [37:47<00:00, 23.38s/it]\nmodel.safetensors: 100%|█████████████████████| 498M/498M [00:10<00:00, 49.0MB/s]\nTraining done!  Start eval\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:14<01:57, 14.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:29<01:42, 14.69s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [00:44<01:28, 14.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [00:58<01:13, 14.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [01:13<00:58, 14.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [01:28<00:44, 14.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [01:43<00:29, 14.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [01:58<00:14, 14.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [02:12<00:00, 14.75s/it]\nmean test reward 0.3281510521725674 +/- 0.009444664124816099 from 0.44669779441903756 +/- 0.010232828356261723\nmedian test reward 0.013053103350102901 from 0.00998551631346345\nmean KL -19.878827105690206 +/- 0.9197389912952046 full 55.68498599943187 +/- 0.18998346696805168\nmedian KL -23.443570137023926 full 54.802358627319336\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=roberta16-256_2_0-full --log_with=wandb --init_kl_coef=2 --ppo_epochs=1 --kl_penalty=full --mini_batch_size=128 --gradient_accumulation_steps=2","metadata":{"execution":{"iopub.status.busy":"2024-05-27T09:40:40.381219Z","iopub.execute_input":"2024-05-27T09:40:40.381828Z","iopub.status.idle":"2024-05-27T10:23:41.956788Z","shell.execute_reply.started":"2024-05-27T09:40:40.381801Z","shell.execute_reply":"2024-05-27T10:23:41.955633Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2024-05-27 09:40:50.338464: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 09:40:50.338603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 09:40:50.441560: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='roberta16-256_2_0-full', dry_run=False, eval_model=None, seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'roberta16-256_2_0-full-2024.05.27.09.40'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=2.0, kl_penalty='full', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=2, world_size=None, ppo_epochs=1, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\ntokenizer_config.json: 100%|█████████████████| 17.0/17.0 [00:00<00:00, 80.4kB/s]\nconfig.json: 100%|█████████████████████████████| 577/577 [00:00<00:00, 2.66MB/s]\nvocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 1.15MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.16MB/s]\nspecial_tokens_map.json: 100%|████████████████| 90.0/90.0 [00:00<00:00, 478kB/s]\nDownloading readme: 100%|██████████████████| 7.81k/7.81k [00:00<00:00, 23.6MB/s]\nDownloading data: 100%|████████████████████| 21.0M/21.0M [00:04<00:00, 4.38MB/s]\nDownloading data: 100%|████████████████████| 20.5M/20.5M [00:01<00:00, 19.8MB/s]\nDownloading data: 100%|████████████████████| 42.0M/42.0M [00:01<00:00, 27.2MB/s]\nGenerating train split: 100%|██| 25000/25000 [00:00<00:00, 141451.16 examples/s]\nGenerating test split: 100%|███| 25000/25000 [00:00<00:00, 196288.30 examples/s]\nGenerating unsupervised split: 100%|█| 50000/50000 [00:00<00:00, 193041.08 examp\nFilter: 100%|██████████████████| 25000/25000 [00:00<00:00, 157724.72 examples/s]\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|████████████████████████| 24895/24895 [00:28<00:00, 861.17 examples/s]\nload ref model lvwerra/gpt2-imdb\npytorch_model.bin: 100%|█████████████████████| 548M/548M [00:12<00:00, 42.9MB/s]\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model lvwerra/gpt2-imdb\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_094220-d6krql1h\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroberta16-256_2_0-full-2024.05.27.09.40\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/d6krql1h\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 1.87MB/s]\npytorch_model.bin: 100%|███████████████████| 1.42G/1.42G [00:33<00:00, 42.0MB/s]\ntokenizer_config.json: 100%|████████████████████| 256/256 [00:00<00:00, 531kB/s]\nvocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 4.06MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.16MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 150/150 [00:00<00:00, 429kB/s]\n  0%|                                                    | 0/97 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n  5%|██▎                                         | 5/97 [02:24<43:57, 28.67s/it]--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 204, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n100%|███████████████████████████████████████████| 97/97 [37:13<00:00, 23.02s/it]\nmodel.safetensors: 100%|█████████████████████| 498M/498M [00:30<00:00, 16.5MB/s]\nTraining done!  Start eval\neval batch size 256\nFilter: 100%|████████████████████| 2500/2500 [00:00<00:00, 126502.11 examples/s]\nMap:   3%|▉                           | 83/2487 [00:00<00:02, 815.45 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|██████████████████████████| 2487/2487 [00:02<00:00, 883.59 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:14<01:59, 14.92s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:29<01:43, 14.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [00:44<01:28, 14.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [00:59<01:14, 14.96s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [01:14<00:59, 14.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [01:29<00:44, 14.93s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [01:44<00:29, 14.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [01:59<00:14, 14.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [02:13<00:00, 14.88s/it]\nmean test reward 0.33437338256271687 +/- 0.009487608949856965 from 0.41316713642902414 +/- 0.010137158626051326\nmedian test reward 0.01274011842906475 from 0.0073151609394699335\nmean KL -53.60478745876915 +/- 1.3746147592585372 full 57.03257550630305 +/- 0.19162083272856556\nmedian KL -52.95632362365723 full 56.146772384643555\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=roberta16-256_0_02-full --log_with=wandb --init_kl_coef=0.02 --kl_penalty=full --mini_batch_size=128 --gradient_accumulation_steps=2","metadata":{"execution":{"iopub.status.busy":"2024-05-27T10:28:17.099546Z","iopub.execute_input":"2024-05-27T10:28:17.100129Z","iopub.status.idle":"2024-05-27T11:13:44.838785Z","shell.execute_reply.started":"2024-05-27T10:28:17.100092Z","shell.execute_reply":"2024-05-27T11:13:44.837798Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2024-05-27 10:28:24.889435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 10:28:24.889510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 10:28:24.891253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='roberta16-256_0_02-full', dry_run=False, eval_model=None, seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'roberta16-256_0_02-full-2024.05.27.10.28'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.02, kl_penalty='full', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=2, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model lvwerra/gpt2-imdb\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240527_102848-oxl35ocz\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroberta16-256_0_02-full-2024.05.27.10.28\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/oxl35ocz\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\n  0%|                                                    | 0/97 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n  5%|██▎                                         | 5/97 [02:34<47:18, 30.85s/it]--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 204, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 65%|███████████████████████████▉               | 63/97 [29:00<11:50, 20.90s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (53.30) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n 66%|████████████████████████████▎              | 64/97 [29:21<11:25, 20.78s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (172.33) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (743.56) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n 97%|█████████████████████████████████████████▋ | 94/97 [39:24<01:00, 20.21s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (71.64) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (231.69) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (1412.16) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (7699.60) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (8667.84) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1223: UserWarning: The average ratio of batch (651.33) exceeds threshold 10.00. Skipping batch.\n  warnings.warn(\n100%|███████████████████████████████████████████| 97/97 [40:40<00:00, 25.16s/it]\nmodel.safetensors: 100%|█████████████████████| 498M/498M [00:22<00:00, 22.1MB/s]\nTraining done!  Start eval\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:22<02:56, 22.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:43<02:30, 21.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:04<02:07, 21.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:24<01:45, 21.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [01:45<01:23, 20.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:06<01:02, 20.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [02:26<00:41, 20.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [02:47<00:20, 20.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:08<00:00, 20.90s/it]\nmean test reward 0.9793168142020678 +/- 0.002837623466624597 from 0.4336433543541413 +/- 0.0102164622103725\nmedian test reward 0.998737633228302 from 0.008541114628314972\nmean KL -87.46819292816024 +/- 0.7813509082161353 full 153.38084413276778 +/- 0.43684008787049233\nmedian KL -88.96494674682617 full 156.4104461669922\n","output_type":"stream"}]}]}