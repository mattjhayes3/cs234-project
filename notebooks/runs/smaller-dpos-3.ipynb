{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !conda install -y gdown\n# print('hi')\n!git clone https://github.com/mattjhayes3/cs234-project.git\n%cd /kaggle/working/cs234-project\n!pip install wandb\nimport wandb\n\nwandb.login(key=\"KEY\")\n# wandb.init()\n\nfrom huggingface_hub import notebook_login,login\n\n# notebook_login(\"KEY\")\nlogin(\"KEY\")\n\n# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n!mkdir results\n!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n!pip install git+https://github.com/mattjhayes3/trl.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-30T05:36:00.446987Z","iopub.execute_input":"2024-05-30T05:36:00.447418Z","iopub.status.idle":"2024-05-30T05:37:14.284004Z","shell.execute_reply.started":"2024-05-30T05:36:00.447381Z","shell.execute_reply":"2024-05-30T05:37:14.282783Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cloning into 'cs234-project'...\nremote: Enumerating objects: 168, done.\u001b[K\nremote: Counting objects: 100% (168/168), done.\u001b[K\nremote: Compressing objects: 100% (105/105), done.\u001b[K\nremote: Total 168 (delta 70), reused 140 (delta 44), pack-reused 0\u001b[K\nReceiving objects: 100% (168/168), 36.81 MiB | 9.99 MiB/s, done.\nResolving deltas: 100% (70/70), done.\nUpdating files: 100% (63/63), done.\nEncountered 8 file(s) that should have been pointers, but weren't:\n\tgpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/data-00000-of-00001.arrow\n\tgpt2-imdb-16-token-pref-pairs-3-choose-2-div-3_tokenized/train/data-00000-of-00001.arrow\n\tgpt2-imdb-16-token-pref-pairs-3-distinct-pairs_tokenized/train/data-00000-of-00001.arrow\n\tgpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/cache-13e54457644d5a49.arrow\n\tgpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/cache-33e29b9c6ccd8398.arrow\n\tgpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/data-00000-of-00001.arrow\n\tpref_pairs_16_token_tokenized_split/data-00000-of-00002.arrow\n\tpref_pairs_16_token_tokenized_split/data-00001-of-00002.arrow\n/kaggle/working/cs234-project\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nCollecting git+https://github.com/mattjhayes3/trl.git\n  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-4vks_hyv\n  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-4vks_hyv\n  Resolved https://github.com/mattjhayes3/trl.git to commit 2719f4cd3aea1c0d75ab12d6a2a7ba5697c99e95\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.39.3)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.29.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.18.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.1)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=dpo_6c2_div6_actual-0_2  --beta=0.2 --dataset_name=/kaggle/working/cs234-project/gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=100 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4","metadata":{"execution":{"iopub.status.busy":"2024-05-30T05:38:23.018860Z","iopub.execute_input":"2024-05-30T05:38:23.019757Z","iopub.status.idle":"2024-05-30T05:58:59.023378Z","shell.execute_reply.started":"2024-05-30T05:38:23.019717Z","shell.execute_reply":"2024-05-30T05:58:59.022053Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2024-05-30 05:38:30.669457: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 05:38:30.669528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 05:38:30.671105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 24895\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stderr","text":"wandb: WARNING current working directory has been invalidated\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240530_053837-xphjcr36\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_6c2_div6_actual-0_2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/xphjcr36\u001b[0m\n  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 4.666251182556152, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -60.50761032104492, 'logps/chosen': -62.50612258911133, 'logits/rejected': -37.702693939208984, 'logits/chosen': -36.72541809082031, 'epoch': 0.01}\n{'loss': 0.6914, 'grad_norm': 4.933937072753906, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.0015919029247015715, 'rewards/rejected': -0.0018327207071706653, 'rewards/accuracies': 0.6666135191917419, 'rewards/margins': 0.0034246237482875586, 'logps/rejected': -59.970252990722656, 'logps/chosen': -60.23571014404297, 'logits/rejected': -38.22844314575195, 'logits/chosen': -37.12561798095703, 'epoch': 0.27}\n{'loss': 0.682, 'grad_norm': 4.301088809967041, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.00808322336524725, 'rewards/rejected': -0.014949246309697628, 'rewards/accuracies': 0.6934375166893005, 'rewards/margins': 0.02303246781229973, 'logps/rejected': -60.30775833129883, 'logps/chosen': -60.01335906982422, 'logits/rejected': -37.7647819519043, 'logits/chosen': -36.87879943847656, 'epoch': 0.54}\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 100/740 [01:49<11:32,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.94it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.89it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.85it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6722241044044495, 'eval_runtime': 5.4591, 'eval_samples_per_second': 228.06, 'eval_steps_per_second': 1.832, 'eval_rewards/chosen': 0.012282684445381165, 'eval_rewards/rejected': -0.03121127188205719, 'eval_rewards/accuracies': 0.7279990315437317, 'eval_rewards/margins': 0.043493956327438354, 'eval_logps/rejected': -60.69384765625, 'eval_logps/chosen': -60.112213134765625, 'eval_logits/rejected': -38.030696868896484, 'eval_logits/chosen': -37.12879943847656, 'epoch': 0.54}\n 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                   | 100/740 [01:54<11:32,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'loss': 0.6645, 'grad_norm': 4.7674174308776855, 'learning_rate': 2e-06, 'rewards/chosen': 0.011552780866622925, 'rewards/rejected': -0.04993312805891037, 'rewards/accuracies': 0.7120312452316284, 'rewards/margins': 0.06148591637611389, 'logps/rejected': -60.48163986206055, 'logps/chosen': -59.78042984008789, 'logits/rejected': -37.33653259277344, 'logits/chosen': -36.56099319458008, 'epoch': 0.81}\n{'loss': 0.6393, 'grad_norm': 4.449898719787598, 'learning_rate': 2e-06, 'rewards/chosen': -0.003269368316978216, 'rewards/rejected': -0.12544052302837372, 'rewards/accuracies': 0.7213169932365417, 'rewards/margins': 0.12217115610837936, 'logps/rejected': -60.66606903076172, 'logps/chosen': -60.28864288330078, 'logits/rejected': -37.23390579223633, 'logits/chosen': -36.48197937011719, 'epoch': 1.08}\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 200/740 [03:45<09:47,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.87it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6274741291999817, 'eval_runtime': 5.4519, 'eval_samples_per_second': 228.362, 'eval_steps_per_second': 1.834, 'eval_rewards/chosen': -0.021298564970493317, 'eval_rewards/rejected': -0.1744583547115326, 'eval_rewards/accuracies': 0.7242859601974487, 'eval_rewards/margins': 0.15315979719161987, 'eval_logps/rejected': -61.41008758544922, 'eval_logps/chosen': -60.280113220214844, 'eval_logits/rejected': -37.443031311035156, 'eval_logits/chosen': -36.65310287475586, 'epoch': 1.08}\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 200/740 [03:50<09:47,  1.09s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'loss': 0.6082, 'grad_norm': 4.090348720550537, 'learning_rate': 2e-06, 'rewards/chosen': -0.02987271547317505, 'rewards/rejected': -0.2329227775335312, 'rewards/accuracies': 0.7456250190734863, 'rewards/margins': 0.20305007696151733, 'logps/rejected': -61.22467803955078, 'logps/chosen': -60.074214935302734, 'logits/rejected': -37.0490608215332, 'logits/chosen': -36.185447692871094, 'epoch': 1.35}\n{'loss': 0.5932, 'grad_norm': 3.7993195056915283, 'learning_rate': 2e-06, 'rewards/chosen': -0.08234183490276337, 'rewards/rejected': -0.3358515799045563, 'rewards/accuracies': 0.7412499785423279, 'rewards/margins': 0.2535097599029541, 'logps/rejected': -61.805904388427734, 'logps/chosen': -60.52070236206055, 'logits/rejected': -36.55487823486328, 'logits/chosen': -35.828975677490234, 'epoch': 1.62}\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 300/740 [05:38<07:54,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.59it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.53it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.19it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5907066464424133, 'eval_runtime': 5.4611, 'eval_samples_per_second': 227.976, 'eval_steps_per_second': 1.831, 'eval_rewards/chosen': -0.12859424948692322, 'eval_rewards/rejected': -0.3987501263618469, 'eval_rewards/accuracies': 0.7326865196228027, 'eval_rewards/margins': 0.2701558470726013, 'eval_logps/rejected': -62.53154373168945, 'eval_logps/chosen': -60.81659698486328, 'eval_logits/rejected': -36.66461944580078, 'eval_logits/chosen': -35.99897003173828, 'epoch': 1.62}\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 300/740 [05:44<07:54,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'loss': 0.5823, 'grad_norm': 3.9935028553009033, 'learning_rate': 2e-06, 'rewards/chosen': -0.14427664875984192, 'rewards/rejected': -0.44588223099708557, 'rewards/accuracies': 0.7332812547683716, 'rewards/margins': 0.30160555243492126, 'logps/rejected': -62.44831848144531, 'logps/chosen': -60.752803802490234, 'logits/rejected': -35.99872970581055, 'logits/chosen': -35.49872970581055, 'epoch': 1.89}\n{'loss': 0.5572, 'grad_norm': 3.8254377841949463, 'learning_rate': 2e-06, 'rewards/chosen': -0.1855662316083908, 'rewards/rejected': -0.5623648166656494, 'rewards/accuracies': 0.7607876658439636, 'rewards/margins': 0.376798540353775, 'logps/rejected': -63.25053405761719, 'logps/chosen': -61.1225700378418, 'logits/rejected': -36.23422622680664, 'logits/chosen': -35.60282897949219, 'epoch': 2.16}\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 400/740 [07:34<06:07,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.19it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.94it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.89it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5658398270606995, 'eval_runtime': 5.4951, 'eval_samples_per_second': 226.567, 'eval_steps_per_second': 1.82, 'eval_rewards/chosen': -0.22944679856300354, 'eval_rewards/rejected': -0.5933295488357544, 'eval_rewards/accuracies': 0.7444052696228027, 'eval_rewards/margins': 0.36388278007507324, 'eval_logps/rejected': -63.50443649291992, 'eval_logps/chosen': -61.32086181640625, 'eval_logits/rejected': -36.286720275878906, 'eval_logits/chosen': -35.68805694580078, 'epoch': 2.16}\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 400/740 [07:40<06:07,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'loss': 0.5385, 'grad_norm': 3.817065477371216, 'learning_rate': 2e-06, 'rewards/chosen': -0.22662872076034546, 'rewards/rejected': -0.6675916910171509, 'rewards/accuracies': 0.7764062285423279, 'rewards/margins': 0.4409630596637726, 'logps/rejected': -63.34330749511719, 'logps/chosen': -61.45826721191406, 'logits/rejected': -35.66251754760742, 'logits/chosen': -35.09193420410156, 'epoch': 2.43}\n{'loss': 0.5286, 'grad_norm': 3.4708962440490723, 'learning_rate': 2e-06, 'rewards/chosen': -0.2652599811553955, 'rewards/rejected': -0.7452523708343506, 'rewards/accuracies': 0.7720312476158142, 'rewards/margins': 0.47999244928359985, 'logps/rejected': -63.98807144165039, 'logps/chosen': -61.44386291503906, 'logits/rejected': -35.740478515625, 'logits/chosen': -35.325653076171875, 'epoch': 2.7}\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 500/740 [09:28<04:19,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.89it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.85it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5468023419380188, 'eval_runtime': 5.4654, 'eval_samples_per_second': 227.798, 'eval_steps_per_second': 1.83, 'eval_rewards/chosen': -0.3340756595134735, 'eval_rewards/rejected': -0.7821601629257202, 'eval_rewards/accuracies': 0.7428427934646606, 'eval_rewards/margins': 0.4480845332145691, 'eval_logps/rejected': -64.44859313964844, 'eval_logps/chosen': -61.8440055847168, 'eval_logits/rejected': -35.97536849975586, 'eval_logits/chosen': -35.43003845214844, 'epoch': 2.7}\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 500/740 [09:34<04:19,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'loss': 0.5233, 'grad_norm': 3.694613456726074, 'learning_rate': 2e-06, 'rewards/chosen': -0.31009167432785034, 'rewards/rejected': -0.8276513814926147, 'rewards/accuracies': 0.7737500071525574, 'rewards/margins': 0.5175597667694092, 'logps/rejected': -63.940093994140625, 'logps/chosen': -61.2987174987793, 'logits/rejected': -35.65168762207031, 'logits/chosen': -35.009761810302734, 'epoch': 2.97}\n{'loss': 0.4986, 'grad_norm': 3.815955877304077, 'learning_rate': 2e-06, 'rewards/chosen': -0.337560772895813, 'rewards/rejected': -0.9259523153305054, 'rewards/accuracies': 0.8039827942848206, 'rewards/margins': 0.5883915424346924, 'logps/rejected': -64.6662368774414, 'logps/chosen': -61.56011199951172, 'logits/rejected': -35.14814376831055, 'logits/chosen': -34.656578063964844, 'epoch': 3.24}\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 600/740 [11:24<02:32,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.86it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5317201614379883, 'eval_runtime': 5.4573, 'eval_samples_per_second': 228.135, 'eval_steps_per_second': 1.832, 'eval_rewards/chosen': -0.42300501465797424, 'eval_rewards/rejected': -0.9424047470092773, 'eval_rewards/accuracies': 0.7480174899101257, 'eval_rewards/margins': 0.5193997621536255, 'eval_logps/rejected': -65.24981689453125, 'eval_logps/chosen': -62.28864669799805, 'eval_logits/rejected': -35.69816970825195, 'eval_logits/chosen': -35.18982696533203, 'epoch': 3.24}\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 600/740 [11:30<02:32,  1.09s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'loss': 0.494, 'grad_norm': 3.670046806335449, 'learning_rate': 2e-06, 'rewards/chosen': -0.37573227286338806, 'rewards/rejected': -0.9961109757423401, 'rewards/accuracies': 0.7965624928474426, 'rewards/margins': 0.620378851890564, 'logps/rejected': -65.2935562133789, 'logps/chosen': -62.28810119628906, 'logits/rejected': -35.376068115234375, 'logits/chosen': -34.833763122558594, 'epoch': 3.51}\n{'loss': 0.4812, 'grad_norm': 3.6513400077819824, 'learning_rate': 2e-06, 'rewards/chosen': -0.4033198654651642, 'rewards/rejected': -1.0748766660690308, 'rewards/accuracies': 0.8051562309265137, 'rewards/margins': 0.671556830406189, 'logps/rejected': -65.38812255859375, 'logps/chosen': -62.010780334472656, 'logits/rejected': -35.28469467163086, 'logits/chosen': -34.787532806396484, 'epoch': 3.78}\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 700/740 [13:18<00:43,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.20it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.87it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5200245380401611, 'eval_runtime': 5.4458, 'eval_samples_per_second': 228.616, 'eval_steps_per_second': 1.836, 'eval_rewards/chosen': -0.48611292243003845, 'eval_rewards/rejected': -1.0604305267333984, 'eval_rewards/accuracies': 0.7553427815437317, 'eval_rewards/margins': 0.5743175745010376, 'eval_logps/rejected': -65.83995056152344, 'eval_logps/chosen': -62.60418701171875, 'eval_logits/rejected': -35.429691314697266, 'eval_logits/chosen': -34.95067596435547, 'epoch': 3.78}\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 700/740 [13:24<00:43,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n{'train_runtime': 866.1964, 'train_samples_per_second': 109.213, 'train_steps_per_second': 0.854, 'train_loss': 0.5719636781795605, 'epoch': 4.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 740/740 [14:09<00:00,  1.15s/it]\nEvaling every 185\nDownloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.81k/7.81k [00:00<00:00, 8.83MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.0M/21.0M [00:00<00:00, 25.9MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:00<00:00, 30.5MB/s]\nDownloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.0M/42.0M [00:01<00:00, 31.8MB/s]\nGenerating train split: 100%|‚ñà‚ñà| 25000/25000 [00:00<00:00, 127990.47 examples/s]\nGenerating test split: 100%|‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 158316.96 examples/s]\nGenerating unsupervised split: 100%|‚ñà| 50000/50000 [00:00<00:00, 150225.25 examp\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 124156.63 examples/s]\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24895/24895 [00:35<00:00, 710.81 examples/s]\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 687/687 [00:00<00:00, 1.62MB/s]\npytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [00:23<00:00, 60.2MB/s]\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 256/256 [00:00<00:00, 715kB/s]\nvocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 798k/798k [00:00<00:00, 16.8MB/s]\nmerges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 29.0MB/s]\nspecial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [00:00<00:00, 409kB/s]\neval batch size 256\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [00:00<00:00, 107990.40 examples/s]\nMap:   3%|‚ñä                           | 72/2487 [00:00<00:03, 685.78 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2487/2487 [00:06<00:00, 363.38 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:32<04:17, 32.21s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [01:02<03:38, 31.20s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:33<03:06, 31.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [02:04<02:34, 30.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:34<02:03, 30.78s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/dpo.py\", line 199, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(f\"{training_args.output_dir}/checkpoint-{epoch*steps}\", f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 260, in eval\n    run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [03:03<01:30, 30.17s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:32<00:59, 29.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [04:01<00:29, 29.35s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:30<00:00, 30.01s/it]\nTraceback (most recent call last):\n  File \"/kaggle/working/cs234-project/dpo.py\", line 199, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(f\"{training_args.output_dir}/checkpoint-{epoch*steps}\", f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 260, in eval\n    run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 251, in run\n    test_stats.to_csv(f'./results/{full_name}.csv')\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 333, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\", line 3967, in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/formats/format.py\", line 1014, in to_csv\n    csv_formatter.save()\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 251, in save\n    with get_handle(\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/common.py\", line 749, in get_handle\n    check_parent_directory(str(handle))\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/common.py\", line 616, in check_parent_directory\n    raise OSError(rf\"Cannot save file into a non-existent directory: '{parent}'\")\nOSError: Cannot save file into a non-existent directory: 'results/dpo_6c2_div6_actual-0_2-2024.05.30.05.38'\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-740\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-555\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-370\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-185","metadata":{"execution":{"iopub.status.busy":"2024-05-30T06:02:55.005995Z","iopub.execute_input":"2024-05-30T06:02:55.007166Z","iopub.status.idle":"2024-05-30T06:22:25.808979Z","shell.execute_reply.started":"2024-05-30T06:02:55.007114Z","shell.execute_reply":"2024-05-30T06:22:25.807746Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"2024-05-30 06:03:02.010526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:03:02.010609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:03:02.012472: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-740', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.03'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 152878.60 examples/s]\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24895/24895 [00:34<00:00, 720.09 examples/s]\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-740\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\nFilter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [00:00<00:00, 109613.74 examples/s]\nMap:   3%|‚ñä                           | 69/2487 [00:00<00:03, 670.87 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2487/2487 [00:03<00:00, 720.78 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:27<03:43, 27.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:55<03:13, 27.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:23<02:46, 27.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:50<02:17, 27.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:17<01:49, 27.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:44<01:22, 27.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:12<00:54, 27.42s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:40<00:27, 27.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:07<00:00, 27.50s/it]\nmean test reward 0.8251224296500621 +/- 0.007818480700374398 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9986090958118439 from 0.00922983093187213\nmean KL 0.6464526310511347 +/- 0.07191851636354106 full 3.703912031940288 +/- 0.030298952680330962\nmedian KL 1.014724612236023 full 3.4716368913650513\n2024-05-30 06:08:10.295021: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:08:10.295098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:08:10.296787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-555', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.08'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-555\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:27<03:42, 27.78s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:55<03:15, 27.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:23<02:47, 27.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:52<02:20, 28.17s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:21<01:53, 28.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:49<01:25, 28.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:18<00:57, 28.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:47<00:28, 28.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:16<00:00, 28.48s/it]\nmean test reward 0.7997476691850048 +/- 0.008239322241779262 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9985056221485138 from 0.00922983093187213\nmean KL 1.1145201548998658 +/- 0.05977391895325829 full 2.9328911493584098 +/- 0.026208124520107536\nmedian KL 1.3972275257110596 full 2.7252472639083862\n2024-05-30 06:12:50.900497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:12:50.900567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:12:50.902362: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-370', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.12'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-370\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:30<04:02, 30.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [01:00<03:31, 30.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:31<03:02, 30.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [02:02<02:33, 30.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:32<02:02, 30.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [03:03<01:31, 30.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:34<01:01, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [04:04<00:30, 30.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:35<00:00, 30.62s/it]\nmean test reward 0.7448346711718538 +/- 0.008979632828818656 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9980942010879517 from 0.00922983093187213\nmean KL 1.1442285464339268 +/- 0.043028407954771176 full 1.8186404579965811 +/- 0.018289033117338493\nmedian KL 1.2582895755767822 full 1.6589776873588562\n2024-05-30 06:17:50.755303: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:17:50.755364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:17:50.757188: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-185', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.17'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_2-2024.05.30.05.38/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:52, 29.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:57<03:21, 28.78s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:26<02:53, 28.96s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:55<02:25, 29.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:24<01:55, 28.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:53<01:26, 28.92s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:22<00:57, 28.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:50<00:28, 28.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:18<00:00, 28.73s/it]\nmean test reward 0.6038798631303481 +/- 0.010074721516629595 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9940570592880249 from 0.00922983093187213\nmean KL 0.45733581440840176 +/- 0.02067418709460136 full 0.42497515909781214 +/- 0.005009939042896116\nmedian KL 0.48027941584587097 full 0.37179042398929596\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=dpo_6c2_div6_actual-0_6  --beta=0.6 --dataset_name=/kaggle/working/cs234-project/gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=200 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4","metadata":{"execution":{"iopub.status.busy":"2024-05-30T06:25:43.929431Z","iopub.execute_input":"2024-05-30T06:25:43.930014Z","iopub.status.idle":"2024-05-30T06:42:06.246276Z","shell.execute_reply.started":"2024-05-30T06:25:43.929969Z","shell.execute_reply":"2024-05-30T06:42:06.244981Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2024-05-30 06:25:51.229523: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:25:51.229600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:25:51.231327: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 24895\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stderr","text":"wandb: WARNING current working directory has been invalidated\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240530_062557-kbnmzu94\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_6c2_div6_actual-0_6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/kbnmzu94\u001b[0m\n  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 13.998750686645508, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -60.50761032104492, 'logps/chosen': -62.50612258911133, 'logits/rejected': -37.702693939208984, 'logits/chosen': -36.72541809082031, 'epoch': 0.01}\n{'loss': 0.6882, 'grad_norm': 14.674250602722168, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.004752668086439371, 'rewards/rejected': -0.005462185014039278, 'rewards/accuracies': 0.667570173740387, 'rewards/margins': 0.01021485310047865, 'logps/rejected': -59.97019577026367, 'logps/chosen': -60.235748291015625, 'logits/rejected': -38.228633880615234, 'logits/chosen': -37.12578582763672, 'epoch': 0.27}\n{'loss': 0.6626, 'grad_norm': 12.520139694213867, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.02409674972295761, 'rewards/rejected': -0.04278497397899628, 'rewards/accuracies': 0.6973437666893005, 'rewards/margins': 0.06688172370195389, 'logps/rejected': -60.304317474365234, 'logps/chosen': -60.013614654541016, 'logits/rejected': -37.77629470825195, 'logits/chosen': -36.88972091674805, 'epoch': 0.54}\n{'loss': 0.6237, 'grad_norm': 13.388449668884277, 'learning_rate': 2e-06, 'rewards/chosen': 0.039339035749435425, 'rewards/rejected': -0.12899182736873627, 'rewards/accuracies': 0.7149999737739563, 'rewards/margins': 0.1683308631181717, 'logps/rejected': -60.446956634521484, 'logps/chosen': -59.77263641357422, 'logits/rejected': -37.398414611816406, 'logits/chosen': -36.618141174316406, 'epoch': 0.81}\n{'loss': 0.5777, 'grad_norm': 12.392179489135742, 'learning_rate': 2e-06, 'rewards/chosen': 0.03191289305686951, 'rewards/rejected': -0.27909719944000244, 'rewards/accuracies': 0.7403188943862915, 'rewards/margins': 0.31101009249687195, 'logps/rejected': -60.5040283203125, 'logps/chosen': -60.2191047668457, 'logits/rejected': -37.387725830078125, 'logits/chosen': -36.616756439208984, 'epoch': 1.08}\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 200/740 [03:37<09:44,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.61it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.05it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.56657874584198, 'eval_runtime': 5.4184, 'eval_samples_per_second': 229.771, 'eval_steps_per_second': 1.846, 'eval_rewards/chosen': 0.009772511199116707, 'eval_rewards/rejected': -0.35283225774765015, 'eval_rewards/accuracies': 0.7394237518310547, 'eval_rewards/margins': 0.3626047670841217, 'eval_logps/rejected': -61.12584686279297, 'eval_logps/chosen': -60.15732955932617, 'eval_logits/rejected': -37.65165710449219, 'eval_logits/chosen': -36.82301330566406, 'epoch': 1.08}\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 200/740 [03:43<09:44,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.5266, 'grad_norm': 11.200895309448242, 'learning_rate': 2e-06, 'rewards/chosen': 0.03707177937030792, 'rewards/rejected': -0.44229456782341003, 'rewards/accuracies': 0.7796875238418579, 'rewards/margins': 0.47936639189720154, 'logps/rejected': -60.79722213745117, 'logps/chosen': -59.86305618286133, 'logits/rejected': -37.290645599365234, 'logits/chosen': -36.37708282470703, 'epoch': 1.35}\n{'loss': 0.5142, 'grad_norm': 10.433982849121094, 'learning_rate': 2e-06, 'rewards/chosen': 0.010772986337542534, 'rewards/rejected': -0.534378707408905, 'rewards/accuracies': 0.7810937762260437, 'rewards/margins': 0.5451517105102539, 'logps/rejected': -61.017269134521484, 'logps/chosen': -60.091041564941406, 'logits/rejected': -36.93905258178711, 'logits/chosen': -36.1441764831543, 'epoch': 1.62}\n{'loss': 0.506, 'grad_norm': 11.059402465820312, 'learning_rate': 2e-06, 'rewards/chosen': -0.01296734344214201, 'rewards/rejected': -0.6156978607177734, 'rewards/accuracies': 0.7749999761581421, 'rewards/margins': 0.6027305126190186, 'logps/rejected': -61.245079040527344, 'logps/chosen': -60.053035736083984, 'logits/rejected': -36.517391204833984, 'logits/chosen': -35.92267990112305, 'epoch': 1.89}\n{'loss': 0.4627, 'grad_norm': 9.748176574707031, 'learning_rate': 2e-06, 'rewards/chosen': 0.006002078764140606, 'rewards/rejected': -0.7221652865409851, 'rewards/accuracies': 0.8231185674667358, 'rewards/margins': 0.7281674146652222, 'logps/rejected': -61.64231872558594, 'logps/chosen': -60.18473815917969, 'logits/rejected': -36.911659240722656, 'logits/chosen': -36.16748809814453, 'epoch': 2.16}\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 400/740 [07:21<06:05,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.54it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.19it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5044671893119812, 'eval_runtime': 5.4302, 'eval_samples_per_second': 229.273, 'eval_steps_per_second': 1.842, 'eval_rewards/chosen': -0.07141022384166718, 'eval_rewards/rejected': -0.7056295871734619, 'eval_rewards/accuracies': 0.7645245790481567, 'eval_rewards/margins': 0.6342193484306335, 'eval_logps/rejected': -61.713844299316406, 'eval_logps/chosen': -60.29265213012695, 'eval_logits/rejected': -37.04064178466797, 'eval_logits/chosen': -36.287315368652344, 'epoch': 2.16}\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 400/740 [07:26<06:05,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.4357, 'grad_norm': 9.613112449645996, 'learning_rate': 2e-06, 'rewards/chosen': 0.010289914906024933, 'rewards/rejected': -0.8156777024269104, 'rewards/accuracies': 0.8440625071525574, 'rewards/margins': 0.82596755027771, 'logps/rejected': -61.36481475830078, 'logps/chosen': -60.30796813964844, 'logits/rejected': -36.443233489990234, 'logits/chosen': -35.72506332397461, 'epoch': 2.43}\n{'loss': 0.424, 'grad_norm': 8.960654258728027, 'learning_rate': 2e-06, 'rewards/chosen': 0.019204555079340935, 'rewards/rejected': -0.8597559928894043, 'rewards/accuracies': 0.8448437452316284, 'rewards/margins': 0.8789605498313904, 'logps/rejected': -61.694732666015625, 'logps/chosen': -60.085548400878906, 'logits/rejected': -36.634521484375, 'logits/chosen': -36.06736755371094, 'epoch': 2.7}\n{'loss': 0.4207, 'grad_norm': 10.015071868896484, 'learning_rate': 2e-06, 'rewards/chosen': -0.002260357839986682, 'rewards/rejected': -0.9250823855400085, 'rewards/accuracies': 0.84765625, 'rewards/margins': 0.9228220582008362, 'logps/rejected': -61.34364318847656, 'logps/chosen': -59.75202178955078, 'logits/rejected': -36.71080780029297, 'logits/chosen': -35.90814971923828, 'epoch': 2.97}\n{'loss': 0.3753, 'grad_norm': 9.243759155273438, 'learning_rate': 2e-06, 'rewards/chosen': 0.029170071706175804, 'rewards/rejected': -1.027238368988037, 'rewards/accuracies': 0.8843653798103333, 'rewards/margins': 1.056408405303955, 'logps/rejected': -61.74855041503906, 'logps/chosen': -59.82368087768555, 'logits/rejected': -36.34205627441406, 'logits/chosen': -35.68872833251953, 'epoch': 3.24}\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 600/740 [11:04<02:31,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.05it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.87it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.46946290135383606, 'eval_runtime': 5.4269, 'eval_samples_per_second': 229.415, 'eval_steps_per_second': 1.843, 'eval_rewards/chosen': -0.13765859603881836, 'eval_rewards/rejected': -0.9618613123893738, 'eval_rewards/accuracies': 0.78523188829422, 'eval_rewards/margins': 0.8242027163505554, 'eval_logps/rejected': -62.140892028808594, 'eval_logps/chosen': -60.40305709838867, 'eval_logits/rejected': -36.95551300048828, 'eval_logits/chosen': -36.25090408325195, 'epoch': 3.24}\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 600/740 [11:10<02:31,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.3717, 'grad_norm': 9.338099479675293, 'learning_rate': 2e-06, 'rewards/chosen': 0.014997401274740696, 'rewards/rejected': -1.082654595375061, 'rewards/accuracies': 0.8839062452316284, 'rewards/margins': 1.0976519584655762, 'logps/rejected': -62.117427825927734, 'logps/chosen': -60.384437561035156, 'logits/rejected': -36.674251556396484, 'logits/chosen': -35.954647064208984, 'epoch': 3.51}\n{'loss': 0.3596, 'grad_norm': 8.785632133483887, 'learning_rate': 2e-06, 'rewards/chosen': 0.013705314137041569, 'rewards/rejected': -1.1452487707138062, 'rewards/accuracies': 0.8892187476158142, 'rewards/margins': 1.15895414352417, 'logps/rejected': -61.922481536865234, 'logps/chosen': -59.971336364746094, 'logits/rejected': -36.628814697265625, 'logits/chosen': -35.9304313659668, 'epoch': 3.78}\n{'train_runtime': 839.9087, 'train_samples_per_second': 112.631, 'train_steps_per_second': 0.881, 'train_loss': 0.48886366212690197, 'epoch': 4.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 740/740 [13:43<00:00,  1.11s/it]\nEvaling every 185\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:30<04:01, 30.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [01:00<03:32, 30.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:30<03:01, 30.31s/it]^C\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:51<03:42, 37.12s/it]\nTraceback (most recent call last):\n  File \"/kaggle/working/cs234-project/dpo.py\", line 199, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(f\"{training_args.output_dir}/checkpoint-{epoch*steps}\", f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 260, in eval\n    run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 223, in run\n    response_tensors, ref_response_tensors = ppo_trainer.generate(\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py\", line 478, in generate\n    ref_response = self._generate_batched(\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py\", line 556, in _generate_batched\n    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/trl/models/modeling_value_head.py\", line 204, in generate\n    return self.pretrained_model.generate(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1383, in generate\n    and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-740\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-555\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-370\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-185","metadata":{"execution":{"iopub.status.busy":"2024-05-30T06:42:06.248378Z","iopub.execute_input":"2024-05-30T06:42:06.248719Z","iopub.status.idle":"2024-05-30T07:01:18.775290Z","shell.execute_reply.started":"2024-05-30T06:42:06.248672Z","shell.execute_reply":"2024-05-30T07:01:18.774030Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"2024-05-30 06:42:13.905138: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:42:13.905205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:42:13.906831: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-740', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.42'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-740\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:52, 29.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:57<03:22, 28.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:26<02:53, 28.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:55<02:24, 28.96s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:25<01:56, 29.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:54<01:27, 29.12s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:23<00:58, 29.15s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:52<00:29, 29.24s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:22<00:00, 29.15s/it]\nmean test reward 0.7034085924882574 +/- 0.009392748340591728 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9974907636642456 from 0.00922983093187213\nmean KL -0.2576208176259469 +/- 0.036006606135383264 full 1.0918037211587135 +/- 0.011830626560320745\nmedian KL -0.11709520220756531 full 0.9640893042087555\n2024-05-30 06:47:01.036654: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:47:01.036722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:47:01.038476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-555', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.47'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-555\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:30<04:02, 30.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:59<03:26, 29.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:28<02:56, 29.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:57<02:26, 29.40s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:27<01:57, 29.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:56<01:28, 29.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:26<00:58, 29.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:56<00:29, 29.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:25<00:00, 29.48s/it]\nmean test reward 0.6832316088307127 +/- 0.009573438578274538 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9971833825111389 from 0.00922983093187213\nmean KL -0.08905372607437635 +/- 0.03154197634652787 full 0.8942583803501394 +/- 0.01035732289932619\nmedian KL 0.015650153160095215 full 0.7834100127220154\n2024-05-30 06:51:50.028546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:51:50.028629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:51:50.030419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-370', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.51'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-370\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:55, 29.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:58<03:23, 29.14s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:27<02:55, 29.26s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:57<02:26, 29.27s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:26<01:56, 29.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:55<01:28, 29.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:25<00:58, 29.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:54<00:29, 29.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:24<00:00, 29.42s/it]\nmean test reward 0.6412283495507912 +/- 0.009876015462135889 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9960003793239594 from 0.00922983093187213\nmean KL 0.13632819442621744 +/- 0.02533206382111644 full 0.6364274720124539 +/- 0.007973475109513734\nmedian KL 0.22270071506500244 full 0.5518112480640411\n2024-05-30 06:56:39.234633: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 06:56:39.234707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 06:56:39.236405: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-185', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.06.56'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_6-2024.05.30.06.25/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:55, 29.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:58<03:25, 29.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:28<02:56, 29.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:57<02:26, 29.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:26<01:57, 29.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:56<01:27, 29.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:24<00:58, 29.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:53<00:29, 29.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:22<00:00, 29.19s/it]\nmean test reward 0.5675511520220956 +/- 0.010206487176022828 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.991876482963562 from 0.00922983093187213\nmean KL 0.2619331166691457 +/- 0.01585646001024928 full 0.2662105659312349 +/- 0.0033633313471227153\nmedian KL 0.2593313753604889 full 0.2284681349992752\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=dpo_6c2_div6_actual-0_1  --beta=0.1 --dataset_name=/kaggle/working/cs234-project/gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=200 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:06:44.306776Z","iopub.execute_input":"2024-05-30T07:06:44.307698Z","iopub.status.idle":"2024-05-30T07:25:40.059962Z","shell.execute_reply.started":"2024-05-30T07:06:44.307657Z","shell.execute_reply":"2024-05-30T07:25:40.058812Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"2024-05-30 07:06:51.343307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 07:06:51.343372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 07:06:51.344980: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 24895\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stderr","text":"wandb: WARNING current working directory has been invalidated\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240530_070657-546tvhe5\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_6c2_div6_actual-0_1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/546tvhe5\u001b[0m\n  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 2.333125591278076, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -60.50761032104492, 'logps/chosen': -62.50612258911133, 'logits/rejected': -37.702693939208984, 'logits/chosen': -36.72541809082031, 'epoch': 0.01}\n{'loss': 0.6923, 'grad_norm': 2.472505807876587, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.00079690245911479, 'rewards/rejected': -0.0009176692110486329, 'rewards/accuracies': 0.667570173740387, 'rewards/margins': 0.0017145717283710837, 'logps/rejected': -59.97027587890625, 'logps/chosen': -60.2357063293457, 'logits/rejected': -38.2284049987793, 'logits/chosen': -37.125579833984375, 'epoch': 0.27}\n{'loss': 0.6874, 'grad_norm': 2.170353889465332, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.004045311827212572, 'rewards/rejected': -0.007563350722193718, 'rewards/accuracies': 0.6931250095367432, 'rewards/margins': 0.011608663015067577, 'logps/rejected': -60.308650970458984, 'logps/chosen': -60.01332092285156, 'logits/rejected': -37.76182556152344, 'logits/chosen': -36.87598419189453, 'epoch': 0.54}\n{'loss': 0.678, 'grad_norm': 2.4391260147094727, 'learning_rate': 2e-06, 'rewards/chosen': 0.0054986802861094475, 'rewards/rejected': -0.025952646508812904, 'rewards/accuracies': 0.7107812762260437, 'rewards/margins': 0.031451329588890076, 'logps/rejected': -60.49150466918945, 'logps/chosen': -59.78321838378906, 'logits/rejected': -37.319183349609375, 'logits/chosen': -36.544857025146484, 'epoch': 0.81}\n{'loss': 0.6633, 'grad_norm': 2.291682481765747, 'learning_rate': 2e-06, 'rewards/chosen': -0.004380336031317711, 'rewards/rejected': -0.06817147135734558, 'rewards/accuracies': 0.7181919813156128, 'rewards/margins': 0.06379113346338272, 'logps/rejected': -60.7205810546875, 'logps/chosen': -60.316104888916016, 'logits/rejected': -37.1859130859375, 'logits/chosen': -36.439632415771484, 'epoch': 1.08}\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 200/740 [03:38<09:42,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.06it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.97it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.92it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6553601026535034, 'eval_runtime': 5.4117, 'eval_samples_per_second': 230.056, 'eval_steps_per_second': 1.848, 'eval_rewards/chosen': -0.015933573246002197, 'eval_rewards/rejected': -0.09768544137477875, 'eval_rewards/accuracies': 0.7185232043266296, 'eval_rewards/margins': 0.08175186812877655, 'eval_logps/rejected': -61.5146484375, 'eval_logps/chosen': -60.33295822143555, 'eval_logits/rejected': -37.37378692626953, 'eval_logits/chosen': -36.59720993041992, 'epoch': 1.08}\n 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 200/740 [03:43<09:42,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.6437, 'grad_norm': 2.157465934753418, 'learning_rate': 2e-06, 'rewards/chosen': -0.025431480258703232, 'rewards/rejected': -0.13450974225997925, 'rewards/accuracies': 0.7359374761581421, 'rewards/margins': 0.10907825827598572, 'logps/rejected': -61.40516662597656, 'logps/chosen': -60.17915344238281, 'logits/rejected': -36.96775436401367, 'logits/chosen': -36.12145233154297, 'epoch': 1.35}\n{'loss': 0.6313, 'grad_norm': 1.986513614654541, 'learning_rate': 2e-06, 'rewards/chosen': -0.06868358701467514, 'rewards/rejected': -0.21090085804462433, 'rewards/accuracies': 0.7307812571525574, 'rewards/margins': 0.1422172486782074, 'logps/rejected': -62.23564910888672, 'logps/chosen': -60.79582595825195, 'logits/rejected': -36.430171966552734, 'logits/chosen': -35.734981536865234, 'epoch': 1.62}\n{'loss': 0.6203, 'grad_norm': 2.080822706222534, 'learning_rate': 2e-06, 'rewards/chosen': -0.12920492887496948, 'rewards/rejected': -0.3064253628253937, 'rewards/accuracies': 0.71875, 'rewards/margins': 0.177220419049263, 'logps/rejected': -63.283164978027344, 'logps/chosen': -61.32347106933594, 'logits/rejected': -35.850624084472656, 'logits/chosen': -35.3931999206543, 'epoch': 1.89}\n{'loss': 0.6022, 'grad_norm': 2.066100835800171, 'learning_rate': 2e-06, 'rewards/chosen': -0.18679937720298767, 'rewards/rejected': -0.41506585478782654, 'rewards/accuracies': 0.7354751825332642, 'rewards/margins': 0.22826644778251648, 'logps/rejected': -64.58936309814453, 'logps/chosen': -62.06272888183594, 'logits/rejected': -36.04943084716797, 'logits/chosen': -35.472957611083984, 'epoch': 2.16}\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 400/740 [07:20<06:05,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.06it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.97it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.92it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.89it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6016090512275696, 'eval_runtime': 5.4012, 'eval_samples_per_second': 230.505, 'eval_steps_per_second': 1.851, 'eval_rewards/chosen': -0.22505243122577667, 'eval_rewards/rejected': -0.45846661925315857, 'eval_rewards/accuracies': 0.7237987518310547, 'eval_rewards/margins': 0.23341412842273712, 'eval_logps/rejected': -65.12245178222656, 'eval_logps/chosen': -62.42414474487305, 'eval_logits/rejected': -36.071563720703125, 'eval_logits/chosen': -35.571250915527344, 'epoch': 2.16}\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 400/740 [07:26<06:05,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.5866, 'grad_norm': 2.0793871879577637, 'learning_rate': 2e-06, 'rewards/chosen': -0.24603405594825745, 'rewards/rejected': -0.5220885872840881, 'rewards/accuracies': 0.7464062571525574, 'rewards/margins': 0.2760545313358307, 'logps/rejected': -65.22622680664062, 'logps/chosen': -62.785457611083984, 'logits/rejected': -35.4499397277832, 'logits/chosen': -34.963523864746094, 'epoch': 2.43}\n{'loss': 0.5769, 'grad_norm': 1.8993587493896484, 'learning_rate': 2e-06, 'rewards/chosen': -0.31124815344810486, 'rewards/rejected': -0.6220883131027222, 'rewards/accuracies': 0.7409374713897705, 'rewards/margins': 0.3108401596546173, 'logps/rejected': -66.4826889038086, 'logps/chosen': -63.230037689208984, 'logits/rejected': -35.4994010925293, 'logits/chosen': -35.17047119140625, 'epoch': 2.7}\n{'loss': 0.571, 'grad_norm': 1.9804582595825195, 'learning_rate': 2e-06, 'rewards/chosen': -0.37837791442871094, 'rewards/rejected': -0.7204198241233826, 'rewards/accuracies': 0.7410937547683716, 'rewards/margins': 0.342041939496994, 'logps/rejected': -67.00602722167969, 'logps/chosen': -63.53203201293945, 'logits/rejected': -35.3139533996582, 'logits/chosen': -34.783695220947266, 'epoch': 2.97}\n{'loss': 0.5521, 'grad_norm': 2.0983779430389404, 'learning_rate': 2e-06, 'rewards/chosen': -0.435682475566864, 'rewards/rejected': -0.8334192633628845, 'rewards/accuracies': 0.7662181258201599, 'rewards/margins': 0.39773690700531006, 'logps/rejected': -68.37067413330078, 'logps/chosen': -64.22911834716797, 'logits/rejected': -34.753108978271484, 'logits/chosen': -34.350257873535156, 'epoch': 3.24}\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 600/740 [11:03<02:31,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                   | 2/10 [00:00<00:02,  3.62it/s]\u001b[A\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 5/10 [00:02<00:02,  2.05it/s]\u001b[A\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 9/10 [00:04<00:00,  1.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5666049122810364, 'eval_runtime': 5.4229, 'eval_samples_per_second': 229.583, 'eval_steps_per_second': 1.844, 'eval_rewards/chosen': -0.5096770524978638, 'eval_rewards/rejected': -0.8822064399719238, 'eval_rewards/accuracies': 0.7279990315437317, 'eval_rewards/margins': 0.37252941727638245, 'eval_logps/rejected': -69.35985565185547, 'eval_logps/chosen': -65.27040100097656, 'eval_logits/rejected': -35.29387664794922, 'eval_logits/chosen': -34.93157958984375, 'epoch': 3.24}\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 600/740 [11:09<02:31,  1.08s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.5487, 'grad_norm': 1.9974801540374756, 'learning_rate': 2e-06, 'rewards/chosen': -0.5005196332931519, 'rewards/rejected': -0.9232709407806396, 'rewards/accuracies': 0.7593749761581421, 'rewards/margins': 0.422751247882843, 'logps/rejected': -69.54571533203125, 'logps/chosen': -65.41463470458984, 'logits/rejected': -34.957881927490234, 'logits/chosen': -34.522090911865234, 'epoch': 3.51}\n{'loss': 0.5363, 'grad_norm': 2.030393600463867, 'learning_rate': 2e-06, 'rewards/chosen': -0.5534284114837646, 'rewards/rejected': -1.02272367477417, 'rewards/accuracies': 0.7660937309265137, 'rewards/margins': 0.4692952334880829, 'logps/rejected': -70.24097442626953, 'logps/chosen': -65.5284652709961, 'logits/rejected': -34.82164001464844, 'logits/chosen': -34.479217529296875, 'epoch': 3.78}\n{'train_runtime': 838.8256, 'train_samples_per_second': 112.777, 'train_steps_per_second': 0.882, 'train_loss': 0.6091532597670684, 'epoch': 4.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 740/740 [13:42<00:00,  1.11s/it]\nEvaling every 185\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:58, 29.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:59<03:29, 29.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:29<02:59, 29.92s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:59<02:30, 30.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:29<02:00, 30.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/dpo.py\", line 199, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(f\"{training_args.output_dir}/checkpoint-{epoch*steps}\", f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 260, in eval\n    run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:59<01:29, 29.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:29<00:59, 29.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:58<00:29, 29.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:28<00:00, 29.84s/it]\nTraceback (most recent call last):\n  File \"/kaggle/working/cs234-project/dpo.py\", line 199, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(f\"{training_args.output_dir}/checkpoint-{epoch*steps}\", f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 260, in eval\n    run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 251, in run\n    test_stats.to_csv(f'./results/{full_name}.csv')\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 333, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\", line 3967, in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/formats/format.py\", line 1014, in to_csv\n    csv_formatter.save()\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/formats/csvs.py\", line 251, in save\n    with get_handle(\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/common.py\", line 749, in get_handle\n    check_parent_directory(str(handle))\n  File \"/opt/conda/lib/python3.10/site-packages/pandas/io/common.py\", line 616, in check_parent_directory\n    raise OSError(rf\"Cannot save file into a non-existent directory: '{parent}'\")\nOSError: Cannot save file into a non-existent directory: 'results/dpo_6c2_div6_actual-0_1-2024.05.30.07.06'\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-740\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-555\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-370\n!python ppo.py --exp_name=eval --eval_model=./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-185","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:33:19.885600Z","iopub.execute_input":"2024-05-30T07:33:19.886054Z","iopub.status.idle":"2024-05-30T07:52:30.675389Z","shell.execute_reply.started":"2024-05-30T07:33:19.886022Z","shell.execute_reply":"2024-05-30T07:52:30.674219Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"2024-05-30 07:33:26.879012: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 07:33:26.879074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 07:33:26.880814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-740', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.07.33'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-740\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:28<03:49, 28.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:56<03:18, 28.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:25<02:50, 28.40s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:53<02:21, 28.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:21<01:53, 28.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:50<01:25, 28.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:18<00:56, 28.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:47<00:28, 28.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:16<00:00, 28.47s/it]\nmean test reward 0.8725804330287146 +/- 0.006850082824531585 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9987750053405762 from 0.00922983093187213\nmean KL 2.6109349589743136 +/- 0.10724962588879085 full 7.281893460856129 +/- 0.048598660320440606\nmedian KL 3.2389501333236694 full 6.973338842391968\n2024-05-30 07:38:07.641474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 07:38:07.641550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 07:38:07.643271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-555', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.07.38'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-555\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:53, 29.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:57<03:22, 28.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:26<02:52, 28.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:55<02:23, 28.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:24<01:55, 28.93s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:53<01:27, 29.10s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:23<00:58, 29.15s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:53<00:29, 29.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:22<00:00, 29.18s/it]\nmean test reward 0.845636380789857 +/- 0.0074333835311413765 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9987088441848755 from 0.00922983093187213\nmean KL 2.844721463538008 +/- 0.08893580897140779 full 5.322484682624538 +/- 0.04074918157428555\nmedian KL 3.3065977096557617 full 5.026855707168579\n2024-05-30 07:42:54.670699: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 07:42:54.670787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 07:42:54.672400: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-370', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.07.42'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-370\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:30<04:00, 30.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:59<03:28, 29.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:29<02:59, 29.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:59<02:29, 29.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:28<01:58, 29.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:59<01:29, 29.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:29<00:59, 29.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:58<00:29, 29.75s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:28<00:00, 29.79s/it]\nmean test reward 0.7830030562356569 +/- 0.008483663746106443 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9984268546104431 from 0.00922983093187213\nmean KL 2.050356687150068 +/- 0.05637189825950154 full 2.798561468021944 +/- 0.02647002765110131\nmedian KL 2.189030885696411 full 2.5926215648651123\n2024-05-30 07:47:48.163168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 07:47:48.163238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 07:47:48.164886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-185', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.30.07.47'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_6c2_div6_actual-0_1-2024.05.30.07.06/checkpoint-185\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 11%|‚ñà‚ñà‚ñà‚ñà‚ñà                                        | 1/9 [00:29<03:55, 29.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                   | 2/9 [00:58<03:24, 29.20s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 3/9 [01:27<02:55, 29.17s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 4/9 [01:57<02:27, 29.42s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 5/9 [02:27<01:58, 29.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 231, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 6/9 [02:56<01:28, 29.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7/9 [03:26<00:59, 29.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/9 [03:56<00:29, 29.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [04:25<00:00, 29.49s/it]\nmean test reward 0.6136052996632265 +/- 0.010035829246816532 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9944998919963837 from 0.00922983093187213\nmean KL 0.5180895996001911 +/- 0.02229446784490338 full 0.48240525097207865 +/- 0.0055138383106989736\nmedian KL 0.5511460900306702 full 0.42772534489631653\n","output_type":"stream"}]}]}