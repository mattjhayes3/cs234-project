{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8282f823",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-02T02:15:59.849739Z",
     "iopub.status.busy": "2024-06-02T02:15:59.849016Z",
     "iopub.status.idle": "2024-06-02T02:18:42.818772Z",
     "shell.execute_reply": "2024-06-02T02:18:42.817185Z"
    },
    "papermill": {
     "duration": 162.976441,
     "end_time": "2024-06-02T02:18:42.821220",
     "exception": false,
     "start_time": "2024-06-02T02:15:59.844779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\r\n",
      "Channels:\r\n",
      " - rapidsai\r\n",
      " - nvidia\r\n",
      " - conda-forge\r\n",
      " - defaults\r\n",
      " - pytorch\r\n",
      "Platform: linux-64\r\n",
      "Collecting package metadata (repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n",
      "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  added / updated specs:\r\n",
      "    - aiohttp\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be downloaded:\r\n",
      "\r\n",
      "    package                    |            build\r\n",
      "    ---------------------------|-----------------\r\n",
      "    aiohttp-3.9.5              |  py310h2372a71_0         682 KB  conda-forge\r\n",
      "    ------------------------------------------------------------\r\n",
      "                                           Total:         682 KB\r\n",
      "\r\n",
      "The following packages will be UPDATED:\r\n",
      "\r\n",
      "  aiohttp                             3.9.1-py310h2372a71_0 --> 3.9.5-py310h2372a71_0 \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages:\r\n",
      "\r\n",
      "Preparing transaction: \\ \b\bdone\r\n",
      "Verifying transaction: / \b\bdone\r\n",
      "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Cloning into 'cs234-project'...\r\n",
      "remote: Enumerating objects: 292, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (72/72), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\r\n",
      "remote: Total 292 (delta 43), reused 53 (delta 25), pack-reused 220\u001b[K\r\n",
      "Receiving objects: 100% (292/292), 60.49 MiB | 14.74 MiB/s, done.\r\n",
      "Resolving deltas: 100% (150/150), done.\r\n",
      "Updating files: 100% (127/127), done.\r\n",
      "/kaggle/working/cs234-project\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.0)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\r\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.3.1)\r\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Collecting git+https://github.com/mattjhayes3/trl.git\r\n",
      "  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-7axkw5jj\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-7axkw5jj\r\n",
      "  Resolved https://github.com/mattjhayes3/trl.git to commit 4c8e35e0e0a6b90b4b7c506cce7e31eb51b170b9\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.41.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.30.1)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.19.1)\r\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.3.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.23.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.4)\r\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\r\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.5)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\r\n",
      "Building wheels for collected packages: trl\r\n",
      "  Building wheel for trl (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for trl: filename=trl-0.8.7.dev0-py3-none-any.whl size=209532 sha256=a2570937429088c32922d721b0955079aa87a6fb60fe4b41cdb136c576528bae\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3_735g7j/wheels/b5/f5/74/f982e27bdb1c23b205f8bfcaaed174cf3d2c06bd1a5f5926cc\r\n",
      "Successfully built trl\r\n",
      "Installing collected packages: trl\r\n",
      "Successfully installed trl-0.8.7.dev0\r\n"
     ]
    }
   ],
   "source": [
    "# !conda install -y gdown\n",
    "# print('hi')\n",
    "!conda install aiohttp -y\n",
    "!git clone https://github.com/mattjhayes3/cs234-project.git\n",
    "%cd /kaggle/working/cs234-project\n",
    "!git pull --rebase\n",
    "!pip install wandb\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"KEY\")\n",
    "# wandb.init()\n",
    "\n",
    "from huggingface_hub import notebook_login,login\n",
    "\n",
    "# notebook_login(\"KEY\")\n",
    "login(\"KEY\")\n",
    "\n",
    "# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n",
    "# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n",
    "!mkdir results\n",
    "!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n",
    "!pip install git+https://github.com/mattjhayes3/trl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed67dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T02:18:42.861699Z",
     "iopub.status.busy": "2024-06-02T02:18:42.861278Z",
     "iopub.status.idle": "2024-06-02T02:50:17.079648Z",
     "shell.execute_reply": "2024-06-02T02:50:17.078392Z"
    },
    "papermill": {
     "duration": 1894.241228,
     "end_time": "2024-06-02T02:50:17.081987",
     "exception": false,
     "start_time": "2024-06-02T02:18:42.840759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-02 02:18:53.418819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-02 02:18:53.418941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-02 02:18:53.554577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "config.json: 100%|█████████████████████████████| 577/577 [00:00<00:00, 3.13MB/s]\r\n",
      "pytorch_model.bin: 100%|█████████████████████| 548M/548M [00:27<00:00, 19.6MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "tokenizer_config.json: 100%|██████████████████| 17.0/17.0 [00:00<00:00, 105kB/s]\r\n",
      "vocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 5.05MB/s]\r\n",
      "merges.txt: 100%|█████████████████████████████| 456k/456k [00:00<00:00, 885kB/s]\r\n",
      "special_tokens_map.json: 100%|████████████████| 90.0/90.0 [00:00<00:00, 432kB/s]\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240602_021941-8091czlb\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_quantile_flipped_2c2_0_25-0_05\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/8091czlb\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.1117808818817139, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -63.58149337768555, 'logps/chosen': -60.01593017578125, 'logits/rejected': -35.83627700805664, 'logits/chosen': -35.73686218261719, 'epoch': 0.01}\r\n",
      "{'loss': 0.6929, 'grad_norm': 1.1147191524505615, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.0002470551407895982, 'rewards/rejected': -0.0003231448645237833, 'rewards/accuracies': 0.6316964030265808, 'rewards/margins': 0.0005701999762095511, 'logps/rejected': -60.68466567993164, 'logps/chosen': -60.20366668701172, 'logits/rejected': -37.26521682739258, 'logits/chosen': -36.49761199951172, 'epoch': 0.27}\r\n",
      "{'loss': 0.6913, 'grad_norm': 1.1783031225204468, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0010638769017532468, 'rewards/rejected': -0.002662566490471363, 'rewards/accuracies': 0.6404687762260437, 'rewards/margins': 0.0037264437414705753, 'logps/rejected': -60.86805725097656, 'logps/chosen': -59.7805061340332, 'logits/rejected': -37.22792053222656, 'logits/chosen': -36.67540740966797, 'epoch': 0.54}\r\n",
      "{'loss': 0.6882, 'grad_norm': 1.124395728111267, 'learning_rate': 2e-06, 'rewards/chosen': 0.0007436443702317774, 'rewards/rejected': -0.009292742237448692, 'rewards/accuracies': 0.6440625190734863, 'rewards/margins': 0.01003638468682766, 'logps/rejected': -60.86701965332031, 'logps/chosen': -60.0139274597168, 'logits/rejected': -37.09642028808594, 'logits/chosen': -36.48192596435547, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:04<11:58,  1.22s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.68it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:03<00:02,  1.73it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.75it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6871581673622131, 'eval_runtime': 5.6671, 'eval_samples_per_second': 219.687, 'eval_steps_per_second': 1.765, 'eval_rewards/chosen': -0.0006799811962991953, 'eval_rewards/rejected': -0.013146176934242249, 'eval_rewards/accuracies': 0.6240927577018738, 'eval_rewards/margins': 0.012466195970773697, 'eval_logps/rejected': -60.94272994995117, 'eval_logps/chosen': -60.6410026550293, 'eval_logits/rejected': -37.03363800048828, 'eval_logits/chosen': -35.83454132080078, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:10<11:58,  1.22s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.99it/s]\u001b[A\r\n",
      "{'loss': 0.6828, 'grad_norm': 1.0656423568725586, 'learning_rate': 2e-06, 'rewards/chosen': -0.004791718907654285, 'rewards/rejected': -0.02627234160900116, 'rewards/accuracies': 0.6518846154212952, 'rewards/margins': 0.0214806217700243, 'logps/rejected': -61.238304138183594, 'logps/chosen': -59.86748123168945, 'logits/rejected': -37.191558837890625, 'logits/chosen': -36.535587310791016, 'epoch': 1.08}\r\n",
      "{'loss': 0.6757, 'grad_norm': 1.1301169395446777, 'learning_rate': 2e-06, 'rewards/chosen': -0.014777127653360367, 'rewards/rejected': -0.051592499017715454, 'rewards/accuracies': 0.6792187690734863, 'rewards/margins': 0.03681537136435509, 'logps/rejected': -61.68647766113281, 'logps/chosen': -60.014739990234375, 'logits/rejected': -36.80313491821289, 'logits/chosen': -36.42076110839844, 'epoch': 1.35}\r\n",
      "{'loss': 0.671, 'grad_norm': 1.1476932764053345, 'learning_rate': 2e-06, 'rewards/chosen': -0.033702302724123, 'rewards/rejected': -0.08156779408454895, 'rewards/accuracies': 0.667187511920929, 'rewards/margins': 0.047865480184555054, 'logps/rejected': -62.0993766784668, 'logps/chosen': -60.63788986206055, 'logits/rejected': -36.93754959106445, 'logits/chosen': -36.38236999511719, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:16<09:08,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.83it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.83it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.83it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6756018996238708, 'eval_runtime': 5.5316, 'eval_samples_per_second': 225.07, 'eval_steps_per_second': 1.808, 'eval_rewards/chosen': -0.04707848280668259, 'eval_rewards/rejected': -0.08645675331354141, 'eval_rewards/accuracies': 0.6237987279891968, 'eval_rewards/margins': 0.03937826305627823, 'eval_logps/rejected': -62.408935546875, 'eval_logps/chosen': -61.5689697265625, 'eval_logits/rejected': -36.69903564453125, 'eval_logits/chosen': -35.54645919799805, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:21<09:08,  1.25s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.6674, 'grad_norm': 1.1702347993850708, 'learning_rate': 2e-06, 'rewards/chosen': -0.05846508592367172, 'rewards/rejected': -0.11557061225175858, 'rewards/accuracies': 0.6596875190734863, 'rewards/margins': 0.05710554122924805, 'logps/rejected': -62.99013137817383, 'logps/chosen': -61.2699089050293, 'logits/rejected': -36.619022369384766, 'logits/chosen': -35.9954719543457, 'epoch': 1.89}\r\n",
      "{'loss': 0.6572, 'grad_norm': 1.1095401048660278, 'learning_rate': 2e-06, 'rewards/chosen': -0.08758082240819931, 'rewards/rejected': -0.16854558885097504, 'rewards/accuracies': 0.683408796787262, 'rewards/margins': 0.08096476644277573, 'logps/rejected': -64.07371520996094, 'logps/chosen': -61.70579528808594, 'logits/rejected': -36.769683837890625, 'logits/chosen': -36.187435150146484, 'epoch': 2.16}\r\n",
      "{'loss': 0.6527, 'grad_norm': 1.1618337631225586, 'learning_rate': 2e-06, 'rewards/chosen': -0.12400612235069275, 'rewards/rejected': -0.21810311079025269, 'rewards/accuracies': 0.6717187762260437, 'rewards/margins': 0.09409697353839874, 'logps/rejected': -65.0743179321289, 'logps/chosen': -62.54052734375, 'logits/rejected': -36.72859573364258, 'logits/chosen': -36.33213806152344, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:27<05:58,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.78it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.79it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.80it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.81it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.82it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6635481119155884, 'eval_runtime': 5.5813, 'eval_samples_per_second': 223.067, 'eval_steps_per_second': 1.792, 'eval_rewards/chosen': -0.15112563967704773, 'eval_rewards/rejected': -0.22491827607154846, 'eval_rewards/accuracies': 0.6205729246139526, 'eval_rewards/margins': 0.07379263639450073, 'eval_logps/rejected': -65.17816162109375, 'eval_logps/chosen': -63.64992141723633, 'eval_logits/rejected': -36.75402069091797, 'eval_logits/chosen': -35.63817596435547, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:32<05:58,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.01it/s]\u001b[A\r\n",
      "{'loss': 0.6483, 'grad_norm': 1.0759668350219727, 'learning_rate': 2e-06, 'rewards/chosen': -0.16593551635742188, 'rewards/rejected': -0.27349987626075745, 'rewards/accuracies': 0.6645312309265137, 'rewards/margins': 0.10756431519985199, 'logps/rejected': -66.28943634033203, 'logps/chosen': -63.39107894897461, 'logits/rejected': -36.77715301513672, 'logits/chosen': -36.43332290649414, 'epoch': 2.7}\r\n",
      "{'loss': 0.6413, 'grad_norm': 1.1461999416351318, 'learning_rate': 2e-06, 'rewards/chosen': -0.20428022742271423, 'rewards/rejected': -0.3304394781589508, 'rewards/accuracies': 0.6840624809265137, 'rewards/margins': 0.12615928053855896, 'logps/rejected': -67.32563018798828, 'logps/chosen': -63.911094665527344, 'logits/rejected': -36.77878189086914, 'logits/chosen': -36.4212760925293, 'epoch': 2.97}\r\n",
      "{'loss': 0.6335, 'grad_norm': 1.133221983909607, 'learning_rate': 2e-06, 'rewards/chosen': -0.25255629420280457, 'rewards/rejected': -0.4020475745201111, 'rewards/accuracies': 0.6801658868789673, 'rewards/margins': 0.1494913101196289, 'logps/rejected': -68.52914428710938, 'logps/chosen': -65.06390380859375, 'logits/rejected': -36.77582550048828, 'logits/chosen': -36.238677978515625, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:38<02:55,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6539502143859863, 'eval_runtime': 5.373, 'eval_samples_per_second': 231.715, 'eval_steps_per_second': 1.861, 'eval_rewards/chosen': -0.29334884881973267, 'eval_rewards/rejected': -0.40267401933670044, 'eval_rewards/accuracies': 0.6257476210594177, 'eval_rewards/margins': 0.10932515561580658, 'eval_logps/rejected': -68.73328399658203, 'eval_logps/chosen': -66.494384765625, 'eval_logits/rejected': -36.82852554321289, 'eval_logits/chosen': -35.72929763793945, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:44<02:55,  1.25s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\r\n",
      "{'loss': 0.6301, 'grad_norm': 1.1573679447174072, 'learning_rate': 2e-06, 'rewards/chosen': -0.3031153380870819, 'rewards/rejected': -0.46492958068847656, 'rewards/accuracies': 0.6862499713897705, 'rewards/margins': 0.16181422770023346, 'logps/rejected': -70.24822998046875, 'logps/chosen': -66.09707641601562, 'logits/rejected': -36.730499267578125, 'logits/chosen': -36.500732421875, 'epoch': 3.51}\r\n",
      "{'loss': 0.6257, 'grad_norm': 1.1922554969787598, 'learning_rate': 2e-06, 'rewards/chosen': -0.3512665927410126, 'rewards/rejected': -0.5297157764434814, 'rewards/accuracies': 0.6798437237739563, 'rewards/margins': 0.17844921350479126, 'logps/rejected': -71.20626068115234, 'logps/chosen': -67.02574920654297, 'logits/rejected': -37.201942443847656, 'logits/chosen': -36.888980865478516, 'epoch': 3.78}\r\n",
      "{'train_runtime': 955.2687, 'train_samples_per_second': 99.03, 'train_steps_per_second': 0.775, 'train_loss': 0.6594011558068765, 'epoch': 4.0}\r\n",
      "100%|█████████████████████████████████████████| 740/740 [15:38<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19/checkpoint-740\r\n",
      "Downloading readme: 100%|██████████████████| 7.81k/7.81k [00:00<00:00, 13.9MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 21.0M/21.0M [00:00<00:00, 31.0MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 20.5M/20.5M [00:00<00:00, 23.1MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 42.0M/42.0M [00:01<00:00, 23.3MB/s]\r\n",
      "Generating train split: 100%|██| 25000/25000 [00:00<00:00, 115026.52 examples/s]\r\n",
      "Generating test split: 100%|███| 25000/25000 [00:00<00:00, 179191.38 examples/s]\r\n",
      "Generating unsupervised split: 100%|█| 50000/50000 [00:00<00:00, 173354.74 examp\r\n",
      "Filter: 100%|███████████████████████| 250/250 [00:00<00:00, 43836.79 examples/s]\r\n",
      "Map:   0%|                                       | 0/250 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\r\n",
      "Map: 100%|████████████████████████████| 250/250 [00:00<00:00, 812.87 examples/s]\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "config.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 2.13MB/s]\r\n",
      "pytorch_model.bin: 100%|███████████████████| 1.42G/1.42G [01:04<00:00, 22.2MB/s]\r\n",
      "tokenizer_config.json: 100%|████████████████████| 256/256 [00:00<00:00, 673kB/s]\r\n",
      "vocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 4.65MB/s]\r\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.30MB/s]\r\n",
      "special_tokens_map.json: 100%|██████████████████| 150/150 [00:00<00:00, 317kB/s]\r\n",
      "eval batch size 256\r\n",
      "Filter: 100%|████████████████████| 2500/2500 [00:00<00:00, 128757.58 examples/s]\r\n",
      "Map:   3%|▉                           | 78/2487 [00:00<00:03, 758.61 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\r\n",
      "Map: 100%|██████████████████████████| 2487/2487 [00:02<00:00, 841.89 examples/s]\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:40, 27.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:55<03:12, 27.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:22<02:45, 27.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:50<02:17, 27.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:17<01:50, 27.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:45<01:22, 27.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:12<00:55, 27.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:40<00:27, 27.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:07<00:00, 27.54s/it]\r\n",
      "mean test reward 0.8742044733596939 +/- 0.006816160869191353 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9987758994102478 from 0.00922983093187213\r\n",
      "mean KL 4.165731773957507 +/- 0.1438242125355252 full 8.289304749729732 +/- 0.0555211405635802\r\n",
      "median KL 5.220917463302612 full 7.994338035583496\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:41, 27.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:55<03:12, 27.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:22<02:44, 27.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:49<02:17, 27.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:17<01:49, 27.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:45<01:22, 27.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:12<00:55, 27.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:40<00:27, 27.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:07<00:00, 27.51s/it]\r\n",
      "mean test reward 0.8408662089736177 +/- 0.007522814808267897 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9986943006515503 from 0.00922983093187213\r\n",
      "mean KL 3.5989620600982257 +/- 0.09968066238980129 full 5.483096216701799 +/- 0.04143771577766229\r\n",
      "median KL 4.213630199432373 full 5.262460708618164\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:37, 27.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:54<03:10, 27.27s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:21<02:44, 27.35s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:49<02:16, 27.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:17<01:50, 27.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:44<01:22, 27.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:12<00:55, 27.54s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:40<00:27, 27.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:07<00:00, 27.53s/it]\r\n",
      "mean test reward 0.7701895805958858 +/- 0.008667955869695506 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9982389807701111 from 0.00922983093187213\r\n",
      "mean KL 2.1040576341264465 +/- 0.05560291232047356 full 2.4664377165156313 +/- 0.021938901316723197\r\n",
      "median KL 2.334352970123291 full 2.323418140411377\r\n",
      "dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19,0.05,epoch 4,0.8742044733596939,0.006816160869191353,8.289304749729732,0.0555211405635802\r\n",
      "dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19,0.05,epoch 3,0.8408662089736177,0.007522814808267897,5.483096216701799,0.04143771577766229\r\n",
      "dpo_quantile_flipped_2c2_0_25-0_05-2024.06.02.02.19,0.05,epoch 2,0.7701895805958858,0.008667955869695506,2.4664377165156313,0.021938901316723197\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_quantile_flipped_2c2_0_25-0_05 --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_quantile_flipped_0_25_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4aa8c3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T02:50:17.302522Z",
     "iopub.status.busy": "2024-06-02T02:50:17.301587Z",
     "iopub.status.idle": "2024-06-02T03:20:10.916153Z",
     "shell.execute_reply": "2024-06-02T03:20:10.915138Z"
    },
    "papermill": {
     "duration": 1793.725943,
     "end_time": "2024-06-02T03:20:10.918398",
     "exception": false,
     "start_time": "2024-06-02T02:50:17.192455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-02 02:50:24.324232: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-02 02:50:24.324315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-02 02:50:24.325899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240602_025032-ojavd8j2\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_quantile_flipped_2c2_0_25-0_2\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/ojavd8j2\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.4471235275268555, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -63.58149337768555, 'logps/chosen': -60.01593017578125, 'logits/rejected': -35.83627700805664, 'logits/chosen': -35.73686218261719, 'epoch': 0.01}\r\n",
      "{'loss': 0.692, 'grad_norm': 4.443666934967041, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.0009861225262284279, 'rewards/rejected': -0.001289083156734705, 'rewards/accuracies': 0.6315369606018066, 'rewards/margins': 0.002275205682963133, 'logps/rejected': -60.68463897705078, 'logps/chosen': -60.20368194580078, 'logits/rejected': -37.26527404785156, 'logits/chosen': -36.4976692199707, 'epoch': 0.27}\r\n",
      "{'loss': 0.6861, 'grad_norm': 4.6675872802734375, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0042657554149627686, 'rewards/rejected': -0.010413768701255322, 'rewards/accuracies': 0.6410937309265137, 'rewards/margins': 0.014679525047540665, 'logps/rejected': -60.86687469482422, 'logps/chosen': -59.780460357666016, 'logits/rejected': -37.23023223876953, 'logits/chosen': -36.67753601074219, 'epoch': 0.54}\r\n",
      "{'loss': 0.6755, 'grad_norm': 4.461545467376709, 'learning_rate': 2e-06, 'rewards/chosen': 0.004055713769048452, 'rewards/rejected': -0.034381356090307236, 'rewards/accuracies': 0.6479687690734863, 'rewards/margins': 0.038437072187662125, 'logps/rejected': -60.85307312011719, 'logps/chosen': -60.00852966308594, 'logits/rejected': -37.105865478515625, 'logits/chosen': -36.489837646484375, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:04<11:59,  1.22s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.70it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.76it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.78it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.80it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6726956367492676, 'eval_runtime': 5.6511, 'eval_samples_per_second': 220.312, 'eval_steps_per_second': 1.77, 'eval_rewards/chosen': 3.525707825247082e-06, 'eval_rewards/rejected': -0.04662283882498741, 'eval_rewards/accuracies': 0.6282930374145508, 'eval_rewards/margins': 0.04662637040019035, 'eval_logps/rejected': -60.91291427612305, 'eval_logps/chosen': -60.62738800048828, 'eval_logits/rejected': -37.04485321044922, 'eval_logits/chosen': -35.84453201293945, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:10<11:59,  1.22s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.659, 'grad_norm': 4.209061145782471, 'learning_rate': 2e-06, 'rewards/chosen': -0.01040960755199194, 'rewards/rejected': -0.08932995796203613, 'rewards/accuracies': 0.6586511135101318, 'rewards/margins': 0.07892034947872162, 'logps/rejected': -61.15950012207031, 'logps/chosen': -59.82368469238281, 'logits/rejected': -37.208534240722656, 'logits/chosen': -36.54445266723633, 'epoch': 1.08}\r\n",
      "{'loss': 0.6388, 'grad_norm': 4.336853981018066, 'learning_rate': 2e-06, 'rewards/chosen': -0.028296897187829018, 'rewards/rejected': -0.15859952569007874, 'rewards/accuracies': 0.6932812333106995, 'rewards/margins': 0.13030262291431427, 'logps/rejected': -61.447628021240234, 'logps/chosen': -59.86068344116211, 'logits/rejected': -36.84504699707031, 'logits/chosen': -36.447174072265625, 'epoch': 1.35}\r\n",
      "{'loss': 0.6301, 'grad_norm': 4.292407512664795, 'learning_rate': 2e-06, 'rewards/chosen': -0.061576321721076965, 'rewards/rejected': -0.22126559913158417, 'rewards/accuracies': 0.6826562285423279, 'rewards/margins': 0.1596892923116684, 'logps/rejected': -61.57435607910156, 'logps/chosen': -60.271728515625, 'logits/rejected': -36.992523193359375, 'logits/chosen': -36.40055847167969, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:15<09:06,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.83it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.83it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.82it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.83it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6504434943199158, 'eval_runtime': 5.5275, 'eval_samples_per_second': 225.238, 'eval_steps_per_second': 1.809, 'eval_rewards/chosen': -0.08982326090335846, 'eval_rewards/rejected': -0.20978298783302307, 'eval_rewards/accuracies': 0.6293683052062988, 'eval_rewards/margins': 0.11995971202850342, 'eval_logps/rejected': -61.72871780395508, 'eval_logps/chosen': -61.07651901245117, 'eval_logits/rejected': -36.72649002075195, 'eval_logits/chosen': -35.56419372558594, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:21<09:06,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.6263, 'grad_norm': 4.368370532989502, 'learning_rate': 2e-06, 'rewards/chosen': -0.09662125259637833, 'rewards/rejected': -0.2750646471977234, 'rewards/accuracies': 0.6785937547683716, 'rewards/margins': 0.17844337224960327, 'logps/rejected': -62.054039001464844, 'logps/chosen': -60.583717346191406, 'logits/rejected': -36.67145919799805, 'logits/chosen': -35.997066497802734, 'epoch': 1.89}\r\n",
      "{'loss': 0.6034, 'grad_norm': 4.26265287399292, 'learning_rate': 2e-06, 'rewards/chosen': -0.11891207098960876, 'rewards/rejected': -0.35835665464401245, 'rewards/accuracies': 0.7135522365570068, 'rewards/margins': 0.2394445836544037, 'logps/rejected': -62.494590759277344, 'logps/chosen': -60.548728942871094, 'logits/rejected': -36.826351165771484, 'logits/chosen': -36.159183502197266, 'epoch': 2.16}\r\n",
      "{'loss': 0.5957, 'grad_norm': 4.1277689933776855, 'learning_rate': 2e-06, 'rewards/chosen': -0.13883353769779205, 'rewards/rejected': -0.406057745218277, 'rewards/accuracies': 0.7151562571525574, 'rewards/margins': 0.26722419261932373, 'logps/rejected': -62.7425422668457, 'logps/chosen': -60.75457000732422, 'logits/rejected': -36.7063102722168, 'logits/chosen': -36.202152252197266, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:26<06:00,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.82it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6394411325454712, 'eval_runtime': 5.5416, 'eval_samples_per_second': 224.666, 'eval_steps_per_second': 1.805, 'eval_rewards/chosen': -0.18386109173297882, 'eval_rewards/rejected': -0.3581090569496155, 'eval_rewards/accuracies': 0.6257560849189758, 'eval_rewards/margins': 0.17424795031547546, 'eval_logps/rejected': -62.47034454345703, 'eval_logps/chosen': -61.54671096801758, 'eval_logits/rejected': -36.67481231689453, 'eval_logits/chosen': -35.53144073486328, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:32<06:00,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.5916, 'grad_norm': 3.9915590286254883, 'learning_rate': 2e-06, 'rewards/chosen': -0.16497604548931122, 'rewards/rejected': -0.4520575702190399, 'rewards/accuracies': 0.707812488079071, 'rewards/margins': 0.2870815396308899, 'logps/rejected': -63.079715728759766, 'logps/chosen': -60.897254943847656, 'logits/rejected': -36.685096740722656, 'logits/chosen': -36.264854431152344, 'epoch': 2.7}\r\n",
      "{'loss': 0.584, 'grad_norm': 4.075953483581543, 'learning_rate': 2e-06, 'rewards/chosen': -0.18060818314552307, 'rewards/rejected': -0.48944541811943054, 'rewards/accuracies': 0.7256249785423279, 'rewards/margins': 0.3088372051715851, 'logps/rejected': -63.1640625, 'logps/chosen': -60.72853088378906, 'logits/rejected': -36.648433685302734, 'logits/chosen': -36.134456634521484, 'epoch': 2.97}\r\n",
      "{'loss': 0.5635, 'grad_norm': 3.979727029800415, 'learning_rate': 2e-06, 'rewards/chosen': -0.192471444606781, 'rewards/rejected': -0.5599571466445923, 'rewards/accuracies': 0.7436607480049133, 'rewards/margins': 0.3674857020378113, 'logps/rejected': -63.2879753112793, 'logps/chosen': -60.975135803222656, 'logits/rejected': -36.66765594482422, 'logits/chosen': -35.950740814208984, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:38<02:54,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6340378522872925, 'eval_runtime': 5.377, 'eval_samples_per_second': 231.541, 'eval_steps_per_second': 1.86, 'eval_rewards/chosen': -0.26577895879745483, 'eval_rewards/rejected': -0.4782453179359436, 'eval_rewards/accuracies': 0.6304435729980469, 'eval_rewards/margins': 0.21246638894081116, 'eval_logps/rejected': -63.0710334777832, 'eval_logps/chosen': -61.956298828125, 'eval_logits/rejected': -36.633399963378906, 'eval_logits/chosen': -35.51267623901367, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:44<02:54,  1.25s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\r\n",
      "{'loss': 0.5628, 'grad_norm': 4.020902633666992, 'learning_rate': 2e-06, 'rewards/chosen': -0.2220115214586258, 'rewards/rejected': -0.5965253710746765, 'rewards/accuracies': 0.7446874976158142, 'rewards/margins': 0.37451374530792236, 'logps/rejected': -63.9322624206543, 'logps/chosen': -61.14482879638672, 'logits/rejected': -36.577266693115234, 'logits/chosen': -36.153865814208984, 'epoch': 3.51}\r\n",
      "{'loss': 0.5575, 'grad_norm': 3.956402540206909, 'learning_rate': 2e-06, 'rewards/chosen': -0.23753885924816132, 'rewards/rejected': -0.636065661907196, 'rewards/accuracies': 0.7434375286102295, 'rewards/margins': 0.39852678775787354, 'logps/rejected': -63.792274475097656, 'logps/chosen': -61.188114166259766, 'logits/rejected': -36.95161437988281, 'logits/chosen': -36.53719711303711, 'epoch': 3.78}\r\n",
      "{'train_runtime': 955.1641, 'train_samples_per_second': 99.041, 'train_steps_per_second': 0.775, 'train_loss': 0.6159569501876831, 'epoch': 4.0}\r\n",
      "100%|█████████████████████████████████████████| 740/740 [15:38<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:34, 26.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:07, 26.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:41, 26.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:14, 26.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:48, 27.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.06s/it]\r\n",
      "mean test reward 0.7752012313675323 +/- 0.008597936238589236 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9983266592025757 from 0.00922983093187213\r\n",
      "mean KL 0.8594373211348688 +/- 0.05888170294200773 full 2.2881468902988775 +/- 0.01888238545868646\r\n",
      "median KL 1.2683939933776855 full 2.130459189414978\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:34, 26.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:08, 26.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:42, 27.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:15, 27.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:15<01:48, 27.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.10s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:04<00:00, 27.18s/it]\r\n",
      "mean test reward 0.7540421556150503 +/- 0.008864838727500733 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9980955719947815 from 0.00922983093187213\r\n",
      "mean KL 1.081930693649661 +/- 0.049506997111319055 full 1.8870299866474751 +/- 0.017006641189081084\r\n",
      "median KL 1.3788427114486694 full 1.7443833947181702\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:36, 27.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:54<03:09, 27.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:21<02:42, 27.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:48<02:15, 27.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:15<01:48, 27.13s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.17s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.08s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.07s/it]\r\n",
      "mean test reward 0.6960019613045334 +/- 0.00947020978783438 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9974364340305328 from 0.00922983093187213\r\n",
      "mean KL 1.0025781369380031 +/- 0.03753085188853301 full 1.2275047620706674 +/- 0.012534919838533868\r\n",
      "median KL 1.1281336545944214 full 1.119271993637085\r\n",
      "dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50,0.2,epoch 4,0.7752012313675323,0.008597936238589236,2.2881468902988775,0.01888238545868646\r\n",
      "dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50,0.2,epoch 3,0.7540421556150503,0.008864838727500733,1.8870299866474751,0.017006641189081084\r\n",
      "dpo_quantile_flipped_2c2_0_25-0_2-2024.06.02.02.50,0.2,epoch 2,0.6960019613045334,0.00947020978783438,1.2275047620706674,0.012534919838533868\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_quantile_flipped_2c2_0_25-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_quantile_flipped_0_25_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d1cc5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T03:20:11.277297Z",
     "iopub.status.busy": "2024-06-02T03:20:11.276518Z",
     "iopub.status.idle": "2024-06-02T03:49:32.687785Z",
     "shell.execute_reply": "2024-06-02T03:49:32.686745Z"
    },
    "papermill": {
     "duration": 1761.590729,
     "end_time": "2024-06-02T03:49:32.690237",
     "exception": false,
     "start_time": "2024-06-02T03:20:11.099508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-02 03:20:18.469615: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-02 03:20:18.469671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-02 03:20:18.471195: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240602_032026-fvj66zxi\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_quantile_flipped_2c2_0_5-0_05\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/fvj66zxi\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 1.1340031623840332, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -63.73570251464844, 'logps/chosen': -59.86172103881836, 'logits/rejected': -35.683475494384766, 'logits/chosen': -35.88967514038086, 'epoch': 0.01}\r\n",
      "{'loss': 0.693, 'grad_norm': 1.0954736471176147, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 8.940298721427098e-05, 'rewards/rejected': -0.00013822481560055166, 'rewards/accuracies': 0.5559630393981934, 'rewards/margins': 0.0002276278246426955, 'logps/rejected': -60.509674072265625, 'logps/chosen': -60.37810516357422, 'logits/rejected': -37.160606384277344, 'logits/chosen': -36.60385513305664, 'epoch': 0.27}\r\n",
      "{'loss': 0.6924, 'grad_norm': 1.1032326221466064, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0003123090718872845, 'rewards/rejected': -0.0011233758414164186, 'rewards/accuracies': 0.5751562714576721, 'rewards/margins': 0.0014356852043420076, 'logps/rejected': -60.68479919433594, 'logps/chosen': -59.94801712036133, 'logits/rejected': -37.10675048828125, 'logits/chosen': -36.8320426940918, 'epoch': 0.54}\r\n",
      "{'loss': 0.6913, 'grad_norm': 1.1426727771759033, 'learning_rate': 2e-06, 'rewards/chosen': 9.933675755746663e-05, 'rewards/rejected': -0.003726853523403406, 'rewards/accuracies': 0.5764062404632568, 'rewards/margins': 0.0038261902518570423, 'logps/rejected': -60.681339263916016, 'logps/chosen': -60.10118103027344, 'logits/rejected': -36.92205810546875, 'logits/chosen': -36.71466064453125, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:04<12:00,  1.22s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.67it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.71it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:03<00:02,  1.74it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.79it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.690762996673584, 'eval_runtime': 5.6348, 'eval_samples_per_second': 220.95, 'eval_steps_per_second': 1.775, 'eval_rewards/chosen': 7.940223554214754e-07, 'eval_rewards/rejected': -0.004969253204762936, 'eval_rewards/accuracies': 0.5631384253501892, 'eval_rewards/margins': 0.004970047157257795, 'eval_logps/rejected': -60.79304885864258, 'eval_logps/chosen': -60.61351776123047, 'eval_logits/rejected': -36.685585021972656, 'eval_logits/chosen': -36.326229095458984, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:09<12:00,  1.22s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.6887, 'grad_norm': 1.1245557069778442, 'learning_rate': 2e-06, 'rewards/chosen': -0.001468892558477819, 'rewards/rejected': -0.01076358463615179, 'rewards/accuracies': 0.606632649898529, 'rewards/margins': 0.009294691495597363, 'logps/rejected': -60.696022033691406, 'logps/chosen': -60.03313064575195, 'logits/rejected': -37.1989631652832, 'logits/chosen': -36.806671142578125, 'epoch': 1.08}\r\n",
      "{'loss': 0.6846, 'grad_norm': 1.1334431171417236, 'learning_rate': 2e-06, 'rewards/chosen': -0.0035980797838419676, 'rewards/rejected': -0.021389827132225037, 'rewards/accuracies': 0.640625, 'rewards/margins': 0.017791748046875, 'logps/rejected': -60.95265579223633, 'logps/chosen': -59.92093276977539, 'logits/rejected': -36.851356506347656, 'logits/chosen': -36.69597244262695, 'epoch': 1.35}\r\n",
      "{'loss': 0.6828, 'grad_norm': 1.17442786693573, 'learning_rate': 2e-06, 'rewards/chosen': -0.010201560333371162, 'rewards/rejected': -0.03206757828593254, 'rewards/accuracies': 0.6292187571525574, 'rewards/margins': 0.02186601795256138, 'logps/rejected': -60.98768997192383, 'logps/chosen': -60.2895622253418, 'logits/rejected': -36.98162841796875, 'logits/chosen': -36.5482292175293, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:15<09:05,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.22it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.83it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.82it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6870266199111938, 'eval_runtime': 5.5384, 'eval_samples_per_second': 224.794, 'eval_steps_per_second': 1.806, 'eval_rewards/chosen': -0.01635543629527092, 'eval_rewards/rejected': -0.029949616640806198, 'eval_rewards/accuracies': 0.5621639490127563, 'eval_rewards/margins': 0.013594180345535278, 'eval_logps/rejected': -61.29265594482422, 'eval_logps/chosen': -60.940650939941406, 'eval_logits/rejected': -36.4135627746582, 'eval_logits/chosen': -36.0623779296875, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:20<09:05,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.6821, 'grad_norm': 1.1014071702957153, 'learning_rate': 2e-06, 'rewards/chosen': -0.01920229010283947, 'rewards/rejected': -0.04322626069188118, 'rewards/accuracies': 0.6143749952316284, 'rewards/margins': 0.02402397245168686, 'logps/rejected': -61.51206970214844, 'logps/chosen': -60.515830993652344, 'logits/rejected': -36.571529388427734, 'logits/chosen': -36.327701568603516, 'epoch': 1.89}\r\n",
      "{'loss': 0.6763, 'grad_norm': 1.0679246187210083, 'learning_rate': 2e-06, 'rewards/chosen': -0.02551676332950592, 'rewards/rejected': -0.06190407648682594, 'rewards/accuracies': 0.6432429552078247, 'rewards/margins': 0.036387309432029724, 'logps/rejected': -61.86370086669922, 'logps/chosen': -60.54169464111328, 'logits/rejected': -36.77936935424805, 'logits/chosen': -36.33530807495117, 'epoch': 2.16}\r\n",
      "{'loss': 0.6714, 'grad_norm': 1.1051452159881592, 'learning_rate': 2e-06, 'rewards/chosen': -0.03209550678730011, 'rewards/rejected': -0.07929860800504684, 'rewards/accuracies': 0.6621875166893005, 'rewards/margins': 0.047203097492456436, 'logps/rejected': -62.171875, 'logps/chosen': -60.828670501708984, 'logits/rejected': -36.6699104309082, 'logits/chosen': -36.51446533203125, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:26<06:00,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.83it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.82it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6833451390266418, 'eval_runtime': 5.5309, 'eval_samples_per_second': 225.1, 'eval_steps_per_second': 1.808, 'eval_rewards/chosen': -0.048424746841192245, 'eval_rewards/rejected': -0.07181583344936371, 'eval_rewards/accuracies': 0.5613827109336853, 'eval_rewards/margins': 0.023391088470816612, 'eval_logps/rejected': -62.12998580932617, 'eval_logps/chosen': -61.58203125, 'eval_logits/rejected': -36.40340042114258, 'eval_logits/chosen': -36.058135986328125, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:31<06:00,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.6722, 'grad_norm': 1.0376688241958618, 'learning_rate': 2e-06, 'rewards/chosen': -0.04913467913866043, 'rewards/rejected': -0.0958922803401947, 'rewards/accuracies': 0.63671875, 'rewards/margins': 0.04675760120153427, 'logps/rejected': -62.417728424072266, 'logps/chosen': -61.37461471557617, 'logits/rejected': -36.853118896484375, 'logits/chosen': -36.3234977722168, 'epoch': 2.7}\r\n",
      "{'loss': 0.6703, 'grad_norm': 1.0933243036270142, 'learning_rate': 2e-06, 'rewards/chosen': -0.05970419570803642, 'rewards/rejected': -0.1111944168806076, 'rewards/accuracies': 0.6426562666893005, 'rewards/margins': 0.05149023234844208, 'logps/rejected': -62.761966705322266, 'logps/chosen': -61.19831848144531, 'logits/rejected': -36.5313720703125, 'logits/chosen': -36.263431549072266, 'epoch': 2.97}\r\n",
      "{'loss': 0.6636, 'grad_norm': 1.080424427986145, 'learning_rate': 2e-06, 'rewards/chosen': -0.06862041354179382, 'rewards/rejected': -0.1353875696659088, 'rewards/accuracies': 0.6559948921203613, 'rewards/margins': 0.06676715612411499, 'logps/rejected': -63.22517395019531, 'logps/chosen': -61.35595703125, 'logits/rejected': -36.597232818603516, 'logits/chosen': -36.03443145751953, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:37<02:54,  1.25s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.67it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.90it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6807937026023865, 'eval_runtime': 5.3826, 'eval_samples_per_second': 231.302, 'eval_steps_per_second': 1.858, 'eval_rewards/chosen': -0.08928513526916504, 'eval_rewards/rejected': -0.12104662507772446, 'eval_rewards/accuracies': 0.5610886812210083, 'eval_rewards/margins': 0.03176147863268852, 'eval_logps/rejected': -63.114593505859375, 'eval_logps/chosen': -62.39924240112305, 'eval_logits/rejected': -36.280818939208984, 'eval_logits/chosen': -35.949275970458984, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:42<02:54,  1.25s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\r\n",
      "{'loss': 0.662, 'grad_norm': 1.1535505056381226, 'learning_rate': 2e-06, 'rewards/chosen': -0.084340400993824, 'rewards/rejected': -0.1555270254611969, 'rewards/accuracies': 0.6615625023841858, 'rewards/margins': 0.0711866244673729, 'logps/rejected': -63.81478500366211, 'logps/chosen': -61.96696853637695, 'logits/rejected': -36.43587112426758, 'logits/chosen': -36.17151641845703, 'epoch': 3.51}\r\n",
      "{'loss': 0.659, 'grad_norm': 1.1015009880065918, 'learning_rate': 2e-06, 'rewards/chosen': -0.09916459769010544, 'rewards/rejected': -0.1782558262348175, 'rewards/accuracies': 0.6596875190734863, 'rewards/margins': 0.07909123599529266, 'logps/rejected': -64.13634490966797, 'logps/chosen': -62.024444580078125, 'logits/rejected': -36.46797180175781, 'logits/chosen': -36.4657096862793, 'epoch': 3.78}\r\n",
      "{'train_runtime': 953.7832, 'train_samples_per_second': 99.184, 'train_steps_per_second': 0.776, 'train_loss': 0.6768781810193448, 'epoch': 4.0}\r\n",
      "100%|█████████████████████████████████████████| 740/740 [15:36<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:38, 27.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:54<03:12, 27.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:22<02:44, 27.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:49<02:17, 27.41s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:16<01:49, 27.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:44<01:22, 27.35s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:11<00:54, 27.41s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:39<00:27, 27.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:06<00:00, 27.38s/it]\r\n",
      "mean test reward 0.7714807523904533 +/- 0.008642871230952366 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9981895387172699 from 0.00922983093187213\r\n",
      "mean KL 2.240728494219689 +/- 0.06952131347708944 full 3.208740465265388 +/- 0.023219320646242585\r\n",
      "median KL 2.5391348600387573 full 3.0535753965377808\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:35, 26.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:54<03:10, 27.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:21<02:44, 27.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:49<02:17, 27.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:17<01:49, 27.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:44<01:22, 27.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:11<00:54, 27.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:39<00:27, 27.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:06<00:00, 27.39s/it]\r\n",
      "mean test reward 0.7184949504686907 +/- 0.009257312557202203 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9975780844688416 from 0.00922983093187213\r\n",
      "mean KL 1.814565659241958 +/- 0.04895291597007698 full 1.9739553660102602 +/- 0.015619140072897079\r\n",
      "median KL 1.9346458911895752 full 1.8731588125228882\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:37, 27.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:54<03:09, 27.07s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:21<02:42, 27.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:48<02:15, 27.16s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:15<01:48, 27.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:43<01:21, 27.28s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:10<00:54, 27.26s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:37<00:27, 27.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:05<00:00, 27.25s/it]\r\n",
      "mean test reward 0.650297018975468 +/- 0.009824888049667723 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9960972368717194 from 0.00922983093187213\r\n",
      "mean KL 1.194008946945865 +/- 0.031793271527003654 full 0.9158358437707648 +/- 0.008095399293056336\r\n",
      "median KL 1.2438854575157166 full 0.8538531064987183\r\n",
      "dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20,0.05,epoch 4,0.7714807523904533,0.008642871230952366,3.208740465265388,0.023219320646242585\r\n",
      "dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20,0.05,epoch 3,0.7184949504686907,0.009257312557202203,1.9739553660102602,0.015619140072897079\r\n",
      "dpo_quantile_flipped_2c2_0_5-0_05-2024.06.02.03.20,0.05,epoch 2,0.650297018975468,0.009824888049667723,0.9158358437707648,0.008095399293056336\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_quantile_flipped_2c2_0_5-0_05 --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_quantile_flipped_0_5_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf814dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T03:49:33.186043Z",
     "iopub.status.busy": "2024-06-02T03:49:33.185686Z",
     "iopub.status.idle": "2024-06-02T04:18:47.375704Z",
     "shell.execute_reply": "2024-06-02T04:18:47.374500Z"
    },
    "papermill": {
     "duration": 1754.440414,
     "end_time": "2024-06-02T04:18:47.378146",
     "exception": false,
     "start_time": "2024-06-02T03:49:32.937732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local changes to save\r\n",
      "Already up to date.\r\n",
      "Current branch main is up to date.\r\n",
      "2024-06-02 03:49:40.499060: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-02 03:49:40.499120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-02 03:49:40.500621: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 24895\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240602_034948-zo5nfp7a\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_quantile_flipped_2c2_0_5-0_2\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/zo5nfp7a\u001b[0m\r\n",
      "  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 4.536012649536133, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -63.73570251464844, 'logps/chosen': -59.86172103881836, 'logits/rejected': -35.683475494384766, 'logits/chosen': -35.88967514038086, 'epoch': 0.01}\r\n",
      "{'loss': 0.6927, 'grad_norm': 4.37495756149292, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.00035681703593581915, 'rewards/rejected': -0.000550927419681102, 'rewards/accuracies': 0.5543686151504517, 'rewards/margins': 0.0009077444556169212, 'logps/rejected': -60.50966262817383, 'logps/chosen': -60.378108978271484, 'logits/rejected': -37.160648345947266, 'logits/chosen': -36.60390090942383, 'epoch': 0.27}\r\n",
      "{'loss': 0.6904, 'grad_norm': 4.393909454345703, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0012688003480434418, 'rewards/rejected': -0.004383910913020372, 'rewards/accuracies': 0.5754687786102295, 'rewards/margins': 0.005652711261063814, 'logps/rejected': -60.68425369262695, 'logps/chosen': -59.94792175292969, 'logits/rejected': -37.108001708984375, 'logits/chosen': -36.83323669433594, 'epoch': 0.54}\r\n",
      "{'loss': 0.6865, 'grad_norm': 4.571755409240723, 'learning_rate': 2e-06, 'rewards/chosen': 0.0009262390667572618, 'rewards/rejected': -0.013630757108330727, 'rewards/accuracies': 0.5760937333106995, 'rewards/margins': 0.01455699559301138, 'logps/rejected': -60.674957275390625, 'logps/chosen': -60.098533630371094, 'logits/rejected': -36.92610168457031, 'logits/chosen': -36.718101501464844, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:04<12:01,  1.22s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.24it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.71it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:03<00:02,  1.75it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.77it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.79it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.81it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6851160526275635, 'eval_runtime': 5.635, 'eval_samples_per_second': 220.94, 'eval_steps_per_second': 1.775, 'eval_rewards/chosen': 0.0011104462901130319, 'eval_rewards/rejected': -0.017342161387205124, 'eval_rewards/accuracies': 0.5659694075584412, 'eval_rewards/margins': 0.018452605232596397, 'eval_logps/rejected': -60.78037643432617, 'eval_logps/chosen': -60.60799026489258, 'eval_logits/rejected': -36.69333267211914, 'eval_logits/chosen': -36.33327102661133, 'epoch': 0.81}\r\n",
      " 20%|████████▎                                | 150/740 [03:09<12:01,  1.22s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  2.00it/s]\u001b[A\r\n",
      "{'loss': 0.6779, 'grad_norm': 4.423372268676758, 'learning_rate': 2e-06, 'rewards/chosen': -0.002187418984249234, 'rewards/rejected': -0.036631666123867035, 'rewards/accuracies': 0.6118367314338684, 'rewards/margins': 0.03444424644112587, 'logps/rejected': -60.663909912109375, 'logps/chosen': -60.014686584472656, 'logits/rejected': -37.2161750793457, 'logits/chosen': -36.82111740112305, 'epoch': 1.08}\r\n",
      "{'loss': 0.6645, 'grad_norm': 4.4379191398620605, 'learning_rate': 2e-06, 'rewards/chosen': -0.0024647307582199574, 'rewards/rejected': -0.06767763197422028, 'rewards/accuracies': 0.6510937213897705, 'rewards/margins': 0.06521289050579071, 'logps/rejected': -60.863250732421875, 'logps/chosen': -59.861289978027344, 'logits/rejected': -36.892574310302734, 'logits/chosen': -36.73340606689453, 'epoch': 1.35}\r\n",
      "{'loss': 0.6611, 'grad_norm': 4.69563102722168, 'learning_rate': 2e-06, 'rewards/chosen': -0.014327840879559517, 'rewards/rejected': -0.09051662683486938, 'rewards/accuracies': 0.6439062356948853, 'rewards/margins': 0.07618878036737442, 'logps/rejected': -60.798919677734375, 'logps/chosen': -60.15717315673828, 'logits/rejected': -37.03769302368164, 'logits/chosen': -36.59125900268555, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:15<09:07,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.55it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.21it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.82it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6780125498771667, 'eval_runtime': 5.5511, 'eval_samples_per_second': 224.279, 'eval_steps_per_second': 1.801, 'eval_rewards/chosen': -0.03130389750003815, 'eval_rewards/rejected': -0.07346801459789276, 'eval_rewards/accuracies': 0.5705645084381104, 'eval_rewards/margins': 0.04216412082314491, 'eval_logps/rejected': -61.061012268066406, 'eval_logps/chosen': -60.770057678222656, 'eval_logits/rejected': -36.46506881713867, 'eval_logits/chosen': -36.11186218261719, 'epoch': 1.62}\r\n",
      " 41%|████████████████▌                        | 300/740 [06:21<09:07,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.6609, 'grad_norm': 4.219643592834473, 'learning_rate': 2e-06, 'rewards/chosen': -0.028328487649559975, 'rewards/rejected': -0.10858884453773499, 'rewards/accuracies': 0.6306250095367432, 'rewards/margins': 0.08026035130023956, 'logps/rejected': -61.190486907958984, 'logps/chosen': -60.2734260559082, 'logits/rejected': -36.64655685424805, 'logits/chosen': -36.391807556152344, 'epoch': 1.89}\r\n",
      "{'loss': 0.6432, 'grad_norm': 4.0908026695251465, 'learning_rate': 2e-06, 'rewards/chosen': -0.027232708409428596, 'rewards/rejected': -0.14789122343063354, 'rewards/accuracies': 0.673603355884552, 'rewards/margins': 0.1206585243344307, 'logps/rejected': -61.365074157714844, 'logps/chosen': -60.167518615722656, 'logits/rejected': -36.8862190246582, 'logits/chosen': -36.429283142089844, 'epoch': 2.16}\r\n",
      "{'loss': 0.6299, 'grad_norm': 4.1624627113342285, 'learning_rate': 2e-06, 'rewards/chosen': -0.022999582812190056, 'rewards/rejected': -0.17600740492343903, 'rewards/accuracies': 0.7009375095367432, 'rewards/margins': 0.15300780534744263, 'logps/rejected': -61.465938568115234, 'logps/chosen': -60.3017578125, 'logits/rejected': -36.79438400268555, 'logits/chosen': -36.60441589355469, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:26<06:00,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.82it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.82it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:04<00:01,  1.82it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.83it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6729174256324768, 'eval_runtime': 5.5396, 'eval_samples_per_second': 224.746, 'eval_steps_per_second': 1.805, 'eval_rewards/chosen': -0.06817537546157837, 'eval_rewards/rejected': -0.13156794011592865, 'eval_rewards/accuracies': 0.5755460262298584, 'eval_rewards/margins': 0.06339256465435028, 'eval_logps/rejected': -61.35151290893555, 'eval_logps/chosen': -60.95441818237305, 'eval_logits/rejected': -36.527931213378906, 'eval_logits/chosen': -36.18122100830078, 'epoch': 2.43}\r\n",
      " 61%|████████████████████████▉                | 450/740 [09:32<06:00,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s]\u001b[A\r\n",
      "{'loss': 0.6342, 'grad_norm': 3.953345537185669, 'learning_rate': 2e-06, 'rewards/chosen': -0.04405844211578369, 'rewards/rejected': -0.19223083555698395, 'rewards/accuracies': 0.6748437285423279, 'rewards/margins': 0.14817240834236145, 'logps/rejected': -61.46104049682617, 'logps/chosen': -60.612205505371094, 'logits/rejected': -36.98765563964844, 'logits/chosen': -36.455406188964844, 'epoch': 2.7}\r\n",
      "{'loss': 0.6315, 'grad_norm': 4.182889461517334, 'learning_rate': 2e-06, 'rewards/chosen': -0.04867956042289734, 'rewards/rejected': -0.20404456555843353, 'rewards/accuracies': 0.6857812404632568, 'rewards/margins': 0.15536503493785858, 'logps/rejected': -61.55830383300781, 'logps/chosen': -60.247642517089844, 'logits/rejected': -36.685638427734375, 'logits/chosen': -36.38597869873047, 'epoch': 2.97}\r\n",
      "{'loss': 0.6098, 'grad_norm': 3.9850594997406006, 'learning_rate': 2e-06, 'rewards/chosen': -0.036830026656389236, 'rewards/rejected': -0.24311697483062744, 'rewards/accuracies': 0.7262213230133057, 'rewards/margins': 0.20628692209720612, 'logps/rejected': -61.733009338378906, 'logps/chosen': -60.167701721191406, 'logits/rejected': -36.78424835205078, 'logits/chosen': -36.18940353393555, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:38<02:54,  1.24s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\r\n",
      " 20%|████████▊                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\r\n",
      " 30%|█████████████▏                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\r\n",
      " 40%|█████████████████▌                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\r\n",
      " 50%|██████████████████████                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\r\n",
      " 60%|██████████████████████████▍                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\r\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\r\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\r\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.6712625622749329, 'eval_runtime': 5.3727, 'eval_samples_per_second': 231.729, 'eval_steps_per_second': 1.861, 'eval_rewards/chosen': -0.09751220792531967, 'eval_rewards/rejected': -0.1720971316099167, 'eval_rewards/accuracies': 0.5805275440216064, 'eval_rewards/margins': 0.07458492368459702, 'eval_logps/rejected': -61.55415725708008, 'eval_logps/chosen': -61.101097106933594, 'eval_logits/rejected': -36.454811096191406, 'eval_logits/chosen': -36.120967864990234, 'epoch': 3.24}\r\n",
      " 81%|█████████████████████████████████▏       | 600/740 [12:43<02:54,  1.24s/it]\r\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\r\n",
      "{'loss': 0.6084, 'grad_norm': 4.1993513107299805, 'learning_rate': 2e-06, 'rewards/chosen': -0.04833048954606056, 'rewards/rejected': -0.25959864258766174, 'rewards/accuracies': 0.7265625, 'rewards/margins': 0.21126815676689148, 'logps/rejected': -62.002235412597656, 'logps/chosen': -60.52181625366211, 'logits/rejected': -36.65566635131836, 'logits/chosen': -36.36115646362305, 'epoch': 3.51}\r\n",
      "{'loss': 0.6038, 'grad_norm': 4.067941188812256, 'learning_rate': 2e-06, 'rewards/chosen': -0.05516193434596062, 'rewards/rejected': -0.2810107171535492, 'rewards/accuracies': 0.7253124713897705, 'rewards/margins': 0.22584876418113708, 'logps/rejected': -61.97627258300781, 'logps/chosen': -60.31696319580078, 'logits/rejected': -36.7524299621582, 'logits/chosen': -36.708702087402344, 'epoch': 3.78}\r\n",
      "{'train_runtime': 954.5632, 'train_samples_per_second': 99.103, 'train_steps_per_second': 0.775, 'train_loss': 0.6473514653540946, 'epoch': 4.0}\r\n",
      "100%|█████████████████████████████████████████| 740/740 [15:37<00:00,  1.27s/it]\r\n",
      "Evaling epochs [4, 3, 2]\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49/checkpoint-740\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49/checkpoint-740\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:27<03:37, 27.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:07, 26.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:41, 26.88s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:14, 26.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:47, 26.97s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:41<01:21, 27.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.08s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.02s/it]\r\n",
      "mean test reward 0.6604302442563039 +/- 0.009755069539480368 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9966520965099335 from 0.00922983093187213\r\n",
      "mean KL 0.6753240211821523 +/- 0.03455432653850782 full 0.9966820260160603 +/- 0.008553571462891034\r\n",
      "median KL 0.8061961531639099 full 0.9256024658679962\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49/checkpoint-555\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49/checkpoint-555\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:34, 26.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:08, 26.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:41, 26.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:14, 26.99s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:48, 27.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:41<01:20, 26.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:08<00:54, 27.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.05s/it]\r\n",
      "mean test reward 0.6471841364306101 +/- 0.009842168400993155 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9956940710544586 from 0.00922983093187213\r\n",
      "mean KL 0.7616546888602898 +/- 0.029244035943672253 full 0.7609664523366114 +/- 0.007173616597694352\r\n",
      "median KL 0.832097738981247 full 0.6962454915046692\r\n",
      "Loading from dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49/checkpoint-370\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49/checkpoint-370\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:35, 26.97s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:08, 26.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:41, 26.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:14, 26.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:48, 27.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:08<00:54, 27.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.02s/it]\r\n",
      "mean test reward 0.5999868175646017 +/- 0.010079736816446772 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9935061633586884 from 0.00922983093187213\r\n",
      "mean KL 0.6770631686700249 +/- 0.02298921850400885 full 0.47546793573969304 +/- 0.004725482374359202\r\n",
      "median KL 0.694518655538559 full 0.43137867748737335\r\n",
      "dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49,0.2,epoch 4,0.6604302442563039,0.009755069539480368,0.9966820260160603,0.008553571462891034\r\n",
      "dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49,0.2,epoch 3,0.6471841364306101,0.009842168400993155,0.7609664523366114,0.007173616597694352\r\n",
      "dpo_quantile_flipped_2c2_0_5-0_2-2024.06.02.03.49,0.2,epoch 2,0.5999868175646017,0.010079736816446772,0.47546793573969304,0.004725482374359202\r\n"
     ]
    }
   ],
   "source": [
    "!git stash && git pull --rebase && rm -rf */*/optimizer.pt && python dpo.py --output_dir=dpo_quantile_flipped_2c2_0_5-0_2 --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_quantile_flipped_0_5_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=150 --no_remove_unused_columns --save_total_limit=3 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45086e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T04:18:48.015980Z",
     "iopub.status.busy": "2024-06-02T04:18:48.015569Z",
     "iopub.status.idle": "2024-06-02T04:46:14.051939Z",
     "shell.execute_reply": "2024-06-02T04:46:14.050761Z"
    },
    "papermill": {
     "duration": 1646.35392,
     "end_time": "2024-06-02T04:46:14.054358",
     "exception": false,
     "start_time": "2024-06-02T04:18:47.700438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 04:18:54.111711: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-06-02 04:18:54.111775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-06-02 04:18:54.113316: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n",
      "  return self.fget.__get__(instance, owner)()\r\n",
      "Load model  lvwerra/gpt2-imdb\r\n",
      "Load ref model  lvwerra/gpt2-imdb\r\n",
      "ds len 10022\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240602_041902-mkaupyuo\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_filtered_2c2_0_3-0_6\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/mkaupyuo\u001b[0m\r\n",
      "  0%|                                                   | 0/450 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\r\n",
      "{'loss': 0.6931, 'grad_norm': 14.200899124145508, 'learning_rate': 2e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -61.769798278808594, 'logps/chosen': -60.97594451904297, 'logits/rejected': -36.121891021728516, 'logits/chosen': -36.62195587158203, 'epoch': 0.01}\r\n",
      "{'loss': 0.6778, 'grad_norm': 14.048479080200195, 'learning_rate': 1e-06, 'rewards/chosen': 0.013038483448326588, 'rewards/rejected': -0.01885358616709709, 'rewards/accuracies': 0.7617984414100647, 'rewards/margins': 0.031892068684101105, 'logps/rejected': -61.347023010253906, 'logps/chosen': -60.41095733642578, 'logits/rejected': -37.92166519165039, 'logits/chosen': -36.45622634887695, 'epoch': 0.67}\r\n",
      "{'loss': 0.5869, 'grad_norm': 11.090371131896973, 'learning_rate': 2e-06, 'rewards/chosen': 0.07300447672605515, 'rewards/rejected': -0.17865005135536194, 'rewards/accuracies': 0.810885488986969, 'rewards/margins': 0.2516545355319977, 'logps/rejected': -61.900146484375, 'logps/chosen': -60.47793960571289, 'logits/rejected': -37.2134895324707, 'logits/chosen': -35.77449035644531, 'epoch': 1.33}\r\n",
      " 22%|█████████                                | 100/450 [02:04<07:03,  1.21s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.65it/s]\u001b[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.56it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.5466251373291016, 'eval_runtime': 2.1699, 'eval_samples_per_second': 231.35, 'eval_steps_per_second': 1.843, 'eval_rewards/chosen': 0.06473838537931442, 'eval_rewards/rejected': -0.31164243817329407, 'eval_rewards/accuracies': 0.8173331618309021, 'eval_rewards/margins': 0.3763808310031891, 'eval_logps/rejected': -61.82476043701172, 'eval_logps/chosen': -59.683807373046875, 'eval_logits/rejected': -37.46409606933594, 'eval_logits/chosen': -35.80860137939453, 'epoch': 1.33}\r\n",
      " 22%|█████████                                | 100/450 [02:06<07:03,  1.21s/it]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.36it/s]\u001b[A\r\n",
      "{'loss': 0.4933, 'grad_norm': 17.43467140197754, 'learning_rate': 2e-06, 'rewards/chosen': 0.07838713377714157, 'rewards/rejected': -0.4868411719799042, 'rewards/accuracies': 0.8261458873748779, 'rewards/margins': 0.565228283405304, 'logps/rejected': -62.57102584838867, 'logps/chosen': -60.7918815612793, 'logits/rejected': -37.125789642333984, 'logits/chosen': -35.799808502197266, 'epoch': 2.0}\r\n",
      "{'loss': 0.4044, 'grad_norm': 8.538296699523926, 'learning_rate': 2e-06, 'rewards/chosen': 0.07693445682525635, 'rewards/rejected': -0.8354084491729736, 'rewards/accuracies': 0.8739062547683716, 'rewards/margins': 0.91234290599823, 'logps/rejected': -62.93220520019531, 'logps/chosen': -60.527992248535156, 'logits/rejected': -36.625038146972656, 'logits/chosen': -35.456851959228516, 'epoch': 2.67}\r\n",
      " 44%|██████████████████▏                      | 200/450 [04:11<05:16,  1.26s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.63it/s]\u001b[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.57it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.44402065873146057, 'eval_runtime': 2.1625, 'eval_samples_per_second': 232.138, 'eval_steps_per_second': 1.85, 'eval_rewards/chosen': -0.03581034019589424, 'eval_rewards/rejected': -0.8746652603149414, 'eval_rewards/accuracies': 0.8315016031265259, 'eval_rewards/margins': 0.8388549089431763, 'eval_logps/rejected': -62.76313018798828, 'eval_logps/chosen': -59.85138702392578, 'eval_logits/rejected': -36.78129577636719, 'eval_logits/chosen': -35.209083557128906, 'epoch': 2.67}\r\n",
      " 44%|██████████████████▏                      | 200/450 [04:13<05:16,  1.26s/it]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]\u001b[A\r\n",
      "{'loss': 0.3575, 'grad_norm': 8.343693733215332, 'learning_rate': 2e-06, 'rewards/chosen': 0.04465845972299576, 'rewards/rejected': -1.1021511554718018, 'rewards/accuracies': 0.8858332633972168, 'rewards/margins': 1.1468095779418945, 'logps/rejected': -63.302703857421875, 'logps/chosen': -60.202354431152344, 'logits/rejected': -36.57380676269531, 'logits/chosen': -35.360958099365234, 'epoch': 3.33}\r\n",
      "{'loss': 0.3199, 'grad_norm': 15.506704330444336, 'learning_rate': 2e-06, 'rewards/chosen': 0.0316435806453228, 'rewards/rejected': -1.316786766052246, 'rewards/accuracies': 0.9095312356948853, 'rewards/margins': 1.3484301567077637, 'logps/rejected': -63.91615676879883, 'logps/chosen': -60.8441162109375, 'logits/rejected': -36.28908920288086, 'logits/chosen': -35.00513458251953, 'epoch': 4.0}\r\n",
      " 67%|███████████████████████████▎             | 300/450 [06:17<02:33,  1.02s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.63it/s]\u001b[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.58it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.4078039526939392, 'eval_runtime': 2.161, 'eval_samples_per_second': 232.304, 'eval_steps_per_second': 1.851, 'eval_rewards/chosen': -0.14305546879768372, 'eval_rewards/rejected': -1.2515308856964111, 'eval_rewards/accuracies': 0.8313360810279846, 'eval_rewards/margins': 1.1084754467010498, 'eval_logps/rejected': -63.3912353515625, 'eval_logps/chosen': -60.03013229370117, 'eval_logits/rejected': -36.600242614746094, 'eval_logits/chosen': -35.09309387207031, 'epoch': 4.0}\r\n",
      " 67%|███████████████████████████▎             | 300/450 [06:19<02:33,  1.02s/it]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]\u001b[A\r\n",
      "{'loss': 0.2718, 'grad_norm': 7.193558692932129, 'learning_rate': 2e-06, 'rewards/chosen': 0.028898244723677635, 'rewards/rejected': -1.5432243347167969, 'rewards/accuracies': 0.938281238079071, 'rewards/margins': 1.5721224546432495, 'logps/rejected': -64.28080749511719, 'logps/chosen': -60.543670654296875, 'logits/rejected': -36.271610260009766, 'logits/chosen': -35.02287292480469, 'epoch': 4.67}\r\n",
      "{'loss': 0.2473, 'grad_norm': 6.176252365112305, 'learning_rate': 2e-06, 'rewards/chosen': 0.012160629965364933, 'rewards/rejected': -1.7430540323257446, 'rewards/accuracies': 0.9448437690734863, 'rewards/margins': 1.7552145719528198, 'logps/rejected': -64.27835083007812, 'logps/chosen': -60.6789436340332, 'logits/rejected': -36.333473205566406, 'logits/chosen': -35.239707946777344, 'epoch': 5.33}\r\n",
      " 89%|████████████████████████████████████▍    | 400/450 [08:26<01:01,  1.23s/it]evaluation_loop\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  3.65it/s]\u001b[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:01<00:00,  2.58it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 0.38973766565322876, 'eval_runtime': 2.1629, 'eval_samples_per_second': 232.1, 'eval_steps_per_second': 1.849, 'eval_rewards/chosen': -0.24243542551994324, 'eval_rewards/rejected': -1.5440272092819214, 'eval_rewards/accuracies': 0.8389830589294434, 'eval_rewards/margins': 1.3015917539596558, 'eval_logps/rejected': -63.87873077392578, 'eval_logps/chosen': -60.195762634277344, 'eval_logits/rejected': -36.54643630981445, 'eval_logits/chosen': -35.087066650390625, 'epoch': 5.33}\r\n",
      " 89%|████████████████████████████████████▍    | 400/450 [08:28<01:01,  1.23s/it]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]\u001b[A\r\n",
      "{'loss': 0.2252, 'grad_norm': 8.914222717285156, 'learning_rate': 2e-06, 'rewards/chosen': -0.007005755789577961, 'rewards/rejected': -1.8902630805969238, 'rewards/accuracies': 0.9545833468437195, 'rewards/margins': 1.8832571506500244, 'logps/rejected': -64.78101348876953, 'logps/chosen': -60.6325798034668, 'logits/rejected': -36.42710494995117, 'logits/chosen': -35.42951583862305, 'epoch': 6.0}\r\n",
      "{'train_runtime': 589.03, 'train_samples_per_second': 96.973, 'train_steps_per_second': 0.764, 'train_loss': 0.3982866085900201, 'epoch': 6.0}\r\n",
      "100%|█████████████████████████████████████████| 450/450 [09:32<00:00,  1.27s/it]\r\n",
      "Evaling epochs [6, 5, 4, 3]\r\n",
      "Loading from dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-450\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-450\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:34, 26.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:09, 27.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:42, 27.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:14, 26.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:48, 27.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:41<01:21, 27.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:35<00:26, 26.92s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:02<00:00, 26.96s/it]\r\n",
      "mean test reward 0.7531040134966057 +/- 0.008883105485393032 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9980894029140472 from 0.00922983093187213\r\n",
      "mean KL 0.09898228603479867 +/- 0.04837293904083273 full 1.671830463604743 +/- 0.01599318507069637\r\n",
      "median KL 0.38520506024360657 full 1.5048987865447998\r\n",
      "Loading from dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-375\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-375\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:33, 26.69s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:07, 26.72s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:40, 26.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:46<02:13, 26.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:13<01:47, 26.78s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:34<00:26, 26.97s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:01<00:00, 26.86s/it]\r\n",
      "mean test reward 0.7407257094226528 +/- 0.009025353788900245 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.997983992099762 from 0.00922983093187213\r\n",
      "mean KL 0.31773483546243775 +/- 0.04364437619203692 full 1.4537476103642903 +/- 0.014551063775815554\r\n",
      "median KL 0.5584914684295654 full 1.3018968105316162\r\n",
      "Loading from dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-300\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-300\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:33, 26.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:07, 26.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:20<02:41, 26.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:47<02:14, 26.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:47, 26.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:41<01:20, 26.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:08<00:53, 26.97s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:35<00:26, 26.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:02<00:00, 26.93s/it]\r\n",
      "mean test reward 0.7075721821286025 +/- 0.00937594682562557 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9976851344108582 from 0.00922983093187213\r\n",
      "mean KL 0.37527329873410054 +/- 0.03899920688170781 full 1.201773686237478 +/- 0.012752084067294464\r\n",
      "median KL 0.6361477673053741 full 1.0751076340675354\r\n",
      "Loading from dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-225\r\n",
      "load ref model lvwerra/gpt2-imdb\r\n",
      "load train model dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18/checkpoint-225\r\n",
      "device cuda\r\n",
      "device 0\r\n",
      "Load reward model siebert/sentiment-roberta-large-english\r\n",
      "eval batch size 256\r\n",
      "test len 9\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 11%|█████                                        | 1/9 [00:26<03:32, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 22%|██████████                                   | 2/9 [00:53<03:06, 26.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 33%|███████████████                              | 3/9 [01:19<02:39, 26.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 44%|████████████████████                         | 4/9 [01:46<02:14, 26.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 56%|█████████████████████████                    | 5/9 [02:14<01:47, 26.93s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 67%|██████████████████████████████               | 6/9 [02:41<01:21, 27.10s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [03:37<00:27, 27.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\r\n",
      "  warnings.warn(\r\n",
      "100%|█████████████████████████████████████████████| 9/9 [04:05<00:00, 27.26s/it]\r\n",
      "mean test reward 0.6769995199278406 +/- 0.009641057377895765 from 0.4351168664733096 +/- 0.010216795621720676\r\n",
      "median test reward 0.9971880912780762 from 0.00922983093187213\r\n",
      "mean KL 0.44083042955349405 +/- 0.03262842037963181 full 0.9165581176413171 +/- 0.009983253336696444\r\n",
      "median KL 0.6190107464790344 full 0.819369912147522\r\n",
      "dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18,0.6,epoch 6,0.7531040134966057,0.008883105485393032,1.671830463604743,0.01599318507069637\r\n",
      "dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18,0.6,epoch 5,0.7407257094226528,0.009025353788900245,1.4537476103642903,0.014551063775815554\r\n",
      "dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18,0.6,epoch 4,0.7075721821286025,0.00937594682562557,1.201773686237478,0.012752084067294464\r\n",
      "dpo_filtered_2c2_0_3-0_6-2024.06.02.04.18,0.6,epoch 3,0.6769995199278406,0.009641057377895765,0.9165581176413171,0.009983253336696444\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf */optimizer.pt && python dpo.py --output_dir=dpo_filtered_2c2_0_3-0_6 --beta=0.6 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_filtered_0_3_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 100     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=100 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=6"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9019.979231,
   "end_time": "2024-06-02T04:46:17.069648",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-02T02:15:57.090417",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
