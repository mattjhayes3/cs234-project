{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !conda install -y gdown\n# print('hi')\n!git clone https://github.com/mattjhayes3/cs234-project.git\n%cd /kaggle/working/cs234-project\n!pip install wandb\nimport wandb\n\nwandb.login(key=\"KEY\")\n# wandb.init()\n\nfrom huggingface_hub import notebook_login,login\n\n# notebook_login(\"KEY\")\nlogin(\"KEY\")\n\n# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n!mkdir results\n!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n!pip install git+https://github.com/mattjhayes3/trl.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-24T01:11:49.593194Z","iopub.execute_input":"2024-05-24T01:11:49.593844Z","iopub.status.idle":"2024-05-24T01:12:52.703572Z","shell.execute_reply.started":"2024-05-24T01:11:49.593802Z","shell.execute_reply":"2024-05-24T01:12:52.702233Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'cs234-project'...\nremote: Enumerating objects: 111, done.\u001b[K\nremote: Counting objects: 100% (111/111), done.\u001b[K\nremote: Compressing objects: 100% (69/69), done.\u001b[K\nremote: Total 111 (delta 37), reused 98 (delta 27), pack-reused 0\u001b[K\nReceiving objects: 100% (111/111), 23.60 MiB | 21.04 MiB/s, done.\nResolving deltas: 100% (37/37), done.\nUpdating files: 100% (46/46), done.\nEncountered 2 file(s) that should have been pointers, but weren't:\n\tpref_pairs_16_token_tokenized_split/data-00000-of-00002.arrow\n\tpref_pairs_16_token_tokenized_split/data-00001-of-00002.arrow\n/kaggle/working/cs234-project\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nCollecting git+https://github.com/mattjhayes3/trl.git\n  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-lntrw15o\n  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-lntrw15o\n  Resolved https://github.com/mattjhayes3/trl.git to commit 0f327a2feb6a1bba0c9494205875bea5c868ed75\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.39.3)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.29.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.18.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.1)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.8.7.dev0-py3-none-any.whl size=209518 sha256=7836afd7f1a65a1de4fab2077bd1046c371d9242cf64de6d982bd1fafd6e5fe9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-m75tf_q_/wheels/b5/f5/74/f982e27bdb1c23b205f8bfcaaed174cf3d2c06bd1a5f5926cc\nSuccessfully built trl\nInstalling collected packages: trl\nSuccessfully installed trl-0.8.7.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=test_dpo_1_3_lr2e_6  --beta=1.3 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_tokenized_split     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=500 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch'","metadata":{"execution":{"iopub.status.busy":"2024-05-24T01:15:06.893162Z","iopub.execute_input":"2024-05-24T01:15:06.894342Z","iopub.status.idle":"2024-05-24T02:18:54.290852Z","shell.execute_reply.started":"2024-05-24T01:15:06.894304Z","shell.execute_reply":"2024-05-24T02:18:54.289629Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2024-05-24 01:15:13.055377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 01:15:13.055446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 01:15:13.056966: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 149370\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_011519-tq084gz4\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest_dpo_1_3_lr2e_6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/tq084gz4\u001b[0m\n  0%|                                                  | 0/3327 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 28.680727005004883, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -58.112220764160156, 'logps/chosen': -60.65317153930664, 'logits/rejected': -39.777442932128906, 'logits/chosen': -37.3552131652832, 'epoch': 0.0}\n{'loss': 0.6834, 'grad_norm': 29.714088439941406, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.009310489520430565, 'rewards/rejected': -0.011098209768533707, 'rewards/accuracies': 0.6626275777816772, 'rewards/margins': 0.02040869928896427, 'logps/rejected': -60.757938385009766, 'logps/chosen': -60.489986419677734, 'logits/rejected': -37.495941162109375, 'logits/chosen': -36.52004623413086, 'epoch': 0.05}\n{'loss': 0.637, 'grad_norm': 29.049484252929688, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0503896102309227, 'rewards/rejected': -0.08625271171331406, 'rewards/accuracies': 0.692187488079071, 'rewards/margins': 0.13664230704307556, 'logps/rejected': -60.466064453125, 'logps/chosen': -60.09718322753906, 'logits/rejected': -37.18763732910156, 'logits/chosen': -36.43196105957031, 'epoch': 0.09}\n{'loss': 0.5864, 'grad_norm': 25.41968536376953, 'learning_rate': 2e-06, 'rewards/chosen': 0.07863486558198929, 'rewards/rejected': -0.23231059312820435, 'rewards/accuracies': 0.7107812762260437, 'rewards/margins': 0.3109454810619354, 'logps/rejected': -61.226844787597656, 'logps/chosen': -59.97611999511719, 'logits/rejected': -37.006370544433594, 'logits/chosen': -36.074073791503906, 'epoch': 0.14}\n{'loss': 0.5582, 'grad_norm': 25.92847442626953, 'learning_rate': 2e-06, 'rewards/chosen': 0.08121586591005325, 'rewards/rejected': -0.3884570300579071, 'rewards/accuracies': 0.7142187356948853, 'rewards/margins': 0.46967288851737976, 'logps/rejected': -60.492488861083984, 'logps/chosen': -60.12797546386719, 'logits/rejected': -37.069942474365234, 'logits/chosen': -36.19537353515625, 'epoch': 0.18}\n{'loss': 0.5458, 'grad_norm': 27.457759857177734, 'learning_rate': 2e-06, 'rewards/chosen': 0.0645158439874649, 'rewards/rejected': -0.4910913407802582, 'rewards/accuracies': 0.7200000286102295, 'rewards/margins': 0.5556071996688843, 'logps/rejected': -60.695152282714844, 'logps/chosen': -59.973388671875, 'logits/rejected': -36.97435760498047, 'logits/chosen': -36.160072326660156, 'epoch': 0.23}\n{'loss': 0.5403, 'grad_norm': 25.751981735229492, 'learning_rate': 2e-06, 'rewards/chosen': 0.06075665354728699, 'rewards/rejected': -0.5325736999511719, 'rewards/accuracies': 0.7254687547683716, 'rewards/margins': 0.5933303833007812, 'logps/rejected': -60.987159729003906, 'logps/chosen': -59.99480438232422, 'logits/rejected': -36.65536117553711, 'logits/chosen': -36.07833480834961, 'epoch': 0.27}\n{'loss': 0.5299, 'grad_norm': 25.56050682067871, 'learning_rate': 2e-06, 'rewards/chosen': 0.04671602323651314, 'rewards/rejected': -0.6014143824577332, 'rewards/accuracies': 0.7326562404632568, 'rewards/margins': 0.6481303572654724, 'logps/rejected': -60.9146614074707, 'logps/chosen': -59.83918380737305, 'logits/rejected': -36.62709045410156, 'logits/chosen': -35.81825637817383, 'epoch': 0.32}\n{'loss': 0.5151, 'grad_norm': 22.4754581451416, 'learning_rate': 2e-06, 'rewards/chosen': 0.053388528525829315, 'rewards/rejected': -0.662733793258667, 'rewards/accuracies': 0.7432812452316284, 'rewards/margins': 0.7161223888397217, 'logps/rejected': -60.87151336669922, 'logps/chosen': -60.18828201293945, 'logits/rejected': -36.57352066040039, 'logits/chosen': -35.71442413330078, 'epoch': 0.36}\n{'loss': 0.5189, 'grad_norm': 24.12607765197754, 'learning_rate': 2e-06, 'rewards/chosen': 0.04271242767572403, 'rewards/rejected': -0.6989091634750366, 'rewards/accuracies': 0.7400000095367432, 'rewards/margins': 0.7416215538978577, 'logps/rejected': -61.073062896728516, 'logps/chosen': -60.18547439575195, 'logits/rejected': -36.594120025634766, 'logits/chosen': -35.82060241699219, 'epoch': 0.41}\n{'loss': 0.5017, 'grad_norm': 23.342205047607422, 'learning_rate': 2e-06, 'rewards/chosen': 0.05745026469230652, 'rewards/rejected': -0.7311557531356812, 'rewards/accuracies': 0.7578125, 'rewards/margins': 0.7886061072349548, 'logps/rejected': -60.89015197753906, 'logps/chosen': -60.28007888793945, 'logits/rejected': -36.437767028808594, 'logits/chosen': -35.49287796020508, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:00<51:08,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.87it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.85it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.84it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.81it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.80it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.78it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.79it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5051154494285583, 'eval_runtime': 32.6198, 'eval_samples_per_second': 228.971, 'eval_steps_per_second': 1.809, 'eval_rewards/chosen': 0.03946491703391075, 'eval_rewards/rejected': -0.7437695860862732, 'eval_rewards/accuracies': 0.7542402744293213, 'eval_rewards/margins': 0.7832344770431519, 'eval_logps/rejected': -60.838985443115234, 'eval_logps/chosen': -59.93927764892578, 'eval_logits/rejected': -36.47865676879883, 'eval_logits/chosen': -35.617881774902344, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:33<51:08,  1.09s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 0.5072, 'grad_norm': 24.321699142456055, 'learning_rate': 2e-06, 'rewards/chosen': 0.05151405185461044, 'rewards/rejected': -0.7372608780860901, 'rewards/accuracies': 0.7564062476158142, 'rewards/margins': 0.788774847984314, 'logps/rejected': -61.1220588684082, 'logps/chosen': -60.3363037109375, 'logits/rejected': -36.388668060302734, 'logits/chosen': -35.56071472167969, 'epoch': 0.5}\n{'loss': 0.4938, 'grad_norm': 24.565059661865234, 'learning_rate': 2e-06, 'rewards/chosen': 0.041533783078193665, 'rewards/rejected': -0.7969645857810974, 'rewards/accuracies': 0.7562500238418579, 'rewards/margins': 0.8384984731674194, 'logps/rejected': -61.31404113769531, 'logps/chosen': -60.12458801269531, 'logits/rejected': -36.3046875, 'logits/chosen': -35.36772155761719, 'epoch': 0.54}\n{'loss': 0.4962, 'grad_norm': 21.433334350585938, 'learning_rate': 2e-06, 'rewards/chosen': 0.04636986553668976, 'rewards/rejected': -0.8044605255126953, 'rewards/accuracies': 0.7581250071525574, 'rewards/margins': 0.8508303761482239, 'logps/rejected': -61.45936584472656, 'logps/chosen': -59.95139694213867, 'logits/rejected': -36.30071258544922, 'logits/chosen': -35.539466857910156, 'epoch': 0.59}\n{'loss': 0.4932, 'grad_norm': 22.974136352539062, 'learning_rate': 2e-06, 'rewards/chosen': 0.021628880873322487, 'rewards/rejected': -0.8567990660667419, 'rewards/accuracies': 0.7565624713897705, 'rewards/margins': 0.878428041934967, 'logps/rejected': -60.9606819152832, 'logps/chosen': -60.343597412109375, 'logits/rejected': -36.37528991699219, 'logits/chosen': -35.60918045043945, 'epoch': 0.63}\n{'loss': 0.4831, 'grad_norm': 29.627872467041016, 'learning_rate': 2e-06, 'rewards/chosen': 0.04037228226661682, 'rewards/rejected': -0.8422033786773682, 'rewards/accuracies': 0.7654687762260437, 'rewards/margins': 0.8825756311416626, 'logps/rejected': -60.649932861328125, 'logps/chosen': -60.22294998168945, 'logits/rejected': -36.4822998046875, 'logits/chosen': -35.72391128540039, 'epoch': 0.68}\n{'loss': 0.4867, 'grad_norm': 25.136762619018555, 'learning_rate': 2e-06, 'rewards/chosen': 0.04137540981173515, 'rewards/rejected': -0.847769558429718, 'rewards/accuracies': 0.7649999856948853, 'rewards/margins': 0.8891448974609375, 'logps/rejected': -60.769065856933594, 'logps/chosen': -59.92762756347656, 'logits/rejected': -36.60947799682617, 'logits/chosen': -35.81864547729492, 'epoch': 0.72}\n{'loss': 0.4823, 'grad_norm': 21.71862030029297, 'learning_rate': 2e-06, 'rewards/chosen': 0.050065454095602036, 'rewards/rejected': -0.8846069574356079, 'rewards/accuracies': 0.7698437571525574, 'rewards/margins': 0.9346724152565002, 'logps/rejected': -61.210323333740234, 'logps/chosen': -60.4069938659668, 'logits/rejected': -36.21344757080078, 'logits/chosen': -35.40279006958008, 'epoch': 0.77}\n{'loss': 0.488, 'grad_norm': 21.8176212310791, 'learning_rate': 2e-06, 'rewards/chosen': 0.020389055833220482, 'rewards/rejected': -0.879865825176239, 'rewards/accuracies': 0.7584375143051147, 'rewards/margins': 0.9002548456192017, 'logps/rejected': -61.025638580322266, 'logps/chosen': -60.47832107543945, 'logits/rejected': -36.3836669921875, 'logits/chosen': -35.5015983581543, 'epoch': 0.81}\n{'loss': 0.4844, 'grad_norm': 22.90935516357422, 'learning_rate': 2e-06, 'rewards/chosen': 0.01894596964120865, 'rewards/rejected': -0.9186501502990723, 'rewards/accuracies': 0.7670312523841858, 'rewards/margins': 0.9375962018966675, 'logps/rejected': -61.07275390625, 'logps/chosen': -59.95576095581055, 'logits/rejected': -36.247859954833984, 'logits/chosen': -35.54298400878906, 'epoch': 0.86}\n{'loss': 0.4711, 'grad_norm': 24.93214225769043, 'learning_rate': 2e-06, 'rewards/chosen': 0.04754520580172539, 'rewards/rejected': -0.9309400320053101, 'rewards/accuracies': 0.7712500095367432, 'rewards/margins': 0.978485107421875, 'logps/rejected': -61.15400695800781, 'logps/chosen': -59.832618713378906, 'logits/rejected': -36.45536804199219, 'logits/chosen': -35.67559814453125, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [18:33<41:50,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.80it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.46802768111228943, 'eval_runtime': 32.5929, 'eval_samples_per_second': 229.161, 'eval_steps_per_second': 1.81, 'eval_rewards/chosen': 0.03646128624677658, 'eval_rewards/rejected': -0.9387010931968689, 'eval_rewards/accuracies': 0.782515287399292, 'eval_rewards/margins': 0.9751623272895813, 'eval_logps/rejected': -60.98893356323242, 'eval_logps/chosen': -59.94158935546875, 'eval_logits/rejected': -36.2937126159668, 'eval_logits/chosen': -35.452938079833984, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [19:05<41:50,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 0.4719, 'grad_norm': 21.58256721496582, 'learning_rate': 2e-06, 'rewards/chosen': 0.03069452941417694, 'rewards/rejected': -0.9285432696342468, 'rewards/accuracies': 0.7737500071525574, 'rewards/margins': 0.959237813949585, 'logps/rejected': -61.39324188232422, 'logps/chosen': -60.05528259277344, 'logits/rejected': -36.44133377075195, 'logits/chosen': -35.71745300292969, 'epoch': 0.95}\n{'loss': 0.4623, 'grad_norm': 24.03400230407715, 'learning_rate': 2e-06, 'rewards/chosen': 0.03917998448014259, 'rewards/rejected': -0.9796926975250244, 'rewards/accuracies': 0.7803124785423279, 'rewards/margins': 1.0188727378845215, 'logps/rejected': -61.17926788330078, 'logps/chosen': -60.362083435058594, 'logits/rejected': -36.379512786865234, 'logits/chosen': -35.556766510009766, 'epoch': 0.99}\n{'loss': 0.3933, 'grad_norm': 18.73689079284668, 'learning_rate': 2e-06, 'rewards/chosen': 0.12836304306983948, 'rewards/rejected': -1.062542200088501, 'rewards/accuracies': 0.8469926714897156, 'rewards/margins': 1.190905213356018, 'logps/rejected': -60.77922439575195, 'logps/chosen': -59.4940185546875, 'logits/rejected': -36.562591552734375, 'logits/chosen': -35.624473571777344, 'epoch': 1.04}\n{'loss': 0.3785, 'grad_norm': 20.985013961791992, 'learning_rate': 2e-06, 'rewards/chosen': 0.13916334509849548, 'rewards/rejected': -1.1114000082015991, 'rewards/accuracies': 0.8470312356948853, 'rewards/margins': 1.2505632638931274, 'logps/rejected': -61.6142578125, 'logps/chosen': -60.13351058959961, 'logits/rejected': -36.32805252075195, 'logits/chosen': -35.5372200012207, 'epoch': 1.08}\n{'loss': 0.378, 'grad_norm': 19.26049041748047, 'learning_rate': 2e-06, 'rewards/chosen': 0.1461867392063141, 'rewards/rejected': -1.0978705883026123, 'rewards/accuracies': 0.860156238079071, 'rewards/margins': 1.2440571784973145, 'logps/rejected': -60.848819732666016, 'logps/chosen': -59.939903259277344, 'logits/rejected': -36.744606018066406, 'logits/chosen': -35.974327087402344, 'epoch': 1.13}\n{'loss': 0.3712, 'grad_norm': 17.592071533203125, 'learning_rate': 2e-06, 'rewards/chosen': 0.15941999852657318, 'rewards/rejected': -1.1119887828826904, 'rewards/accuracies': 0.8590624928474426, 'rewards/margins': 1.2714086771011353, 'logps/rejected': -61.332279205322266, 'logps/chosen': -59.966712951660156, 'logits/rejected': -36.629676818847656, 'logits/chosen': -35.77434158325195, 'epoch': 1.17}\n{'loss': 0.371, 'grad_norm': 18.542984008789062, 'learning_rate': 2e-06, 'rewards/chosen': 0.15306299924850464, 'rewards/rejected': -1.161117434501648, 'rewards/accuracies': 0.8525000214576721, 'rewards/margins': 1.3141802549362183, 'logps/rejected': -61.72096252441406, 'logps/chosen': -59.93807601928711, 'logits/rejected': -36.079490661621094, 'logits/chosen': -35.47669982910156, 'epoch': 1.22}\n{'loss': 0.3774, 'grad_norm': 18.890914916992188, 'learning_rate': 2e-06, 'rewards/chosen': 0.09826189279556274, 'rewards/rejected': -1.1817271709442139, 'rewards/accuracies': 0.8534374833106995, 'rewards/margins': 1.2799891233444214, 'logps/rejected': -61.40115737915039, 'logps/chosen': -60.561214447021484, 'logits/rejected': -36.510963439941406, 'logits/chosen': -35.65562438964844, 'epoch': 1.26}\n{'loss': 0.3744, 'grad_norm': 20.252107620239258, 'learning_rate': 2e-06, 'rewards/chosen': 0.14170807600021362, 'rewards/rejected': -1.153419017791748, 'rewards/accuracies': 0.8496875166893005, 'rewards/margins': 1.295127272605896, 'logps/rejected': -61.435821533203125, 'logps/chosen': -59.917423248291016, 'logits/rejected': -36.53202819824219, 'logits/chosen': -35.785518646240234, 'epoch': 1.31}\n{'loss': 0.3836, 'grad_norm': 19.4464054107666, 'learning_rate': 2e-06, 'rewards/chosen': 0.09910521656274796, 'rewards/rejected': -1.1352981328964233, 'rewards/accuracies': 0.8492187261581421, 'rewards/margins': 1.2344032526016235, 'logps/rejected': -61.18263626098633, 'logps/chosen': -60.158878326416016, 'logits/rejected': -36.54319381713867, 'logits/chosen': -35.77825164794922, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:08<32:47,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.79it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.80it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4438716471195221, 'eval_runtime': 32.615, 'eval_samples_per_second': 229.005, 'eval_steps_per_second': 1.809, 'eval_rewards/chosen': 0.03973976895213127, 'eval_rewards/rejected': -1.0642683506011963, 'eval_rewards/accuracies': 0.7945238351821899, 'eval_rewards/margins': 1.1040080785751343, 'eval_logps/rejected': -61.08552551269531, 'eval_logps/chosen': -59.93906784057617, 'eval_logits/rejected': -36.63630676269531, 'eval_logits/chosen': -35.80488204956055, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:40<32:47,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 0.3759, 'grad_norm': 22.321672439575195, 'learning_rate': 2e-06, 'rewards/chosen': 0.12406224012374878, 'rewards/rejected': -1.1519160270690918, 'rewards/accuracies': 0.8515625, 'rewards/margins': 1.2759782075881958, 'logps/rejected': -61.012901306152344, 'logps/chosen': -60.02115249633789, 'logits/rejected': -36.6638069152832, 'logits/chosen': -35.77156066894531, 'epoch': 1.4}\n{'loss': 0.3738, 'grad_norm': 20.406614303588867, 'learning_rate': 2e-06, 'rewards/chosen': 0.12403781712055206, 'rewards/rejected': -1.180033802986145, 'rewards/accuracies': 0.8504687547683716, 'rewards/margins': 1.3040716648101807, 'logps/rejected': -61.29558563232422, 'logps/chosen': -59.76491165161133, 'logits/rejected': -36.903587341308594, 'logits/chosen': -36.02775192260742, 'epoch': 1.44}\n{'loss': 0.3639, 'grad_norm': 19.90598487854004, 'learning_rate': 2e-06, 'rewards/chosen': 0.10065354406833649, 'rewards/rejected': -1.2685000896453857, 'rewards/accuracies': 0.8600000143051147, 'rewards/margins': 1.369153618812561, 'logps/rejected': -61.400428771972656, 'logps/chosen': -60.013919830322266, 'logits/rejected': -36.627784729003906, 'logits/chosen': -35.84627914428711, 'epoch': 1.49}\n{'loss': 0.3793, 'grad_norm': 21.28959083557129, 'learning_rate': 2e-06, 'rewards/chosen': 0.11003610491752625, 'rewards/rejected': -1.1978702545166016, 'rewards/accuracies': 0.8443750143051147, 'rewards/margins': 1.3079065084457397, 'logps/rejected': -61.33380889892578, 'logps/chosen': -59.77509307861328, 'logits/rejected': -36.7216682434082, 'logits/chosen': -36.045188903808594, 'epoch': 1.53}\n{'loss': 0.3666, 'grad_norm': 19.944650650024414, 'learning_rate': 2e-06, 'rewards/chosen': 0.11708708107471466, 'rewards/rejected': -1.247520089149475, 'rewards/accuracies': 0.8510937690734863, 'rewards/margins': 1.3646070957183838, 'logps/rejected': -61.45610046386719, 'logps/chosen': -60.09591293334961, 'logits/rejected': -36.75907516479492, 'logits/chosen': -35.9613037109375, 'epoch': 1.58}\n{'loss': 0.3694, 'grad_norm': 19.467153549194336, 'learning_rate': 2e-06, 'rewards/chosen': 0.09928090870380402, 'rewards/rejected': -1.257913589477539, 'rewards/accuracies': 0.8496875166893005, 'rewards/margins': 1.3571946620941162, 'logps/rejected': -61.444793701171875, 'logps/chosen': -60.430599212646484, 'logits/rejected': -36.72454833984375, 'logits/chosen': -35.9322395324707, 'epoch': 1.62}\n{'loss': 0.3698, 'grad_norm': 23.46709442138672, 'learning_rate': 2e-06, 'rewards/chosen': 0.11017318814992905, 'rewards/rejected': -1.2513782978057861, 'rewards/accuracies': 0.8467187285423279, 'rewards/margins': 1.3615514039993286, 'logps/rejected': -61.70998001098633, 'logps/chosen': -60.037391662597656, 'logits/rejected': -36.62726974487305, 'logits/chosen': -35.777626037597656, 'epoch': 1.67}\n{'loss': 0.3654, 'grad_norm': 21.255388259887695, 'learning_rate': 2e-06, 'rewards/chosen': 0.10388097912073135, 'rewards/rejected': -1.2632696628570557, 'rewards/accuracies': 0.8514062762260437, 'rewards/margins': 1.3671507835388184, 'logps/rejected': -61.5478515625, 'logps/chosen': -60.19187545776367, 'logits/rejected': -36.7227897644043, 'logits/chosen': -36.052894592285156, 'epoch': 1.71}\n{'loss': 0.37, 'grad_norm': 17.416200637817383, 'learning_rate': 2e-06, 'rewards/chosen': 0.11345477402210236, 'rewards/rejected': -1.2511897087097168, 'rewards/accuracies': 0.848437488079071, 'rewards/margins': 1.3646445274353027, 'logps/rejected': -61.46379852294922, 'logps/chosen': -60.12120056152344, 'logits/rejected': -36.6293830871582, 'logits/chosen': -36.019527435302734, 'epoch': 1.76}\n{'loss': 0.3631, 'grad_norm': 20.1650447845459, 'learning_rate': 2e-06, 'rewards/chosen': 0.13284027576446533, 'rewards/rejected': -1.2549678087234497, 'rewards/accuracies': 0.8531249761581421, 'rewards/margins': 1.3878079652786255, 'logps/rejected': -61.853633880615234, 'logps/chosen': -60.084327697753906, 'logits/rejected': -36.710445404052734, 'logits/chosen': -35.845672607421875, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [37:41<25:14,  1.14s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.64it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.42606380581855774, 'eval_runtime': 32.5864, 'eval_samples_per_second': 229.206, 'eval_steps_per_second': 1.811, 'eval_rewards/chosen': 0.026489099487662315, 'eval_rewards/rejected': -1.2052404880523682, 'eval_rewards/accuracies': 0.8026424050331116, 'eval_rewards/margins': 1.2317296266555786, 'eval_logps/rejected': -61.193965911865234, 'eval_logps/chosen': -59.949256896972656, 'eval_logits/rejected': -36.822723388671875, 'eval_logits/chosen': -36.0059700012207, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [38:14<25:14,  1.14s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 0.3642, 'grad_norm': 21.98200225830078, 'learning_rate': 2e-06, 'rewards/chosen': 0.11667142063379288, 'rewards/rejected': -1.2835876941680908, 'rewards/accuracies': 0.8520312309265137, 'rewards/margins': 1.4002591371536255, 'logps/rejected': -60.97523498535156, 'logps/chosen': -59.97018051147461, 'logits/rejected': -37.22747039794922, 'logits/chosen': -36.40961456298828, 'epoch': 1.85}\n{'loss': 0.3607, 'grad_norm': 20.272689819335938, 'learning_rate': 2e-06, 'rewards/chosen': 0.14256137609481812, 'rewards/rejected': -1.2763248682022095, 'rewards/accuracies': 0.8520312309265137, 'rewards/margins': 1.418886423110962, 'logps/rejected': -61.724464416503906, 'logps/chosen': -60.15056228637695, 'logits/rejected': -36.886165618896484, 'logits/chosen': -36.26763153076172, 'epoch': 1.89}\n{'loss': 0.3635, 'grad_norm': 20.325881958007812, 'learning_rate': 2e-06, 'rewards/chosen': 0.1110922321677208, 'rewards/rejected': -1.28879976272583, 'rewards/accuracies': 0.8503124713897705, 'rewards/margins': 1.3998918533325195, 'logps/rejected': -61.56521987915039, 'logps/chosen': -60.29570007324219, 'logits/rejected': -36.968849182128906, 'logits/chosen': -36.24735641479492, 'epoch': 1.94}\n{'loss': 0.3585, 'grad_norm': 20.320232391357422, 'learning_rate': 2e-06, 'rewards/chosen': 0.10607419908046722, 'rewards/rejected': -1.3364014625549316, 'rewards/accuracies': 0.8568750023841858, 'rewards/margins': 1.4424757957458496, 'logps/rejected': -61.36711883544922, 'logps/chosen': -60.2730598449707, 'logits/rejected': -36.971317291259766, 'logits/chosen': -36.15277099609375, 'epoch': 1.98}\n{'loss': 0.3156, 'grad_norm': 17.691978454589844, 'learning_rate': 2e-06, 'rewards/chosen': 0.18796837329864502, 'rewards/rejected': -1.3564752340316772, 'rewards/accuracies': 0.894123375415802, 'rewards/margins': 1.5444436073303223, 'logps/rejected': -61.356990814208984, 'logps/chosen': -60.14603042602539, 'logits/rejected': -36.970497131347656, 'logits/chosen': -36.24467849731445, 'epoch': 2.03}\n{'loss': 0.2847, 'grad_norm': 14.764436721801758, 'learning_rate': 2e-06, 'rewards/chosen': 0.2268770933151245, 'rewards/rejected': -1.4270706176757812, 'rewards/accuracies': 0.9223437309265137, 'rewards/margins': 1.6539477109909058, 'logps/rejected': -61.338253021240234, 'logps/chosen': -60.4528923034668, 'logits/rejected': -37.198360443115234, 'logits/chosen': -36.1561393737793, 'epoch': 2.07}\n{'loss': 0.2847, 'grad_norm': 16.300983428955078, 'learning_rate': 2e-06, 'rewards/chosen': 0.2155076563358307, 'rewards/rejected': -1.4501861333847046, 'rewards/accuracies': 0.9203125238418579, 'rewards/margins': 1.665693998336792, 'logps/rejected': -61.42988204956055, 'logps/chosen': -60.07707977294922, 'logits/rejected': -36.931400299072266, 'logits/chosen': -36.208099365234375, 'epoch': 2.12}\n{'loss': 0.2875, 'grad_norm': 15.889260292053223, 'learning_rate': 2e-06, 'rewards/chosen': 0.21435602009296417, 'rewards/rejected': -1.4467166662216187, 'rewards/accuracies': 0.917187511920929, 'rewards/margins': 1.661072850227356, 'logps/rejected': -61.68509292602539, 'logps/chosen': -60.069820404052734, 'logits/rejected': -36.689430236816406, 'logits/chosen': -36.21900939941406, 'epoch': 2.16}\n{'loss': 0.2885, 'grad_norm': 17.288944244384766, 'learning_rate': 2e-06, 'rewards/chosen': 0.16397333145141602, 'rewards/rejected': -1.5080946683883667, 'rewards/accuracies': 0.9106249809265137, 'rewards/margins': 1.6720678806304932, 'logps/rejected': -61.77054214477539, 'logps/chosen': -59.863773345947266, 'logits/rejected': -37.079410552978516, 'logits/chosen': -36.45085525512695, 'epoch': 2.21}\n{'loss': 0.2837, 'grad_norm': 16.681547164916992, 'learning_rate': 2e-06, 'rewards/chosen': 0.1892741173505783, 'rewards/rejected': -1.4972434043884277, 'rewards/accuracies': 0.9170312285423279, 'rewards/margins': 1.6865174770355225, 'logps/rejected': -62.04133224487305, 'logps/chosen': -60.23812484741211, 'logits/rejected': -37.249427795410156, 'logits/chosen': -36.3848991394043, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:15<14:53,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.64it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:15,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4121769368648529, 'eval_runtime': 32.5599, 'eval_samples_per_second': 229.392, 'eval_steps_per_second': 1.812, 'eval_rewards/chosen': 0.004608260467648506, 'eval_rewards/rejected': -1.3479666709899902, 'eval_rewards/accuracies': 0.8087747097015381, 'eval_rewards/margins': 1.3525750637054443, 'eval_logps/rejected': -61.30375671386719, 'eval_logps/chosen': -59.96609115600586, 'eval_logits/rejected': -37.19011688232422, 'eval_logits/chosen': -36.3887825012207, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:48<14:53,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 0.2828, 'grad_norm': 16.261150360107422, 'learning_rate': 2e-06, 'rewards/chosen': 0.1817120760679245, 'rewards/rejected': -1.540083885192871, 'rewards/accuracies': 0.9193750023841858, 'rewards/margins': 1.721795916557312, 'logps/rejected': -61.524070739746094, 'logps/chosen': -60.325279235839844, 'logits/rejected': -37.29489517211914, 'logits/chosen': -36.35076141357422, 'epoch': 2.3}\n{'loss': 0.2836, 'grad_norm': 16.81736946105957, 'learning_rate': 2e-06, 'rewards/chosen': 0.22001582384109497, 'rewards/rejected': -1.498349666595459, 'rewards/accuracies': 0.9126562476158142, 'rewards/margins': 1.7183656692504883, 'logps/rejected': -61.921424865722656, 'logps/chosen': -59.83879089355469, 'logits/rejected': -37.174346923828125, 'logits/chosen': -36.4049072265625, 'epoch': 2.34}\n{'loss': 0.2877, 'grad_norm': 16.016069412231445, 'learning_rate': 2e-06, 'rewards/chosen': 0.16473154723644257, 'rewards/rejected': -1.566258430480957, 'rewards/accuracies': 0.9060937762260437, 'rewards/margins': 1.730989933013916, 'logps/rejected': -61.86433410644531, 'logps/chosen': -60.06007385253906, 'logits/rejected': -37.147613525390625, 'logits/chosen': -36.45291519165039, 'epoch': 2.39}\n{'loss': 0.2889, 'grad_norm': 17.903316497802734, 'learning_rate': 2e-06, 'rewards/chosen': 0.1657436341047287, 'rewards/rejected': -1.5671380758285522, 'rewards/accuracies': 0.9084374904632568, 'rewards/margins': 1.7328816652297974, 'logps/rejected': -61.47099304199219, 'logps/chosen': -59.66455078125, 'logits/rejected': -37.315650939941406, 'logits/chosen': -36.62849426269531, 'epoch': 2.43}\n{'loss': 0.2919, 'grad_norm': 17.233787536621094, 'learning_rate': 2e-06, 'rewards/chosen': 0.16579881310462952, 'rewards/rejected': -1.5741299390792847, 'rewards/accuracies': 0.8987500071525574, 'rewards/margins': 1.7399290800094604, 'logps/rejected': -61.619686126708984, 'logps/chosen': -59.652198791503906, 'logits/rejected': -37.52986145019531, 'logits/chosen': -36.73877716064453, 'epoch': 2.48}\n{'loss': 0.2933, 'grad_norm': 17.420412063598633, 'learning_rate': 2e-06, 'rewards/chosen': 0.13121755421161652, 'rewards/rejected': -1.5701278448104858, 'rewards/accuracies': 0.9039062261581421, 'rewards/margins': 1.7013453245162964, 'logps/rejected': -61.563968658447266, 'logps/chosen': -60.283626556396484, 'logits/rejected': -37.376319885253906, 'logits/chosen': -36.703758239746094, 'epoch': 2.52}\n{'loss': 0.2941, 'grad_norm': 17.767183303833008, 'learning_rate': 2e-06, 'rewards/chosen': 0.1916223168373108, 'rewards/rejected': -1.516129732131958, 'rewards/accuracies': 0.9028124809265137, 'rewards/margins': 1.7077518701553345, 'logps/rejected': -61.80517578125, 'logps/chosen': -59.989444732666016, 'logits/rejected': -37.248355865478516, 'logits/chosen': -36.655643463134766, 'epoch': 2.57}\n{'loss': 0.2749, 'grad_norm': 16.99772834777832, 'learning_rate': 2e-06, 'rewards/chosen': 0.1919701248407364, 'rewards/rejected': -1.6170878410339355, 'rewards/accuracies': 0.9143750071525574, 'rewards/margins': 1.8090580701828003, 'logps/rejected': -61.559696197509766, 'logps/chosen': -59.98139953613281, 'logits/rejected': -37.401588439941406, 'logits/chosen': -36.5427131652832, 'epoch': 2.61}\n{'loss': 0.2823, 'grad_norm': 17.376001358032227, 'learning_rate': 2e-06, 'rewards/chosen': 0.1775948852300644, 'rewards/rejected': -1.5944744348526, 'rewards/accuracies': 0.9073437452316284, 'rewards/margins': 1.7720694541931152, 'logps/rejected': -61.749637603759766, 'logps/chosen': -60.0208740234375, 'logits/rejected': -37.48182678222656, 'logits/chosen': -36.639892578125, 'epoch': 2.66}\n{'loss': 0.2786, 'grad_norm': 18.237457275390625, 'learning_rate': 2e-06, 'rewards/chosen': 0.15851950645446777, 'rewards/rejected': -1.63445246219635, 'rewards/accuracies': 0.9107812643051147, 'rewards/margins': 1.7929719686508179, 'logps/rejected': -61.91805648803711, 'logps/chosen': -59.9798698425293, 'logits/rejected': -37.43754196166992, 'logits/chosen': -36.762908935546875, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [56:48<05:51,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.65it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.57it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.18it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.79it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.81it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.40032249689102173, 'eval_runtime': 32.5534, 'eval_samples_per_second': 229.439, 'eval_steps_per_second': 1.812, 'eval_rewards/chosen': -0.014445748180150986, 'eval_rewards/rejected': -1.4888336658477783, 'eval_rewards/accuracies': 0.8126647472381592, 'eval_rewards/margins': 1.4743880033493042, 'eval_logps/rejected': -61.41211700439453, 'eval_logps/chosen': -59.980751037597656, 'eval_logits/rejected': -37.287654876708984, 'eval_logits/chosen': -36.49500274658203, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [57:20<05:51,  1.07s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.20it/s]\u001b[A\n{'loss': 0.285, 'grad_norm': 17.180002212524414, 'learning_rate': 2e-06, 'rewards/chosen': 0.16911888122558594, 'rewards/rejected': -1.609602928161621, 'rewards/accuracies': 0.9064062237739563, 'rewards/margins': 1.7787219285964966, 'logps/rejected': -61.65388107299805, 'logps/chosen': -59.80027389526367, 'logits/rejected': -37.449974060058594, 'logits/chosen': -36.642738342285156, 'epoch': 2.75}\n{'loss': 0.2805, 'grad_norm': 18.523937225341797, 'learning_rate': 2e-06, 'rewards/chosen': 0.18180784583091736, 'rewards/rejected': -1.6185640096664429, 'rewards/accuracies': 0.9081249833106995, 'rewards/margins': 1.8003718852996826, 'logps/rejected': -61.635440826416016, 'logps/chosen': -60.14193344116211, 'logits/rejected': -37.25813674926758, 'logits/chosen': -36.470664978027344, 'epoch': 2.8}\n{'loss': 0.2824, 'grad_norm': 17.04134750366211, 'learning_rate': 2e-06, 'rewards/chosen': 0.1710614413022995, 'rewards/rejected': -1.6390491724014282, 'rewards/accuracies': 0.9079687595367432, 'rewards/margins': 1.8101105690002441, 'logps/rejected': -61.8556022644043, 'logps/chosen': -60.25514602661133, 'logits/rejected': -37.396949768066406, 'logits/chosen': -36.543418884277344, 'epoch': 2.84}\n{'loss': 0.2816, 'grad_norm': 17.08812141418457, 'learning_rate': 2e-06, 'rewards/chosen': 0.1675855666399002, 'rewards/rejected': -1.6382739543914795, 'rewards/accuracies': 0.9051562547683716, 'rewards/margins': 1.8058595657348633, 'logps/rejected': -61.517303466796875, 'logps/chosen': -60.28920364379883, 'logits/rejected': -37.24633026123047, 'logits/chosen': -36.387149810791016, 'epoch': 2.89}\n{'loss': 0.2889, 'grad_norm': 16.387924194335938, 'learning_rate': 2e-06, 'rewards/chosen': 0.16425499320030212, 'rewards/rejected': -1.6314414739608765, 'rewards/accuracies': 0.9014062285423279, 'rewards/margins': 1.7956963777542114, 'logps/rejected': -61.242000579833984, 'logps/chosen': -60.075199127197266, 'logits/rejected': -37.19670867919922, 'logits/chosen': -36.45808792114258, 'epoch': 2.93}\n{'loss': 0.2864, 'grad_norm': 15.85517692565918, 'learning_rate': 2e-06, 'rewards/chosen': 0.1547333002090454, 'rewards/rejected': -1.6396924257278442, 'rewards/accuracies': 0.9012500047683716, 'rewards/margins': 1.7944256067276, 'logps/rejected': -61.761756896972656, 'logps/chosen': -59.899654388427734, 'logits/rejected': -37.36986541748047, 'logits/chosen': -36.73062515258789, 'epoch': 2.98}\n{'train_runtime': 3811.996, 'train_samples_per_second': 111.675, 'train_steps_per_second': 0.873, 'train_loss': 0.3918151160349959, 'epoch': 3.0}\n100%|█████████████████████████████████████| 3327/3327 [1:03:15<00:00,  1.14s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-3327\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-2218\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-1109","metadata":{"execution":{"iopub.status.busy":"2024-05-24T02:20:59.995469Z","iopub.execute_input":"2024-05-24T02:20:59.996081Z","iopub.status.idle":"2024-05-24T02:35:55.280916Z","shell.execute_reply.started":"2024-05-24T02:20:59.996032Z","shell.execute_reply":"2024-05-24T02:35:55.279733Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2024-05-24 02:21:06.404006: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 02:21:06.404054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 02:21:06.405314: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-3327', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.02.21'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|████████████████████████| 24895/24895 [00:29<00:00, 850.20 examples/s]\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-3327\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_022147-9qddixoi\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.02.21\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/9qddixoi\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 2.25MB/s]\npytorch_model.bin: 100%|████████████████████| 1.42G/1.42G [00:03<00:00, 358MB/s]\ntokenizer_config.json: 100%|████████████████████| 256/256 [00:00<00:00, 760kB/s]\nvocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 3.06MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 3.69MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 150/150 [00:00<00:00, 553kB/s]\neval batch size 256\nFilter: 100%|████████████████████| 2500/2500 [00:00<00:00, 132603.57 examples/s]\nMap:   3%|▉                           | 82/2487 [00:00<00:02, 807.35 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|██████████████████████████| 2487/2487 [00:03<00:00, 768.06 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:28<03:45, 28.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:54<03:08, 26.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:20<02:39, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:46<02:11, 26.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:12<01:45, 26.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:38<01:18, 26.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:04<00:52, 26.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:31<00:26, 26.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:57<00:00, 26.38s/it]\nmean test reward 0.6772101209902025 +/- 0.009617592291198419 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9970443844795227 from 0.00922983093187213\nmean KL -0.691589875182318 +/- 0.033713892457848914 full 0.7139764305708619 +/- 0.007527183680461405\nmedian KL -0.588729053735733 full 0.6307419836521149\n2024-05-24 02:26:29.913540: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 02:26:29.913592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 02:26:29.914934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-2218', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.02.26'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-2218\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_022648-85kpsvyp\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.02.26\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/85kpsvyp\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:33, 26.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:52<03:05, 26.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:38, 26.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:45<02:12, 26.43s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:12<01:45, 26.46s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:38<01:19, 26.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:05<00:52, 26.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:31<00:26, 26.49s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:58<00:00, 26.50s/it]\nmean test reward 0.6524214433893955 +/- 0.009796054678910475 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9959471523761749 from 0.00922983093187213\nmean KL -0.48981281636062907 +/- 0.02681365938430603 full 0.5468901008344902 +/- 0.00640542116490156\nmedian KL -0.41805875301361084 full 0.47374413907527924\n2024-05-24 02:31:20.367587: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 02:31:20.367645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 02:31:20.369085: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-1109', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.02.31'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_dpo_1_3_lr2e_6-2024.05.24.01.15/checkpoint-1109\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_023132-89f8hxyp\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.02.31\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/89f8hxyp\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:30, 26.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:52<03:02, 26.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:18<02:35, 25.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:44<02:09, 25.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:10<01:43, 25.99s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:36<01:18, 26.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:02<00:52, 26.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:28<00:26, 26.05s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:55<00:00, 26.16s/it]\nmean test reward 0.6250597677083457 +/- 0.009963310611123492 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9952550828456879 from 0.00922983093187213\nmean KL -0.2537546904988833 +/- 0.022119854136805218 full 0.4295403949719103 +/- 0.0056300241672642935\nmedian KL -0.17667560279369354 full 0.3656252324581146\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --loss_type=ipo --output_dir=test_ipo_0_2_lr2e_6  --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_tokenized_split     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=500 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch'","metadata":{"execution":{"iopub.status.busy":"2024-05-24T02:39:29.002683Z","iopub.execute_input":"2024-05-24T02:39:29.003090Z","iopub.status.idle":"2024-05-24T03:43:24.154238Z","shell.execute_reply.started":"2024-05-24T02:39:29.003057Z","shell.execute_reply":"2024-05-24T03:43:24.152903Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"2024-05-24 02:39:34.889220: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 02:39:34.889274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 02:39:34.890834: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 149370\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_023941-vsrmg20d\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest_ipo_0_2_lr2e_6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/vsrmg20d\u001b[0m\n  0%|                                                  | 0/3327 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 6.25, 'grad_norm': 14.456218719482422, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -3.736757755279541, 'logps/chosen': -3.8562333583831787, 'logits/rejected': -39.777442932128906, 'logits/chosen': -37.3552131652832, 'epoch': 0.0}\n{'loss': 6.2451, 'grad_norm': 14.814682006835938, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 8.420879748882726e-05, 'rewards/rejected': -0.0001104798138840124, 'rewards/accuracies': 0.659757673740387, 'rewards/margins': 0.00019468861864879727, 'logps/rejected': -3.8712146282196045, 'logps/chosen': -3.8451056480407715, 'logits/rejected': -37.495357513427734, 'logits/chosen': -36.519248962402344, 'epoch': 0.05}\n{'loss': 6.2149, 'grad_norm': 16.015718460083008, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.00046305530122481287, 'rewards/rejected': -0.0009553615236654878, 'rewards/accuracies': 0.6907812356948853, 'rewards/margins': 0.001418416853994131, 'logps/rejected': -3.8469817638397217, 'logps/chosen': -3.8276281356811523, 'logits/rejected': -37.16141128540039, 'logits/chosen': -36.40571594238281, 'epoch': 0.09}\n{'loss': 6.1514, 'grad_norm': 14.404092788696289, 'learning_rate': 2e-06, 'rewards/chosen': 0.0004925158573314548, 'rewards/rejected': -0.0035355815198272467, 'rewards/accuracies': 0.6881250143051147, 'rewards/margins': 0.00402809726074338, 'logps/rejected': -3.8932945728302, 'logps/chosen': -3.8177614212036133, 'logits/rejected': -36.846744537353516, 'logits/chosen': -35.93013000488281, 'epoch': 0.14}\n{'loss': 6.0584, 'grad_norm': 15.64615249633789, 'learning_rate': 2e-06, 'rewards/chosen': -0.0013593235053122044, 'rewards/rejected': -0.009350047446787357, 'rewards/accuracies': 0.6948437690734863, 'rewards/margins': 0.00799072440713644, 'logps/rejected': -3.8836634159088135, 'logps/chosen': -3.8414223194122314, 'logits/rejected': -36.741615295410156, 'logits/chosen': -35.91670227050781, 'epoch': 0.18}\n{'loss': 5.96, 'grad_norm': 15.048084259033203, 'learning_rate': 2e-06, 'rewards/chosen': -0.006556093692779541, 'rewards/rejected': -0.018922174349427223, 'rewards/accuracies': 0.6920312643051147, 'rewards/margins': 0.012366078794002533, 'logps/rejected': -3.933176279067993, 'logps/chosen': -3.8491406440734863, 'logits/rejected': -36.58111572265625, 'logits/chosen': -35.87077713012695, 'epoch': 0.23}\n{'loss': 5.869, 'grad_norm': 15.50841999053955, 'learning_rate': 2e-06, 'rewards/chosen': -0.014647203497588634, 'rewards/rejected': -0.03142208233475685, 'rewards/accuracies': 0.6860937476158142, 'rewards/margins': 0.01677487976849079, 'logps/rejected': -4.014113426208496, 'logps/chosen': -3.893577814102173, 'logits/rejected': -36.222023010253906, 'logits/chosen': -35.836734771728516, 'epoch': 0.27}\n{'loss': 5.7506, 'grad_norm': 14.567360877990723, 'learning_rate': 2e-06, 'rewards/chosen': -0.026183972135186195, 'rewards/rejected': -0.048831433057785034, 'rewards/accuracies': 0.6889062523841858, 'rewards/margins': 0.022647468373179436, 'logps/rejected': -4.0906291007995605, 'logps/chosen': -3.9456536769866943, 'logits/rejected': -36.034576416015625, 'logits/chosen': -35.48628234863281, 'epoch': 0.32}\n{'loss': 5.6374, 'grad_norm': 13.97270679473877, 'learning_rate': 2e-06, 'rewards/chosen': -0.041644010692834854, 'rewards/rejected': -0.07039479911327362, 'rewards/accuracies': 0.694531261920929, 'rewards/margins': 0.028750792145729065, 'logps/rejected': -4.192198276519775, 'logps/chosen': -4.032529830932617, 'logits/rejected': -35.889041900634766, 'logits/chosen': -35.323673248291016, 'epoch': 0.36}\n{'loss': 5.5439, 'grad_norm': 17.914260864257812, 'learning_rate': 2e-06, 'rewards/chosen': -0.061536382883787155, 'rewards/rejected': -0.09631569683551788, 'rewards/accuracies': 0.6871874928474426, 'rewards/margins': 0.03477931395173073, 'logps/rejected': -4.326098918914795, 'logps/chosen': -4.131920337677002, 'logits/rejected': -35.884098052978516, 'logits/chosen': -35.464298248291016, 'epoch': 0.41}\n{'loss': 5.4321, 'grad_norm': 14.169734001159668, 'learning_rate': 2e-06, 'rewards/chosen': -0.07886410504579544, 'rewards/rejected': -0.12062884122133255, 'rewards/accuracies': 0.6926562786102295, 'rewards/margins': 0.04176473617553711, 'logps/rejected': -4.4417338371276855, 'logps/chosen': -4.228853225708008, 'logits/rejected': -35.4750862121582, 'logits/chosen': -34.938053131103516, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:01<51:16,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.18it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.96it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.82it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.82it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.82it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.79it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.81it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.77it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 5.411036491394043, 'eval_runtime': 32.5385, 'eval_samples_per_second': 229.544, 'eval_steps_per_second': 1.813, 'eval_rewards/chosen': -0.0895095020532608, 'eval_rewards/rejected': -0.1330842524766922, 'eval_rewards/accuracies': 0.6879208087921143, 'eval_rewards/margins': 0.043574754148721695, 'eval_logps/rejected': -4.508194923400879, 'eval_logps/chosen': -4.268660068511963, 'eval_logits/rejected': -35.4733772277832, 'eval_logits/chosen': -35.08473587036133, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:33<51:16,  1.09s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 5.3543, 'grad_norm': 13.778247833251953, 'learning_rate': 2e-06, 'rewards/chosen': -0.09832478314638138, 'rewards/rejected': -0.14555130898952484, 'rewards/accuracies': 0.6953125, 'rewards/margins': 0.04722652956843376, 'logps/rejected': -4.5809712409973145, 'logps/chosen': -4.33078145980835, 'logits/rejected': -35.462371826171875, 'logits/chosen': -35.09955978393555, 'epoch': 0.5}\n{'loss': 5.2539, 'grad_norm': 15.19157600402832, 'learning_rate': 2e-06, 'rewards/chosen': -0.12102029472589493, 'rewards/rejected': -0.17552761733531952, 'rewards/accuracies': 0.6937500238418579, 'rewards/margins': 0.05450734123587608, 'logps/rejected': -4.739797592163086, 'logps/chosen': -4.431573867797852, 'logits/rejected': -35.32045364379883, 'logits/chosen': -34.74464797973633, 'epoch': 0.54}\n{'loss': 5.2438, 'grad_norm': 14.257322311401367, 'learning_rate': 2e-06, 'rewards/chosen': -0.14091023802757263, 'rewards/rejected': -0.1988517791032791, 'rewards/accuracies': 0.6951562762260437, 'rewards/margins': 0.057941537350416183, 'logps/rejected': -4.864294052124023, 'logps/chosen': -4.524480819702148, 'logits/rejected': -35.194339752197266, 'logits/chosen': -34.94398498535156, 'epoch': 0.59}\n{'loss': 5.1599, 'grad_norm': 14.982271194458008, 'learning_rate': 2e-06, 'rewards/chosen': -0.16102266311645508, 'rewards/rejected': -0.2262958586215973, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.06527320295572281, 'logps/rejected': -4.96926736831665, 'logps/chosen': -4.642488479614258, 'logits/rejected': -35.1453971862793, 'logits/chosen': -34.87565612792969, 'epoch': 0.63}\n{'loss': 5.1362, 'grad_norm': 16.880565643310547, 'learning_rate': 2e-06, 'rewards/chosen': -0.17330066859722137, 'rewards/rejected': -0.2419087290763855, 'rewards/accuracies': 0.6853125095367432, 'rewards/margins': 0.06860806792974472, 'logps/rejected': -5.031151294708252, 'logps/chosen': -4.701061248779297, 'logits/rejected': -35.12385177612305, 'logits/chosen': -34.856075286865234, 'epoch': 0.68}\n{'loss': 5.1249, 'grad_norm': 16.305553436279297, 'learning_rate': 2e-06, 'rewards/chosen': -0.18310663104057312, 'rewards/rejected': -0.2531338930130005, 'rewards/accuracies': 0.6912500262260437, 'rewards/margins': 0.07002724707126617, 'logps/rejected': -5.102138042449951, 'logps/chosen': -4.731621265411377, 'logits/rejected': -35.367652893066406, 'logits/chosen': -35.25687789916992, 'epoch': 0.72}\n{'loss': 4.9734, 'grad_norm': 14.129005432128906, 'learning_rate': 2e-06, 'rewards/chosen': -0.1961684674024582, 'rewards/rejected': -0.27549317479133606, 'rewards/accuracies': 0.7073437571525574, 'rewards/margins': 0.07932469993829727, 'logps/rejected': -5.232447624206543, 'logps/chosen': -4.818490505218506, 'logits/rejected': -35.34927749633789, 'logits/chosen': -35.152931213378906, 'epoch': 0.77}\n{'loss': 5.0495, 'grad_norm': 14.871194839477539, 'learning_rate': 2e-06, 'rewards/chosen': -0.21320265531539917, 'rewards/rejected': -0.2922776937484741, 'rewards/accuracies': 0.6962500214576721, 'rewards/margins': 0.07907503843307495, 'logps/rejected': -5.305164337158203, 'logps/chosen': -4.9056572914123535, 'logits/rejected': -35.366695404052734, 'logits/chosen': -35.051475524902344, 'epoch': 0.81}\n{'loss': 4.9837, 'grad_norm': 15.625060081481934, 'learning_rate': 2e-06, 'rewards/chosen': -0.21945522725582123, 'rewards/rejected': -0.30393850803375244, 'rewards/accuracies': 0.6964062452316284, 'rewards/margins': 0.08448329567909241, 'logps/rejected': -5.35853385925293, 'logps/chosen': -4.910303115844727, 'logits/rejected': -35.13427734375, 'logits/chosen': -35.07331466674805, 'epoch': 0.86}\n{'loss': 4.8949, 'grad_norm': 15.844674110412598, 'learning_rate': 2e-06, 'rewards/chosen': -0.22890348732471466, 'rewards/rejected': -0.31836676597595215, 'rewards/accuracies': 0.7126562595367432, 'rewards/margins': 0.08946329355239868, 'logps/rejected': -5.432451248168945, 'logps/chosen': -4.9575514793396, 'logits/rejected': -35.082210540771484, 'logits/chosen': -35.04193115234375, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [18:35<41:48,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.64it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.96it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.83it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:15,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.82it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.80it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.79it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.77it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.78it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.79it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.900213718414307, 'eval_runtime': 32.5879, 'eval_samples_per_second': 229.195, 'eval_steps_per_second': 1.81, 'eval_rewards/chosen': -0.237092524766922, 'eval_rewards/rejected': -0.3274223804473877, 'eval_rewards/accuracies': 0.706682562828064, 'eval_rewards/margins': 0.09032983332872391, 'eval_logps/rejected': -5.479886531829834, 'eval_logps/chosen': -5.006575584411621, 'eval_logits/rejected': -35.088314056396484, 'eval_logits/chosen': -35.000244140625, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [19:07<41:48,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.19it/s]\u001b[A\n{'loss': 4.8847, 'grad_norm': 14.602006912231445, 'learning_rate': 2e-06, 'rewards/chosen': -0.2379971444606781, 'rewards/rejected': -0.33006787300109863, 'rewards/accuracies': 0.7068750262260437, 'rewards/margins': 0.09207070618867874, 'logps/rejected': -5.514904975891113, 'logps/chosen': -5.0109782218933105, 'logits/rejected': -35.27653884887695, 'logits/chosen': -35.1113166809082, 'epoch': 0.95}\n{'loss': 4.7867, 'grad_norm': 15.946731567382812, 'learning_rate': 2e-06, 'rewards/chosen': -0.24709676206111908, 'rewards/rejected': -0.34688040614128113, 'rewards/accuracies': 0.7153124809265137, 'rewards/margins': 0.09978362917900085, 'logps/rejected': -5.57862663269043, 'logps/chosen': -5.072547435760498, 'logits/rejected': -34.98556137084961, 'logits/chosen': -34.98997497558594, 'epoch': 0.99}\n{'loss': 4.8031, 'grad_norm': 14.646976470947266, 'learning_rate': 2e-06, 'rewards/chosen': -0.25042805075645447, 'rewards/rejected': -0.3505445718765259, 'rewards/accuracies': 0.7191842794418335, 'rewards/margins': 0.1001165360212326, 'logps/rejected': -5.578861236572266, 'logps/chosen': -5.045675754547119, 'logits/rejected': -34.811832427978516, 'logits/chosen': -34.587528228759766, 'epoch': 1.04}\n{'loss': 4.6511, 'grad_norm': 16.602413177490234, 'learning_rate': 2e-06, 'rewards/chosen': -0.255126029253006, 'rewards/rejected': -0.3638002872467041, 'rewards/accuracies': 0.7320312261581421, 'rewards/margins': 0.10867424309253693, 'logps/rejected': -5.679156303405762, 'logps/chosen': -5.102951526641846, 'logits/rejected': -34.781822204589844, 'logits/chosen': -34.614158630371094, 'epoch': 1.08}\n{'loss': 4.6775, 'grad_norm': 15.048513412475586, 'learning_rate': 2e-06, 'rewards/chosen': -0.2660582959651947, 'rewards/rejected': -0.37551483511924744, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 0.10945650935173035, 'logps/rejected': -5.708311080932617, 'logps/chosen': -5.149472713470459, 'logits/rejected': -35.22917175292969, 'logits/chosen': -35.189422607421875, 'epoch': 1.13}\n{'loss': 4.5797, 'grad_norm': 14.612348556518555, 'learning_rate': 2e-06, 'rewards/chosen': -0.271099328994751, 'rewards/rejected': -0.38601818680763245, 'rewards/accuracies': 0.7365624904632568, 'rewards/margins': 0.11491887271404266, 'logps/rejected': -5.771615505218506, 'logps/chosen': -5.1781325340271, 'logits/rejected': -35.059993743896484, 'logits/chosen': -34.94908905029297, 'epoch': 1.17}\n{'loss': 4.5896, 'grad_norm': 15.746075630187988, 'learning_rate': 2e-06, 'rewards/chosen': -0.2796368896961212, 'rewards/rejected': -0.3968695104122162, 'rewards/accuracies': 0.739062488079071, 'rewards/margins': 0.11723259091377258, 'logps/rejected': -5.853598594665527, 'logps/chosen': -5.217813491821289, 'logits/rejected': -34.5348014831543, 'logits/chosen': -34.8331184387207, 'epoch': 1.22}\n{'loss': 4.6262, 'grad_norm': 15.148516654968262, 'learning_rate': 2e-06, 'rewards/chosen': -0.2898784875869751, 'rewards/rejected': -0.40486523509025574, 'rewards/accuracies': 0.7289062738418579, 'rewards/margins': 0.11498674750328064, 'logps/rejected': -5.881509780883789, 'logps/chosen': -5.298671722412109, 'logits/rejected': -34.544403076171875, 'logits/chosen': -34.566104888916016, 'epoch': 1.26}\n{'loss': 4.656, 'grad_norm': 16.12259292602539, 'learning_rate': 2e-06, 'rewards/chosen': -0.28483858704566956, 'rewards/rejected': -0.4015912711620331, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 0.11675264686346054, 'logps/rejected': -5.857202053070068, 'logps/chosen': -5.2417144775390625, 'logits/rejected': -34.53535461425781, 'logits/chosen': -34.5793342590332, 'epoch': 1.31}\n{'loss': 4.7336, 'grad_norm': 16.009552001953125, 'learning_rate': 2e-06, 'rewards/chosen': -0.292044073343277, 'rewards/rejected': -0.40071964263916016, 'rewards/accuracies': 0.7168750166893005, 'rewards/margins': 0.10867558419704437, 'logps/rejected': -5.846381664276123, 'logps/chosen': -5.291360855102539, 'logits/rejected': -34.221370697021484, 'logits/chosen': -34.23912048339844, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:11<32:47,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.79it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.670396327972412, 'eval_runtime': 32.6206, 'eval_samples_per_second': 228.966, 'eval_steps_per_second': 1.809, 'eval_rewards/chosen': -0.2903130352497101, 'eval_rewards/rejected': -0.4034031629562378, 'eval_rewards/accuracies': 0.7270538806915283, 'eval_rewards/margins': 0.11309009790420532, 'eval_logps/rejected': -5.8597893714904785, 'eval_logps/chosen': -5.272677898406982, 'eval_logits/rejected': -34.590824127197266, 'eval_logits/chosen': -34.69788360595703, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:43<32:47,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 4.5682, 'grad_norm': 17.417139053344727, 'learning_rate': 2e-06, 'rewards/chosen': -0.2911164164543152, 'rewards/rejected': -0.4099990725517273, 'rewards/accuracies': 0.733593761920929, 'rewards/margins': 0.11888265609741211, 'logps/rejected': -5.880793571472168, 'logps/chosen': -5.277803421020508, 'logits/rejected': -35.094783782958984, 'logits/chosen': -35.03144836425781, 'epoch': 1.4}\n{'loss': 4.5171, 'grad_norm': 15.418819427490234, 'learning_rate': 2e-06, 'rewards/chosen': -0.29899537563323975, 'rewards/rejected': -0.42365315556526184, 'rewards/accuracies': 0.7395312786102295, 'rewards/margins': 0.1246577724814415, 'logps/rejected': -5.965676784515381, 'logps/chosen': -5.301961898803711, 'logits/rejected': -35.75520706176758, 'logits/chosen': -35.708763122558594, 'epoch': 1.44}\n{'loss': 4.4991, 'grad_norm': 15.14261531829834, 'learning_rate': 2e-06, 'rewards/chosen': -0.3008679747581482, 'rewards/rejected': -0.4262542724609375, 'rewards/accuracies': 0.7403125166893005, 'rewards/margins': 0.12538625299930573, 'logps/rejected': -5.978110313415527, 'logps/chosen': -5.331019401550293, 'logits/rejected': -35.413787841796875, 'logits/chosen': -35.328712463378906, 'epoch': 1.49}\n{'loss': 4.6313, 'grad_norm': 15.28490924835205, 'learning_rate': 2e-06, 'rewards/chosen': -0.298508882522583, 'rewards/rejected': -0.4182929992675781, 'rewards/accuracies': 0.7284374833106995, 'rewards/margins': 0.11978411674499512, 'logps/rejected': -5.934910297393799, 'logps/chosen': -5.301441669464111, 'logits/rejected': -34.980838775634766, 'logits/chosen': -35.248687744140625, 'epoch': 1.53}\n{'loss': 4.4663, 'grad_norm': 16.588716506958008, 'learning_rate': 2e-06, 'rewards/chosen': -0.299543559551239, 'rewards/rejected': -0.425602525472641, 'rewards/accuracies': 0.7450000047683716, 'rewards/margins': 0.12605899572372437, 'logps/rejected': -5.978107452392578, 'logps/chosen': -5.327468395233154, 'logits/rejected': -34.80727005004883, 'logits/chosen': -34.98020935058594, 'epoch': 1.58}\n{'loss': 4.5393, 'grad_norm': 15.82684326171875, 'learning_rate': 2e-06, 'rewards/chosen': -0.3096967935562134, 'rewards/rejected': -0.4356791377067566, 'rewards/accuracies': 0.7320312261581421, 'rewards/margins': 0.1259823590517044, 'logps/rejected': -6.024076461791992, 'logps/chosen': -5.3939208984375, 'logits/rejected': -34.7959098815918, 'logits/chosen': -34.99693298339844, 'epoch': 1.62}\n{'loss': 4.5374, 'grad_norm': 17.198942184448242, 'learning_rate': 2e-06, 'rewards/chosen': -0.30630916357040405, 'rewards/rejected': -0.43317559361457825, 'rewards/accuracies': 0.7301562428474426, 'rewards/margins': 0.1268664449453354, 'logps/rejected': -6.026361465454102, 'logps/chosen': -5.3484272956848145, 'logits/rejected': -34.468318939208984, 'logits/chosen': -34.70356369018555, 'epoch': 1.67}\n{'loss': 4.5497, 'grad_norm': 17.042619705200195, 'learning_rate': 2e-06, 'rewards/chosen': -0.31066572666168213, 'rewards/rejected': -0.4361276924610138, 'rewards/accuracies': 0.7353125214576721, 'rewards/margins': 0.12546196579933167, 'logps/rejected': -6.0297770500183105, 'logps/chosen': -5.393453598022461, 'logits/rejected': -35.1285400390625, 'logits/chosen': -35.266265869140625, 'epoch': 1.71}\n{'loss': 4.5713, 'grad_norm': 14.824372291564941, 'learning_rate': 2e-06, 'rewards/chosen': -0.3043898940086365, 'rewards/rejected': -0.42829352617263794, 'rewards/accuracies': 0.7326562404632568, 'rewards/margins': 0.12390360236167908, 'logps/rejected': -5.988033294677734, 'logps/chosen': -5.355487823486328, 'logits/rejected': -35.03837585449219, 'logits/chosen': -35.49043273925781, 'epoch': 1.76}\n{'loss': 4.4962, 'grad_norm': 17.115779876708984, 'learning_rate': 2e-06, 'rewards/chosen': -0.3005554676055908, 'rewards/rejected': -0.42787590622901917, 'rewards/accuracies': 0.7403125166893005, 'rewards/margins': 0.12732040882110596, 'logps/rejected': -6.011086463928223, 'logps/chosen': -5.325493812561035, 'logits/rejected': -35.203857421875, 'logits/chosen': -35.1955451965332, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [37:45<24:04,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:31,  1.61it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:29,  1.67it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:28,  1.70it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:27,  1.74it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:26,  1.76it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.75it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:08<00:24,  1.77it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:24,  1.78it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:09<00:23,  1.79it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.79it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:13<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:14<00:18,  1.81it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:18<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:19<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:23<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:24<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:28<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:29<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:32<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.5350260734558105, 'eval_runtime': 32.8986, 'eval_samples_per_second': 227.031, 'eval_steps_per_second': 1.793, 'eval_rewards/chosen': -0.3089539706707001, 'eval_rewards/rejected': -0.4334964454174042, 'eval_rewards/accuracies': 0.7351312041282654, 'eval_rewards/margins': 0.12454245239496231, 'eval_logps/rejected': -6.010255813598633, 'eval_logps/chosen': -5.36588191986084, 'eval_logits/rejected': -35.11371612548828, 'eval_logits/chosen': -35.375362396240234, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [38:18<24:04,  1.09s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 4.4452, 'grad_norm': 19.144636154174805, 'learning_rate': 2e-06, 'rewards/chosen': -0.30778926610946655, 'rewards/rejected': -0.43940961360931396, 'rewards/accuracies': 0.742968738079071, 'rewards/margins': 0.13162033259868622, 'logps/rejected': -6.023488998413086, 'logps/chosen': -5.361232280731201, 'logits/rejected': -35.30021667480469, 'logits/chosen': -35.43443298339844, 'epoch': 1.85}\n{'loss': 4.3925, 'grad_norm': 17.434255599975586, 'learning_rate': 2e-06, 'rewards/chosen': -0.3122895359992981, 'rewards/rejected': -0.44607970118522644, 'rewards/accuracies': 0.7459375262260437, 'rewards/margins': 0.1337902694940567, 'logps/rejected': -6.090388298034668, 'logps/chosen': -5.395754337310791, 'logits/rejected': -35.4467658996582, 'logits/chosen': -35.69256591796875, 'epoch': 1.89}\n{'loss': 4.4697, 'grad_norm': 18.008026123046875, 'learning_rate': 2e-06, 'rewards/chosen': -0.3198738098144531, 'rewards/rejected': -0.4511170983314514, 'rewards/accuracies': 0.7435937523841858, 'rewards/margins': 0.1312432736158371, 'logps/rejected': -6.102530479431152, 'logps/chosen': -5.439764022827148, 'logits/rejected': -35.18544387817383, 'logits/chosen': -35.42698669433594, 'epoch': 1.94}\n{'loss': 4.3726, 'grad_norm': 16.673900604248047, 'learning_rate': 2e-06, 'rewards/chosen': -0.32020559906959534, 'rewards/rejected': -0.45823612809181213, 'rewards/accuracies': 0.7435937523841858, 'rewards/margins': 0.1380305141210556, 'logps/rejected': -6.131802558898926, 'logps/chosen': -5.435929775238037, 'logits/rejected': -35.20784378051758, 'logits/chosen': -35.24630355834961, 'epoch': 1.98}\n{'loss': 4.389, 'grad_norm': 15.810800552368164, 'learning_rate': 2e-06, 'rewards/chosen': -0.315560519695282, 'rewards/rejected': -0.4540020823478699, 'rewards/accuracies': 0.7472037672996521, 'rewards/margins': 0.13844157755374908, 'logps/rejected': -6.115234851837158, 'logps/chosen': -5.4126386642456055, 'logits/rejected': -35.04567337036133, 'logits/chosen': -35.511390686035156, 'epoch': 2.03}\n{'loss': 4.3438, 'grad_norm': 16.733551025390625, 'learning_rate': 2e-06, 'rewards/chosen': -0.32071784138679504, 'rewards/rejected': -0.4594731628894806, 'rewards/accuracies': 0.7407812476158142, 'rewards/margins': 0.13875533640384674, 'logps/rejected': -6.125610828399658, 'logps/chosen': -5.447628021240234, 'logits/rejected': -35.56923294067383, 'logits/chosen': -35.503238677978516, 'epoch': 2.07}\n{'loss': 4.2706, 'grad_norm': 19.261335372924805, 'learning_rate': 2e-06, 'rewards/chosen': -0.32120463252067566, 'rewards/rejected': -0.4646836519241333, 'rewards/accuracies': 0.7596874833106995, 'rewards/margins': 0.14347907900810242, 'logps/rejected': -6.162178993225098, 'logps/chosen': -5.4384989738464355, 'logits/rejected': -34.996551513671875, 'logits/chosen': -35.55754852294922, 'epoch': 2.12}\n{'loss': 4.2964, 'grad_norm': 17.033893585205078, 'learning_rate': 2e-06, 'rewards/chosen': -0.32844504714012146, 'rewards/rejected': -0.47285330295562744, 'rewards/accuracies': 0.7571874856948853, 'rewards/margins': 0.14440827071666718, 'logps/rejected': -6.211242198944092, 'logps/chosen': -5.475950717926025, 'logits/rejected': -35.025081634521484, 'logits/chosen': -35.5851936340332, 'epoch': 2.16}\n{'loss': 4.2776, 'grad_norm': 17.79657554626465, 'learning_rate': 2e-06, 'rewards/chosen': -0.3424564003944397, 'rewards/rejected': -0.4887354373931885, 'rewards/accuracies': 0.7556250095367432, 'rewards/margins': 0.14627903699874878, 'logps/rejected': -6.300898551940918, 'logps/chosen': -5.5322089195251465, 'logits/rejected': -35.54072189331055, 'logits/chosen': -36.06669616699219, 'epoch': 2.21}\n{'loss': 4.1363, 'grad_norm': 17.20643424987793, 'learning_rate': 2e-06, 'rewards/chosen': -0.3477957248687744, 'rewards/rejected': -0.5007663369178772, 'rewards/accuracies': 0.7706249952316284, 'rewards/margins': 0.15297064185142517, 'logps/rejected': -6.37523078918457, 'logps/chosen': -5.580541610717773, 'logits/rejected': -36.09304428100586, 'logits/chosen': -36.56156921386719, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:22<14:54,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.80it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.78it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.430030822753906, 'eval_runtime': 32.6338, 'eval_samples_per_second': 228.873, 'eval_steps_per_second': 1.808, 'eval_rewards/chosen': -0.3546942472457886, 'eval_rewards/rejected': -0.49467575550079346, 'eval_rewards/accuracies': 0.7425464987754822, 'eval_rewards/margins': 0.1399814784526825, 'eval_logps/rejected': -6.316152572631836, 'eval_logps/chosen': -5.594583511352539, 'eval_logits/rejected': -36.423423767089844, 'eval_logits/chosen': -36.83535385131836, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:54<14:54,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 4.129, 'grad_norm': 17.604978561401367, 'learning_rate': 2e-06, 'rewards/chosen': -0.3503383696079254, 'rewards/rejected': -0.5081827044487, 'rewards/accuracies': 0.7676562666893005, 'rewards/margins': 0.15784433484077454, 'logps/rejected': -6.3841352462768555, 'logps/chosen': -5.589139938354492, 'logits/rejected': -37.013763427734375, 'logits/chosen': -37.135616302490234, 'epoch': 2.3}\n{'loss': 4.1507, 'grad_norm': 18.47601318359375, 'learning_rate': 2e-06, 'rewards/chosen': -0.3461320400238037, 'rewards/rejected': -0.5026758313179016, 'rewards/accuracies': 0.7654687762260437, 'rewards/margins': 0.15654385089874268, 'logps/rejected': -6.3783416748046875, 'logps/chosen': -5.5510478019714355, 'logits/rejected': -37.04291915893555, 'logits/chosen': -37.52658462524414, 'epoch': 2.34}\n{'loss': 4.2162, 'grad_norm': 17.544231414794922, 'learning_rate': 2e-06, 'rewards/chosen': -0.3546007573604584, 'rewards/rejected': -0.5081969499588013, 'rewards/accuracies': 0.765625, 'rewards/margins': 0.1535961776971817, 'logps/rejected': -6.40798282623291, 'logps/chosen': -5.599600791931152, 'logits/rejected': -36.726951599121094, 'logits/chosen': -36.97418975830078, 'epoch': 2.39}\n{'loss': 4.1779, 'grad_norm': 19.354909896850586, 'learning_rate': 2e-06, 'rewards/chosen': -0.35513731837272644, 'rewards/rejected': -0.5090346336364746, 'rewards/accuracies': 0.7662500143051147, 'rewards/margins': 0.15389734506607056, 'logps/rejected': -6.3771653175354, 'logps/chosen': -5.581923961639404, 'logits/rejected': -36.77790451049805, 'logits/chosen': -37.14581298828125, 'epoch': 2.43}\n{'loss': 4.1935, 'grad_norm': 17.738548278808594, 'learning_rate': 2e-06, 'rewards/chosen': -0.35211819410324097, 'rewards/rejected': -0.5061068534851074, 'rewards/accuracies': 0.7646874785423279, 'rewards/margins': 0.15398864448070526, 'logps/rejected': -6.3692708015441895, 'logps/chosen': -5.574731349945068, 'logits/rejected': -37.36232376098633, 'logits/chosen': -37.72333908081055, 'epoch': 2.48}\n{'loss': 4.2403, 'grad_norm': 18.41737174987793, 'learning_rate': 2e-06, 'rewards/chosen': -0.35598933696746826, 'rewards/rejected': -0.5048841834068298, 'rewards/accuracies': 0.7598437666893005, 'rewards/margins': 0.14889484643936157, 'logps/rejected': -6.36811637878418, 'logps/chosen': -5.614960193634033, 'logits/rejected': -37.31494140625, 'logits/chosen': -37.511436462402344, 'epoch': 2.52}\n{'loss': 4.2489, 'grad_norm': 19.757904052734375, 'learning_rate': 2e-06, 'rewards/chosen': -0.35336601734161377, 'rewards/rejected': -0.504413366317749, 'rewards/accuracies': 0.7579687237739563, 'rewards/margins': 0.15104731917381287, 'logps/rejected': -6.375553131103516, 'logps/chosen': -5.5929365158081055, 'logits/rejected': -37.717308044433594, 'logits/chosen': -38.322269439697266, 'epoch': 2.57}\n{'loss': 4.0675, 'grad_norm': 21.978776931762695, 'learning_rate': 2e-06, 'rewards/chosen': -0.3518175184726715, 'rewards/rejected': -0.5116370916366577, 'rewards/accuracies': 0.7681249976158142, 'rewards/margins': 0.15981954336166382, 'logps/rejected': -6.3944878578186035, 'logps/chosen': -5.581358432769775, 'logits/rejected': -38.105560302734375, 'logits/chosen': -38.22039031982422, 'epoch': 2.61}\n{'loss': 4.1699, 'grad_norm': 19.47774314880371, 'learning_rate': 2e-06, 'rewards/chosen': -0.3602809011936188, 'rewards/rejected': -0.5172338485717773, 'rewards/accuracies': 0.7720312476158142, 'rewards/margins': 0.15695291757583618, 'logps/rejected': -6.439422607421875, 'logps/chosen': -5.630650520324707, 'logits/rejected': -38.38785171508789, 'logits/chosen': -38.6583366394043, 'epoch': 2.66}\n{'loss': 4.0768, 'grad_norm': 19.41534423828125, 'learning_rate': 2e-06, 'rewards/chosen': -0.3658095598220825, 'rewards/rejected': -0.5260773301124573, 'rewards/accuracies': 0.7729687690734863, 'rewards/margins': 0.16026777029037476, 'logps/rejected': -6.491098403930664, 'logps/chosen': -5.647166728973389, 'logits/rejected': -38.57024002075195, 'logits/chosen': -38.993743896484375, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [56:56<05:53,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.64it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.17it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.91it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.88it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:26,  1.86it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:04<00:26,  1.85it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.84it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.83it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.83it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.79it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.81it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:09<00:22,  1.81it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:20,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.82it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.82it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.82it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.82it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.82it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:15,  1.82it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.79it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.80it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.80it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.81it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:19<00:12,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.81it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.80it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.81it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.78it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:24<00:07,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.80it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.79it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.80it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:29<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.75it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.355116844177246, 'eval_runtime': 32.5226, 'eval_samples_per_second': 229.656, 'eval_steps_per_second': 1.814, 'eval_rewards/chosen': -0.3725181519985199, 'eval_rewards/rejected': -0.5205990672111511, 'eval_rewards/accuracies': 0.748505175113678, 'eval_rewards/margins': 0.14808088541030884, 'eval_logps/rejected': -6.445769309997559, 'eval_logps/chosen': -5.6837029457092285, 'eval_logits/rejected': -38.5062141418457, 'eval_logits/chosen': -38.95470428466797, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [57:28<05:53,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:31<00:00,  2.20it/s]\u001b[A\n{'loss': 4.2008, 'grad_norm': 18.128427505493164, 'learning_rate': 2e-06, 'rewards/chosen': -0.3641723692417145, 'rewards/rejected': -0.5191721320152283, 'rewards/accuracies': 0.7676562666893005, 'rewards/margins': 0.15499980747699738, 'logps/rejected': -6.441456317901611, 'logps/chosen': -5.630821704864502, 'logits/rejected': -38.7299690246582, 'logits/chosen': -39.22233581542969, 'epoch': 2.75}\n{'loss': 4.0992, 'grad_norm': 18.52496910095215, 'learning_rate': 2e-06, 'rewards/chosen': -0.3593904972076416, 'rewards/rejected': -0.5193559527397156, 'rewards/accuracies': 0.7724999785423279, 'rewards/margins': 0.15996553003787994, 'logps/rejected': -6.4415812492370605, 'logps/chosen': -5.626469135284424, 'logits/rejected': -38.244041442871094, 'logits/chosen': -38.478511810302734, 'epoch': 2.8}\n{'loss': 4.1275, 'grad_norm': 18.221195220947266, 'learning_rate': 2e-06, 'rewards/chosen': -0.3632570505142212, 'rewards/rejected': -0.5227717757225037, 'rewards/accuracies': 0.7665625214576721, 'rewards/margins': 0.15951475501060486, 'logps/rejected': -6.473721504211426, 'logps/chosen': -5.6553778648376465, 'logits/rejected': -38.75328063964844, 'logits/chosen': -39.007080078125, 'epoch': 2.84}\n{'loss': 4.099, 'grad_norm': 17.65823745727539, 'learning_rate': 2e-06, 'rewards/chosen': -0.36429327726364136, 'rewards/rejected': -0.5254828333854675, 'rewards/accuracies': 0.7706249952316284, 'rewards/margins': 0.16118957102298737, 'logps/rejected': -6.464510440826416, 'logps/chosen': -5.660465240478516, 'logits/rejected': -38.35292053222656, 'logits/chosen': -38.559410095214844, 'epoch': 2.89}\n{'loss': 4.1775, 'grad_norm': 20.915239334106445, 'learning_rate': 2e-06, 'rewards/chosen': -0.364093542098999, 'rewards/rejected': -0.523322343826294, 'rewards/accuracies': 0.7626562714576721, 'rewards/margins': 0.15922881662845612, 'logps/rejected': -6.438938140869141, 'logps/chosen': -5.646876811981201, 'logits/rejected': -38.512489318847656, 'logits/chosen': -38.77714157104492, 'epoch': 2.93}\n{'loss': 4.0964, 'grad_norm': 17.81879997253418, 'learning_rate': 2e-06, 'rewards/chosen': -0.36024391651153564, 'rewards/rejected': -0.5206279158592224, 'rewards/accuracies': 0.7724999785423279, 'rewards/margins': 0.16038402915000916, 'logps/rejected': -6.459568023681641, 'logps/chosen': -5.628156661987305, 'logits/rejected': -39.204002380371094, 'logits/chosen': -39.65256118774414, 'epoch': 2.98}\n{'train_runtime': 3820.7145, 'train_samples_per_second': 111.42, 'train_steps_per_second': 0.871, 'train_loss': 4.7261456144726735, 'epoch': 3.0}\n100%|█████████████████████████████████████| 3327/3327 [1:03:24<00:00,  1.14s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-3327\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-2218\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-1109","metadata":{"execution":{"iopub.status.busy":"2024-05-24T03:49:46.092689Z","iopub.execute_input":"2024-05-24T03:49:46.093715Z","iopub.status.idle":"2024-05-24T04:04:21.141377Z","shell.execute_reply.started":"2024-05-24T03:49:46.093666Z","shell.execute_reply":"2024-05-24T04:04:21.140063Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2024-05-24 03:49:52.831518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 03:49:52.831573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 03:49:52.833205: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-3327', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.03.49'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-3327\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_035016-u0acz16x\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.03.49\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/u0acz16x\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:35, 26.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:54<03:10, 27.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:21<02:42, 27.08s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:48<02:15, 27.13s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:15<01:48, 27.13s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.13s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.14s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.02s/it]\nmean test reward 0.9531158090788318 +/- 0.004310372281593319 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9988835453987122 from 0.00922983093187213\nmean KL 4.858753251015312 +/- 0.22202681005222072 full 24.884591753698057 +/- 0.12373624310131896\nmedian KL 6.3025829792022705 full 24.124000549316406\n2024-05-24 03:54:58.576043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 03:54:58.576103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 03:54:58.577631: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-2218', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.03.55'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-2218\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_035510-j5nkwddm\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.03.55\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/j5nkwddm\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:32, 26.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:52<03:04, 26.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:38, 26.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:45<02:11, 26.31s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:11<01:45, 26.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:38<01:19, 26.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:04<00:52, 26.43s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:31<00:26, 26.43s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [03:57<00:00, 26.43s/it]\nmean test reward 0.9467337571171962 +/- 0.004594803573190978 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9988786578178406 from 0.00922983093187213\nmean KL 5.886386766564101 +/- 0.21980112860940745 full 21.667366866436268 +/- 0.10470502051618823\nmedian KL 7.499309301376343 full 21.15037441253662\n2024-05-24 03:59:41.130277: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 03:59:41.130333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 03:59:41.131657: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-1109', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.03.59'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_ipo_0_2_lr2e_6-2024.05.24.02.39/checkpoint-1109\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_035953-zidsnzeb\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.03.59\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/zidsnzeb\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:34, 26.81s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:53<03:06, 26.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:19<02:39, 26.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:46<02:12, 26.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:13<01:46, 26.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:34<00:26, 27.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:01<00:00, 26.85s/it]\nmean test reward 0.9343359101277555 +/- 0.005091941134072881 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9988711476325989 from 0.00922983093187213\nmean KL 6.972632918558601 +/- 0.18620649512650247 full 17.656481547695066 +/- 0.08996870267738973\nmedian KL 8.084254264831543 full 17.19426155090332\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --loss_type=ipo --output_dir=test_ipo_0_4_lr2e_6  --beta=0.4 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_tokenized_split     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=500 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch'","metadata":{"execution":{"iopub.status.busy":"2024-05-24T04:22:06.993952Z","iopub.execute_input":"2024-05-24T04:22:06.994817Z","iopub.status.idle":"2024-05-24T05:26:09.505044Z","shell.execute_reply.started":"2024-05-24T04:22:06.994779Z","shell.execute_reply":"2024-05-24T05:26:09.503615Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"2024-05-24 04:22:13.072404: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 04:22:13.072462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 04:22:13.074088: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 149370\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_042219-jzzv73y8\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest_ipo_0_4_lr2e_6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/jzzv73y8\u001b[0m\n  0%|                                                  | 0/3327 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 1.5625, 'grad_norm': 7.228109359741211, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -3.736757755279541, 'logps/chosen': -3.8562333583831787, 'logits/rejected': -39.777442932128906, 'logits/chosen': -37.3552131652832, 'epoch': 0.0}\n{'loss': 1.5601, 'grad_norm': 7.400205612182617, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.00016836225404404104, 'rewards/rejected': -0.0002207142679253593, 'rewards/accuracies': 0.6589604616165161, 'rewards/margins': 0.0003890765365213156, 'logps/rejected': -3.871213912963867, 'logps/chosen': -3.8451058864593506, 'logits/rejected': -37.49537658691406, 'logits/chosen': -36.51926803588867, 'epoch': 0.05}\n{'loss': 1.5452, 'grad_norm': 7.981834888458252, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.0009273114847019315, 'rewards/rejected': -0.001896015601232648, 'rewards/accuracies': 0.6910937428474426, 'rewards/margins': 0.0028233269695192575, 'logps/rejected': -3.846944570541382, 'logps/chosen': -3.827625036239624, 'logits/rejected': -37.16267013549805, 'logits/chosen': -36.40694046020508, 'epoch': 0.09}\n{'loss': 1.5148, 'grad_norm': 7.116278648376465, 'learning_rate': 2e-06, 'rewards/chosen': 0.0010507107945159078, 'rewards/rejected': -0.006905084941536188, 'rewards/accuracies': 0.6899999976158142, 'rewards/margins': 0.007955795153975487, 'logps/rejected': -3.8928799629211426, 'logps/chosen': -3.8175971508026123, 'logits/rejected': -36.856849670410156, 'logits/chosen': -35.93939971923828, 'epoch': 0.14}\n{'loss': 1.4731, 'grad_norm': 7.75335693359375, 'learning_rate': 2e-06, 'rewards/chosen': -0.002225959673523903, 'rewards/rejected': -0.0177568681538105, 'rewards/accuracies': 0.6967187523841858, 'rewards/margins': 0.0155309047549963, 'logps/rejected': -3.88130521774292, 'logps/chosen': -3.8401904106140137, 'logits/rejected': -36.76581573486328, 'logits/chosen': -35.937347412109375, 'epoch': 0.18}\n{'loss': 1.4325, 'grad_norm': 7.379964351654053, 'learning_rate': 2e-06, 'rewards/chosen': -0.010967588052153587, 'rewards/rejected': -0.03447309508919716, 'rewards/accuracies': 0.694531261920929, 'rewards/margins': 0.02350550703704357, 'logps/rejected': -3.924748182296753, 'logps/chosen': -3.8437790870666504, 'logits/rejected': -36.61513900756836, 'logits/chosen': -35.89467239379883, 'epoch': 0.23}\n{'loss': 1.4012, 'grad_norm': 7.713629245758057, 'learning_rate': 2e-06, 'rewards/chosen': -0.023184629157185555, 'rewards/rejected': -0.054034262895584106, 'rewards/accuracies': 0.6896874904632568, 'rewards/margins': 0.030849630013108253, 'logps/rejected': -3.9920883178710938, 'logps/chosen': -3.8783035278320312, 'logits/rejected': -36.25334930419922, 'logits/chosen': -35.84507369995117, 'epoch': 0.27}\n{'loss': 1.3621, 'grad_norm': 7.019481658935547, 'learning_rate': 2e-06, 'rewards/chosen': -0.03852478414773941, 'rewards/rejected': -0.07856079936027527, 'rewards/accuracies': 0.6953125, 'rewards/margins': 0.04003601521253586, 'logps/rejected': -4.042873382568359, 'logps/chosen': -3.911045551300049, 'logits/rejected': -36.0502815246582, 'logits/chosen': -35.461875915527344, 'epoch': 0.32}\n{'loss': 1.3281, 'grad_norm': 6.5793070793151855, 'learning_rate': 2e-06, 'rewards/chosen': -0.05652293190360069, 'rewards/rejected': -0.10556735843420029, 'rewards/accuracies': 0.7017187476158142, 'rewards/margins': 0.049044419080019, 'logps/rejected': -4.104142665863037, 'logps/chosen': -3.9656167030334473, 'logits/rejected': -35.85407257080078, 'logits/chosen': -35.237205505371094, 'epoch': 0.36}\n{'loss': 1.3084, 'grad_norm': 8.039196968078613, 'learning_rate': 2e-06, 'rewards/chosen': -0.07691627740859985, 'rewards/rejected': -0.13365492224693298, 'rewards/accuracies': 0.6956250071525574, 'rewards/margins': 0.05673865228891373, 'logps/rejected': -4.178657531738281, 'logps/chosen': -4.016528129577637, 'logits/rejected': -35.747802734375, 'logits/chosen': -35.26899719238281, 'epoch': 0.41}\n{'loss': 1.2773, 'grad_norm': 6.594487190246582, 'learning_rate': 2e-06, 'rewards/chosen': -0.08998072892427444, 'rewards/rejected': -0.1558780074119568, 'rewards/accuracies': 0.7034375071525574, 'rewards/margins': 0.06589728593826294, 'logps/rejected': -4.22828483581543, 'logps/chosen': -4.059484481811523, 'logits/rejected': -35.283878326416016, 'logits/chosen': -34.6610221862793, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:02<51:10,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:16,  3.55it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:22,  2.53it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.15it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.94it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.89it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.82it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.81it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.78it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:24,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.79it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.79it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.80it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.80it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.79it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.79it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.79it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.79it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:29<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.2776670455932617, 'eval_runtime': 32.7098, 'eval_samples_per_second': 228.342, 'eval_steps_per_second': 1.804, 'eval_rewards/chosen': -0.09824612736701965, 'eval_rewards/rejected': -0.16511155664920807, 'eval_rewards/accuracies': 0.700632631778717, 'eval_rewards/margins': 0.06686542183160782, 'eval_logps/rejected': -4.255552768707275, 'eval_logps/chosen': -4.066728115081787, 'eval_logits/rejected': -35.216957092285156, 'eval_logits/chosen': -34.74100112915039, 'epoch': 0.45}\n 15%|██████                                  | 500/3327 [09:35<51:10,  1.09s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 1.2661, 'grad_norm': 6.530534267425537, 'learning_rate': 2e-06, 'rewards/chosen': -0.10325212776660919, 'rewards/rejected': -0.1735280603170395, 'rewards/accuracies': 0.7082812786102295, 'rewards/margins': 0.0702759325504303, 'logps/rejected': -4.2870354652404785, 'logps/chosen': -4.097288131713867, 'logits/rejected': -35.13304138183594, 'logits/chosen': -34.67094421386719, 'epoch': 0.5}\n{'loss': 1.2412, 'grad_norm': 7.202078819274902, 'learning_rate': 2e-06, 'rewards/chosen': -0.11797996610403061, 'rewards/rejected': -0.1962297260761261, 'rewards/accuracies': 0.7081249952316284, 'rewards/margins': 0.07824976742267609, 'logps/rejected': -4.352733612060547, 'logps/chosen': -4.121422290802002, 'logits/rejected': -34.77302551269531, 'logits/chosen': -34.13660430908203, 'epoch': 0.54}\n{'loss': 1.2473, 'grad_norm': 6.30176305770874, 'learning_rate': 2e-06, 'rewards/chosen': -0.12804371118545532, 'rewards/rejected': -0.20770004391670227, 'rewards/accuracies': 0.7026562690734863, 'rewards/margins': 0.07965633273124695, 'logps/rejected': -4.389285087585449, 'logps/chosen': -4.140038967132568, 'logits/rejected': -34.49418640136719, 'logits/chosen': -34.117767333984375, 'epoch': 0.59}\n{'loss': 1.2288, 'grad_norm': 6.868149757385254, 'learning_rate': 2e-06, 'rewards/chosen': -0.1384623497724533, 'rewards/rejected': -0.22504064440727234, 'rewards/accuracies': 0.7064062356948853, 'rewards/margins': 0.08657827973365784, 'logps/rejected': -4.400389671325684, 'logps/chosen': -4.183530807495117, 'logits/rejected': -34.28541564941406, 'logits/chosen': -33.909461975097656, 'epoch': 0.63}\n{'loss': 1.2242, 'grad_norm': 7.591238021850586, 'learning_rate': 2e-06, 'rewards/chosen': -0.14154408872127533, 'rewards/rejected': -0.22998180985450745, 'rewards/accuracies': 0.7057812213897705, 'rewards/margins': 0.08843772858381271, 'logps/rejected': -4.396562099456787, 'logps/chosen': -4.188417434692383, 'logits/rejected': -34.19776916503906, 'logits/chosen': -33.79094314575195, 'epoch': 0.68}\n{'loss': 1.2247, 'grad_norm': 7.132004261016846, 'learning_rate': 2e-06, 'rewards/chosen': -0.14496584236621857, 'rewards/rejected': -0.23371641337871552, 'rewards/accuracies': 0.7085937261581421, 'rewards/margins': 0.08875056356191635, 'logps/rejected': -4.420759677886963, 'logps/chosen': -4.178502082824707, 'logits/rejected': -34.26363754272461, 'logits/chosen': -33.92536926269531, 'epoch': 0.72}\n{'loss': 1.1948, 'grad_norm': 6.118073463439941, 'learning_rate': 2e-06, 'rewards/chosen': -0.15127377212047577, 'rewards/rejected': -0.24794624745845795, 'rewards/accuracies': 0.7196875214576721, 'rewards/margins': 0.09667249023914337, 'logps/rejected': -4.474847316741943, 'logps/chosen': -4.215832710266113, 'logits/rejected': -33.935367584228516, 'logits/chosen': -33.534481048583984, 'epoch': 0.77}\n{'loss': 1.2176, 'grad_norm': 6.586092948913574, 'learning_rate': 2e-06, 'rewards/chosen': -0.16089771687984467, 'rewards/rejected': -0.25516754388809204, 'rewards/accuracies': 0.7118750214576721, 'rewards/margins': 0.09426982700824738, 'logps/rejected': -4.481694221496582, 'logps/chosen': -4.241888046264648, 'logits/rejected': -33.873252868652344, 'logits/chosen': -33.41965103149414, 'epoch': 0.81}\n{'loss': 1.206, 'grad_norm': 7.0395917892456055, 'learning_rate': 2e-06, 'rewards/chosen': -0.1635240614414215, 'rewards/rejected': -0.26284340023994446, 'rewards/accuracies': 0.7143750190734863, 'rewards/margins': 0.09931933134794235, 'logps/rejected': -4.495950222015381, 'logps/chosen': -4.221837997436523, 'logits/rejected': -33.5980110168457, 'logits/chosen': -33.3418083190918, 'epoch': 0.86}\n{'loss': 1.181, 'grad_norm': 6.797332763671875, 'learning_rate': 2e-06, 'rewards/chosen': -0.16457808017730713, 'rewards/rejected': -0.26830291748046875, 'rewards/accuracies': 0.7215625047683716, 'rewards/margins': 0.10372484475374222, 'logps/rejected': -4.5113749504089355, 'logps/chosen': -4.2244791984558105, 'logits/rejected': -33.440826416015625, 'logits/chosen': -33.16385269165039, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [18:37<41:54,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:24,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.79it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:20,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.80it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.80it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.80it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.79it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:15,  1.80it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.77it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.78it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.79it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.79it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.78it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.76it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.77it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.78it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.79it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:29<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.180207371711731, 'eval_runtime': 32.721, 'eval_samples_per_second': 228.263, 'eval_steps_per_second': 1.803, 'eval_rewards/chosen': -0.16919013857841492, 'eval_rewards/rejected': -0.2735345661640167, 'eval_rewards/accuracies': 0.7216866612434387, 'eval_rewards/margins': 0.10434447973966599, 'eval_logps/rejected': -4.526610851287842, 'eval_logps/chosen': -4.244088172912598, 'eval_logits/rejected': -33.27132034301758, 'eval_logits/chosen': -32.97673034667969, 'epoch': 0.9}\n 30%|███████████▋                           | 1000/3327 [19:10<41:54,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.18it/s]\u001b[A\n{'loss': 1.1799, 'grad_norm': 6.202761650085449, 'learning_rate': 2e-06, 'rewards/chosen': -0.16939887404441833, 'rewards/rejected': -0.27422505617141724, 'rewards/accuracies': 0.7240625023841858, 'rewards/margins': 0.10482614487409592, 'logps/rejected': -4.550127983093262, 'logps/chosen': -4.244489669799805, 'logits/rejected': -33.41956329345703, 'logits/chosen': -33.10097885131836, 'epoch': 0.95}\n{'loss': 1.1612, 'grad_norm': 6.82050895690918, 'learning_rate': 2e-06, 'rewards/chosen': -0.1737891137599945, 'rewards/rejected': -0.28498637676239014, 'rewards/accuracies': 0.7281249761581421, 'rewards/margins': 0.11119729280471802, 'logps/rejected': -4.556690692901611, 'logps/chosen': -4.271536350250244, 'logits/rejected': -33.160621643066406, 'logits/chosen': -32.89955139160156, 'epoch': 0.99}\n{'loss': 1.1534, 'grad_norm': 6.201323509216309, 'learning_rate': 2e-06, 'rewards/chosen': -0.1741303950548172, 'rewards/rejected': -0.2876553535461426, 'rewards/accuracies': 0.7389224767684937, 'rewards/margins': 0.11352493613958359, 'logps/rejected': -4.545276165008545, 'logps/chosen': -4.228861331939697, 'logits/rejected': -33.11741256713867, 'logits/chosen': -32.687740325927734, 'epoch': 1.04}\n{'loss': 1.1129, 'grad_norm': 6.89778470993042, 'learning_rate': 2e-06, 'rewards/chosen': -0.17490269243717194, 'rewards/rejected': -0.29699042439460754, 'rewards/accuracies': 0.7440624833106995, 'rewards/margins': 0.12208770960569382, 'logps/rejected': -4.602631092071533, 'logps/chosen': -4.264578342437744, 'logits/rejected': -32.84143829345703, 'logits/chosen': -32.49224853515625, 'epoch': 1.08}\n{'loss': 1.1255, 'grad_norm': 6.2703070640563965, 'learning_rate': 2e-06, 'rewards/chosen': -0.1822184920310974, 'rewards/rejected': -0.30317139625549316, 'rewards/accuracies': 0.7454687356948853, 'rewards/margins': 0.12095290422439575, 'logps/rejected': -4.58866548538208, 'logps/chosen': -4.274727821350098, 'logits/rejected': -33.287818908691406, 'logits/chosen': -32.99757385253906, 'epoch': 1.13}\n{'loss': 1.1013, 'grad_norm': 6.025094509124756, 'learning_rate': 2e-06, 'rewards/chosen': -0.18266621232032776, 'rewards/rejected': -0.308096319437027, 'rewards/accuracies': 0.7476562261581421, 'rewards/margins': 0.12543009221553802, 'logps/rejected': -4.6117658615112305, 'logps/chosen': -4.279300689697266, 'logits/rejected': -33.07286071777344, 'logits/chosen': -32.709041595458984, 'epoch': 1.17}\n{'loss': 1.103, 'grad_norm': 6.457464218139648, 'learning_rate': 2e-06, 'rewards/chosen': -0.18900562822818756, 'rewards/rejected': -0.31747400760650635, 'rewards/accuracies': 0.750781238079071, 'rewards/margins': 0.12846845388412476, 'logps/rejected': -4.662936210632324, 'logps/chosen': -4.292142868041992, 'logits/rejected': -32.39280700683594, 'logits/chosen': -32.414608001708984, 'epoch': 1.22}\n{'loss': 1.1138, 'grad_norm': 6.454369068145752, 'learning_rate': 2e-06, 'rewards/chosen': -0.19940787553787231, 'rewards/rejected': -0.3254622220993042, 'rewards/accuracies': 0.7462499737739563, 'rewards/margins': 0.1260543167591095, 'logps/rejected': -4.670840263366699, 'logps/chosen': -4.347799777984619, 'logits/rejected': -32.580650329589844, 'logits/chosen': -32.339447021484375, 'epoch': 1.26}\n{'loss': 1.1231, 'grad_norm': 6.668505668640137, 'learning_rate': 2e-06, 'rewards/chosen': -0.1931018829345703, 'rewards/rejected': -0.3201076090335846, 'rewards/accuracies': 0.7435937523841858, 'rewards/margins': 0.12700572609901428, 'logps/rejected': -4.649514675140381, 'logps/chosen': -4.300275802612305, 'logits/rejected': -32.623023986816406, 'logits/chosen': -32.40977096557617, 'epoch': 1.31}\n{'loss': 1.1352, 'grad_norm': 6.421352386474609, 'learning_rate': 2e-06, 'rewards/chosen': -0.1978125423192978, 'rewards/rejected': -0.317552387714386, 'rewards/accuracies': 0.7334374785423279, 'rewards/margins': 0.1197398379445076, 'logps/rejected': -4.636663913726807, 'logps/chosen': -4.325672626495361, 'logits/rejected': -32.4075813293457, 'logits/chosen': -32.209625244140625, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:14<32:55,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.62it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.94it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.81it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.80it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.80it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.80it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.75it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.75it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.1345921754837036, 'eval_runtime': 32.6818, 'eval_samples_per_second': 228.537, 'eval_steps_per_second': 1.805, 'eval_rewards/chosen': -0.19665750861167908, 'eval_rewards/rejected': -0.3186725974082947, 'eval_rewards/accuracies': 0.7357932925224304, 'eval_rewards/margins': 0.1220150962471962, 'eval_logps/rejected': -4.639455318450928, 'eval_logps/chosen': -4.312756061553955, 'eval_logits/rejected': -32.503021240234375, 'eval_logits/chosen': -32.302337646484375, 'epoch': 1.35}\n 45%|█████████████████▌                     | 1500/3327 [28:46<32:55,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.18it/s]\u001b[A\n{'loss': 1.1004, 'grad_norm': 7.3877763748168945, 'learning_rate': 2e-06, 'rewards/chosen': -0.1948939859867096, 'rewards/rejected': -0.3236549496650696, 'rewards/accuracies': 0.7503125071525574, 'rewards/margins': 0.1287609338760376, 'logps/rejected': -4.639935493469238, 'logps/chosen': -4.309456825256348, 'logits/rejected': -32.73188781738281, 'logits/chosen': -32.40843200683594, 'epoch': 1.4}\n{'loss': 1.0852, 'grad_norm': 6.324052333831787, 'learning_rate': 2e-06, 'rewards/chosen': -0.19774198532104492, 'rewards/rejected': -0.3315146565437317, 'rewards/accuracies': 0.75, 'rewards/margins': 0.13377267122268677, 'logps/rejected': -4.676197528839111, 'logps/chosen': -4.301340103149414, 'logits/rejected': -33.164833068847656, 'logits/chosen': -32.83790588378906, 'epoch': 1.44}\n{'loss': 1.0835, 'grad_norm': 6.100252151489258, 'learning_rate': 2e-06, 'rewards/chosen': -0.2021074742078781, 'rewards/rejected': -0.33767232298851013, 'rewards/accuracies': 0.7542187571525574, 'rewards/margins': 0.1355648636817932, 'logps/rejected': -4.6910200119018555, 'logps/chosen': -4.331948757171631, 'logits/rejected': -32.90086364746094, 'logits/chosen': -32.63475036621094, 'epoch': 1.49}\n{'loss': 1.1132, 'grad_norm': 6.230849742889404, 'learning_rate': 2e-06, 'rewards/chosen': -0.2004268318414688, 'rewards/rejected': -0.33037325739860535, 'rewards/accuracies': 0.7443749904632568, 'rewards/margins': 0.12994644045829773, 'logps/rejected': -4.669378757476807, 'logps/chosen': -4.309963703155518, 'logits/rejected': -32.8188591003418, 'logits/chosen': -32.73752975463867, 'epoch': 1.53}\n{'loss': 1.0748, 'grad_norm': 6.643650054931641, 'learning_rate': 2e-06, 'rewards/chosen': -0.2007983922958374, 'rewards/rejected': -0.33666926622390747, 'rewards/accuracies': 0.7646874785423279, 'rewards/margins': 0.13587085902690887, 'logps/rejected': -4.691767692565918, 'logps/chosen': -4.331746578216553, 'logits/rejected': -32.72340393066406, 'logits/chosen': -32.541072845458984, 'epoch': 1.58}\n{'loss': 1.1007, 'grad_norm': 6.1619062423706055, 'learning_rate': 2e-06, 'rewards/chosen': -0.20707933604717255, 'rewards/rejected': -0.34085091948509216, 'rewards/accuracies': 0.7462499737739563, 'rewards/margins': 0.13377156853675842, 'logps/rejected': -4.697808265686035, 'logps/chosen': -4.363134860992432, 'logits/rejected': -32.615325927734375, 'logits/chosen': -32.44398880004883, 'epoch': 1.62}\n{'loss': 1.0961, 'grad_norm': 6.663593292236328, 'learning_rate': 2e-06, 'rewards/chosen': -0.20407260954380035, 'rewards/rejected': -0.33859604597091675, 'rewards/accuracies': 0.7503125071525574, 'rewards/margins': 0.1345233917236328, 'logps/rejected': -4.706974029541016, 'logps/chosen': -4.327063083648682, 'logits/rejected': -32.47429656982422, 'logits/chosen': -32.24998474121094, 'epoch': 1.67}\n{'loss': 1.0964, 'grad_norm': 6.7456889152526855, 'learning_rate': 2e-06, 'rewards/chosen': -0.20665684342384338, 'rewards/rejected': -0.3404284417629242, 'rewards/accuracies': 0.7512500286102295, 'rewards/margins': 0.1337715983390808, 'logps/rejected': -4.700209617614746, 'logps/chosen': -4.356766223907471, 'logits/rejected': -32.82615280151367, 'logits/chosen': -32.750118255615234, 'epoch': 1.71}\n{'loss': 1.1048, 'grad_norm': 5.827284812927246, 'learning_rate': 2e-06, 'rewards/chosen': -0.20356664061546326, 'rewards/rejected': -0.335869699716568, 'rewards/accuracies': 0.745312511920929, 'rewards/margins': 0.13230305910110474, 'logps/rejected': -4.6862406730651855, 'logps/chosen': -4.342454433441162, 'logits/rejected': -32.9290771484375, 'logits/chosen': -33.017723083496094, 'epoch': 1.76}\n{'loss': 1.0853, 'grad_norm': 6.677635669708252, 'learning_rate': 2e-06, 'rewards/chosen': -0.20215407013893127, 'rewards/rejected': -0.33734798431396484, 'rewards/accuracies': 0.7545312643051147, 'rewards/margins': 0.13519389927387238, 'logps/rejected': -4.715076923370361, 'logps/chosen': -4.328102111816406, 'logits/rejected': -33.101688385009766, 'logits/chosen': -32.799137115478516, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [37:49<25:17,  1.14s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.61it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.95it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.83it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.82it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.80it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.81it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.81it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.81it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.81it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.81it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.80it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:14<00:17,  1.80it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.81it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.79it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.75it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.1071197986602783, 'eval_runtime': 32.6747, 'eval_samples_per_second': 228.586, 'eval_steps_per_second': 1.806, 'eval_rewards/chosen': -0.21068094670772552, 'eval_rewards/rejected': -0.3426346480846405, 'eval_rewards/accuracies': 0.7446445822715759, 'eval_rewards/margins': 0.13195374608039856, 'eval_logps/rejected': -4.699360370635986, 'eval_logps/chosen': -4.347815036773682, 'eval_logits/rejected': -33.05653762817383, 'eval_logits/chosen': -32.94581985473633, 'epoch': 1.8}\n 60%|███████████████████████▍               | 2000/3327 [38:22<25:17,  1.14s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.18it/s]\u001b[A\n{'loss': 1.0722, 'grad_norm': 7.213840007781982, 'learning_rate': 2e-06, 'rewards/chosen': -0.20814499258995056, 'rewards/rejected': -0.34896084666252136, 'rewards/accuracies': 0.7614062428474426, 'rewards/margins': 0.140815868973732, 'logps/rejected': -4.6988420486450195, 'logps/chosen': -4.342648506164551, 'logits/rejected': -33.37926483154297, 'logits/chosen': -33.18202209472656, 'epoch': 1.85}\n{'loss': 1.0639, 'grad_norm': 6.610597133636475, 'learning_rate': 2e-06, 'rewards/chosen': -0.2073604166507721, 'rewards/rejected': -0.3491588830947876, 'rewards/accuracies': 0.7573437690734863, 'rewards/margins': 0.1417984813451767, 'logps/rejected': -4.732886791229248, 'logps/chosen': -4.352708339691162, 'logits/rejected': -33.157657623291016, 'logits/chosen': -33.08275604248047, 'epoch': 1.89}\n{'loss': 1.0769, 'grad_norm': 6.635571002960205, 'learning_rate': 2e-06, 'rewards/chosen': -0.21389935910701752, 'rewards/rejected': -0.35261961817741394, 'rewards/accuracies': 0.7562500238418579, 'rewards/margins': 0.13872025907039642, 'logps/rejected': -4.728494167327881, 'logps/chosen': -4.375143527984619, 'logits/rejected': -33.07160568237305, 'logits/chosen': -32.97690963745117, 'epoch': 1.94}\n{'loss': 1.0582, 'grad_norm': 6.417399883270264, 'learning_rate': 2e-06, 'rewards/chosen': -0.21399907767772675, 'rewards/rejected': -0.36008262634277344, 'rewards/accuracies': 0.7601562738418579, 'rewards/margins': 0.1460835188627243, 'logps/rejected': -4.740828514099121, 'logps/chosen': -4.369899749755859, 'logits/rejected': -33.11415100097656, 'logits/chosen': -32.9155158996582, 'epoch': 1.98}\n{'loss': 1.0536, 'grad_norm': 6.134243965148926, 'learning_rate': 2e-06, 'rewards/chosen': -0.2076454609632492, 'rewards/rejected': -0.3540574312210083, 'rewards/accuracies': 0.7678247094154358, 'rewards/margins': 0.1464119851589203, 'logps/rejected': -4.730367183685303, 'logps/chosen': -4.353949546813965, 'logits/rejected': -33.2220344543457, 'logits/chosen': -33.188873291015625, 'epoch': 2.03}\n{'loss': 1.0336, 'grad_norm': 6.100524425506592, 'learning_rate': 2e-06, 'rewards/chosen': -0.2078760713338852, 'rewards/rejected': -0.3560766279697418, 'rewards/accuracies': 0.7698437571525574, 'rewards/margins': 0.14820052683353424, 'logps/rejected': -4.718436241149902, 'logps/chosen': -4.363729476928711, 'logits/rejected': -33.54550552368164, 'logits/chosen': -33.12234115600586, 'epoch': 2.07}\n{'loss': 1.0276, 'grad_norm': 7.02501106262207, 'learning_rate': 2e-06, 'rewards/chosen': -0.20753198862075806, 'rewards/rejected': -0.35776299238204956, 'rewards/accuracies': 0.7739062309265137, 'rewards/margins': 0.1502310335636139, 'logps/rejected': -4.733168125152588, 'logps/chosen': -4.351305961608887, 'logits/rejected': -33.100101470947266, 'logits/chosen': -33.116233825683594, 'epoch': 2.12}\n{'loss': 1.0325, 'grad_norm': 6.0730462074279785, 'learning_rate': 2e-06, 'rewards/chosen': -0.21003007888793945, 'rewards/rejected': -0.3591846525669098, 'rewards/accuracies': 0.7728124856948853, 'rewards/margins': 0.14915457367897034, 'logps/rejected': -4.744936466217041, 'logps/chosen': -4.358800888061523, 'logits/rejected': -33.098968505859375, 'logits/chosen': -33.268001556396484, 'epoch': 2.16}\n{'loss': 1.0292, 'grad_norm': 6.685102462768555, 'learning_rate': 2e-06, 'rewards/chosen': -0.2174166887998581, 'rewards/rejected': -0.36845502257347107, 'rewards/accuracies': 0.7734375, 'rewards/margins': 0.15103831887245178, 'logps/rejected': -4.778358459472656, 'logps/chosen': -4.363469123840332, 'logits/rejected': -33.475833892822266, 'logits/chosen': -33.604244232177734, 'epoch': 2.21}\n{'loss': 0.9936, 'grad_norm': 6.269606113433838, 'learning_rate': 2e-06, 'rewards/chosen': -0.21931537985801697, 'rewards/rejected': -0.3757648169994354, 'rewards/accuracies': 0.7818750143051147, 'rewards/margins': 0.15644943714141846, 'logps/rejected': -4.8108110427856445, 'logps/chosen': -4.389851093292236, 'logits/rejected': -33.64702224731445, 'logits/chosen': -33.508235931396484, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:26<14:57,  1.09s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.56it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.02it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.94it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.83it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.82it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.82it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:19,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.81it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.81it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.80it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.77it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:14,  1.78it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.79it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.80it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.78it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.78it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.79it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:28<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.78it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.0854710340499878, 'eval_runtime': 32.693, 'eval_samples_per_second': 228.459, 'eval_steps_per_second': 1.805, 'eval_rewards/chosen': -0.22987763583660126, 'eval_rewards/rejected': -0.37201833724975586, 'eval_rewards/accuracies': 0.753760576248169, 'eval_rewards/margins': 0.1421407014131546, 'eval_logps/rejected': -4.772819995880127, 'eval_logps/chosen': -4.395806789398193, 'eval_logits/rejected': -33.60525894165039, 'eval_logits/chosen': -33.57010269165039, 'epoch': 2.25}\n 75%|█████████████████████████████▎         | 2500/3327 [47:59<14:57,  1.09s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.18it/s]\u001b[A\n{'loss': 0.9945, 'grad_norm': 6.349998950958252, 'learning_rate': 2e-06, 'rewards/chosen': -0.2261015772819519, 'rewards/rejected': -0.3886004686355591, 'rewards/accuracies': 0.7870312333106995, 'rewards/margins': 0.16249890625476837, 'logps/rejected': -4.814723014831543, 'logps/chosen': -4.4027018547058105, 'logits/rejected': -33.86115264892578, 'logits/chosen': -33.58479309082031, 'epoch': 2.3}\n{'loss': 0.9973, 'grad_norm': 6.721961498260498, 'learning_rate': 2e-06, 'rewards/chosen': -0.22256366908550262, 'rewards/rejected': -0.384453684091568, 'rewards/accuracies': 0.7825000286102295, 'rewards/margins': 0.16189002990722656, 'logps/rejected': -4.826096057891846, 'logps/chosen': -4.376797199249268, 'logits/rejected': -33.82787322998047, 'logits/chosen': -33.766780853271484, 'epoch': 2.34}\n{'loss': 1.0128, 'grad_norm': 6.371404647827148, 'learning_rate': 2e-06, 'rewards/chosen': -0.2301291674375534, 'rewards/rejected': -0.3891184329986572, 'rewards/accuracies': 0.7793750166893005, 'rewards/margins': 0.15898928046226501, 'logps/rejected': -4.839794158935547, 'logps/chosen': -4.401920318603516, 'logits/rejected': -33.726768493652344, 'logits/chosen': -33.671939849853516, 'epoch': 2.39}\n{'loss': 1.0066, 'grad_norm': 7.1012115478515625, 'learning_rate': 2e-06, 'rewards/chosen': -0.23225505650043488, 'rewards/rejected': -0.3928684592247009, 'rewards/accuracies': 0.780468761920929, 'rewards/margins': 0.16061335802078247, 'logps/rejected': -4.8141632080078125, 'logps/chosen': -4.386875152587891, 'logits/rejected': -34.005802154541016, 'logits/chosen': -33.96961975097656, 'epoch': 2.43}\n{'loss': 1.0048, 'grad_norm': 6.488617897033691, 'learning_rate': 2e-06, 'rewards/chosen': -0.23096050322055817, 'rewards/rejected': -0.39232635498046875, 'rewards/accuracies': 0.7828124761581421, 'rewards/margins': 0.16136586666107178, 'logps/rejected': -4.819551944732666, 'logps/chosen': -4.391541957855225, 'logits/rejected': -34.507049560546875, 'logits/chosen': -34.469974517822266, 'epoch': 2.48}\n{'loss': 1.0198, 'grad_norm': 6.608018398284912, 'learning_rate': 2e-06, 'rewards/chosen': -0.23541724681854248, 'rewards/rejected': -0.3902861475944519, 'rewards/accuracies': 0.7760937213897705, 'rewards/margins': 0.1548689305782318, 'logps/rejected': -4.819410800933838, 'logps/chosen': -4.423556327819824, 'logits/rejected': -34.618194580078125, 'logits/chosen': -34.593971252441406, 'epoch': 2.52}\n{'loss': 1.0218, 'grad_norm': 7.048628807067871, 'learning_rate': 2e-06, 'rewards/chosen': -0.22937680780887604, 'rewards/rejected': -0.38502371311187744, 'rewards/accuracies': 0.7731249928474426, 'rewards/margins': 0.15564695000648499, 'logps/rejected': -4.816045761108398, 'logps/chosen': -4.399548053741455, 'logits/rejected': -34.81236267089844, 'logits/chosen': -34.936397552490234, 'epoch': 2.57}\n{'loss': 0.9826, 'grad_norm': 7.5032219886779785, 'learning_rate': 2e-06, 'rewards/chosen': -0.229564368724823, 'rewards/rejected': -0.3940567076206207, 'rewards/accuracies': 0.7873437404632568, 'rewards/margins': 0.16449233889579773, 'logps/rejected': -4.821444511413574, 'logps/chosen': -4.396182060241699, 'logits/rejected': -34.99010467529297, 'logits/chosen': -34.789459228515625, 'epoch': 2.61}\n{'loss': 1.0004, 'grad_norm': 7.348494529724121, 'learning_rate': 2e-06, 'rewards/chosen': -0.23324142396450043, 'rewards/rejected': -0.3940543830394745, 'rewards/accuracies': 0.7839062213897705, 'rewards/margins': 0.16081298887729645, 'logps/rejected': -4.838388919830322, 'logps/chosen': -4.412349224090576, 'logits/rejected': -34.9981803894043, 'logits/chosen': -34.81827163696289, 'epoch': 2.66}\n{'loss': 0.9802, 'grad_norm': 6.966770648956299, 'learning_rate': 2e-06, 'rewards/chosen': -0.2385445088148117, 'rewards/rejected': -0.40340277552604675, 'rewards/accuracies': 0.7889062762260437, 'rewards/margins': 0.16485825181007385, 'logps/rejected': -4.869218349456787, 'logps/chosen': -4.414480209350586, 'logits/rejected': -34.92680358886719, 'logits/chosen': -34.96626281738281, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [57:02<05:53,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                          | 2/59 [00:00<00:15,  3.63it/s]\u001b[A\n  5%|██▏                                         | 3/59 [00:01<00:21,  2.55it/s]\u001b[A\n  7%|██▉                                         | 4/59 [00:01<00:25,  2.16it/s]\u001b[A\n  8%|███▋                                        | 5/59 [00:02<00:26,  2.03it/s]\u001b[A\n 10%|████▍                                       | 6/59 [00:02<00:27,  1.94it/s]\u001b[A\n 12%|█████▏                                      | 7/59 [00:03<00:27,  1.90it/s]\u001b[A\n 14%|█████▉                                      | 8/59 [00:03<00:27,  1.87it/s]\u001b[A\n 15%|██████▋                                     | 9/59 [00:04<00:27,  1.85it/s]\u001b[A\n 17%|███████▎                                   | 10/59 [00:05<00:26,  1.84it/s]\u001b[A\n 19%|████████                                   | 11/59 [00:05<00:26,  1.82it/s]\u001b[A\n 20%|████████▋                                  | 12/59 [00:06<00:25,  1.81it/s]\u001b[A\n 22%|█████████▍                                 | 13/59 [00:06<00:25,  1.81it/s]\u001b[A\n 24%|██████████▏                                | 14/59 [00:07<00:25,  1.78it/s]\u001b[A\n 25%|██████████▉                                | 15/59 [00:07<00:24,  1.79it/s]\u001b[A\n 27%|███████████▋                               | 16/59 [00:08<00:23,  1.79it/s]\u001b[A\n 29%|████████████▍                              | 17/59 [00:08<00:23,  1.80it/s]\u001b[A\n 31%|█████████████                              | 18/59 [00:09<00:22,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 19/59 [00:10<00:22,  1.80it/s]\u001b[A\n 34%|██████████████▌                            | 20/59 [00:10<00:21,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 21/59 [00:11<00:21,  1.80it/s]\u001b[A\n 37%|████████████████                           | 22/59 [00:11<00:20,  1.80it/s]\u001b[A\n 39%|████████████████▊                          | 23/59 [00:12<00:20,  1.80it/s]\u001b[A\n 41%|█████████████████▍                         | 24/59 [00:12<00:19,  1.80it/s]\u001b[A\n 42%|██████████████████▏                        | 25/59 [00:13<00:18,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 26/59 [00:13<00:18,  1.80it/s]\u001b[A\n 46%|███████████████████▋                       | 27/59 [00:14<00:17,  1.80it/s]\u001b[A\n 47%|████████████████████▍                      | 28/59 [00:15<00:17,  1.81it/s]\u001b[A\n 49%|█████████████████████▏                     | 29/59 [00:15<00:16,  1.81it/s]\u001b[A\n 51%|█████████████████████▊                     | 30/59 [00:16<00:16,  1.80it/s]\u001b[A\n 53%|██████████████████████▌                    | 31/59 [00:16<00:15,  1.80it/s]\u001b[A\n 54%|███████████████████████▎                   | 32/59 [00:17<00:14,  1.81it/s]\u001b[A\n 56%|████████████████████████                   | 33/59 [00:17<00:14,  1.78it/s]\u001b[A\n 58%|████████████████████████▊                  | 34/59 [00:18<00:13,  1.79it/s]\u001b[A\n 59%|█████████████████████████▌                 | 35/59 [00:18<00:13,  1.79it/s]\u001b[A\n 61%|██████████████████████████▏                | 36/59 [00:19<00:12,  1.80it/s]\u001b[A\n 63%|██████████████████████████▉                | 37/59 [00:20<00:12,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 38/59 [00:20<00:11,  1.80it/s]\u001b[A\n 66%|████████████████████████████▍              | 39/59 [00:21<00:11,  1.80it/s]\u001b[A\n 68%|█████████████████████████████▏             | 40/59 [00:21<00:10,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▉             | 41/59 [00:22<00:10,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▌            | 42/59 [00:22<00:09,  1.79it/s]\u001b[A\n 73%|███████████████████████████████▎           | 43/59 [00:23<00:08,  1.79it/s]\u001b[A\n 75%|████████████████████████████████           | 44/59 [00:23<00:08,  1.80it/s]\u001b[A\n 76%|████████████████████████████████▊          | 45/59 [00:24<00:07,  1.77it/s]\u001b[A\n 78%|█████████████████████████████████▌         | 46/59 [00:25<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 47/59 [00:25<00:06,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 48/59 [00:26<00:06,  1.76it/s]\u001b[A\n 83%|███████████████████████████████████▋       | 49/59 [00:26<00:05,  1.77it/s]\u001b[A\n 85%|████████████████████████████████████▍      | 50/59 [00:27<00:05,  1.78it/s]\u001b[A\n 86%|█████████████████████████████████████▏     | 51/59 [00:27<00:04,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 52/59 [00:28<00:03,  1.80it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 53/59 [00:29<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▎   | 54/59 [00:29<00:02,  1.77it/s]\u001b[A\n 93%|████████████████████████████████████████   | 55/59 [00:30<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 56/59 [00:30<00:01,  1.75it/s]\u001b[A\n 97%|█████████████████████████████████████████▌ | 57/59 [00:31<00:01,  1.74it/s]\u001b[A\n 98%|██████████████████████████████████████████▎| 58/59 [00:31<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.0687332153320312, 'eval_runtime': 32.6972, 'eval_samples_per_second': 228.429, 'eval_steps_per_second': 1.804, 'eval_rewards/chosen': -0.24721567332744598, 'eval_rewards/rejected': -0.3971201479434967, 'eval_rewards/accuracies': 0.7589247822761536, 'eval_rewards/margins': 0.14990445971488953, 'eval_logps/rejected': -4.835574626922607, 'eval_logps/chosen': -4.439152240753174, 'eval_logits/rejected': -34.76693344116211, 'eval_logits/chosen': -34.78671646118164, 'epoch': 2.71}\n 90%|███████████████████████████████████▏   | 3000/3327 [57:34<05:53,  1.08s/it]\n100%|███████████████████████████████████████████| 59/59 [00:32<00:00,  2.19it/s]\u001b[A\n{'loss': 1.0026, 'grad_norm': 6.7472147941589355, 'learning_rate': 2e-06, 'rewards/chosen': -0.23933716118335724, 'rewards/rejected': -0.4014037847518921, 'rewards/accuracies': 0.7867187261581421, 'rewards/margins': 0.16206665337085724, 'logps/rejected': -4.8491058349609375, 'logps/chosen': -4.408302307128906, 'logits/rejected': -35.03540802001953, 'logits/chosen': -35.061187744140625, 'epoch': 2.75}\n{'loss': 0.9851, 'grad_norm': 6.794745922088623, 'learning_rate': 2e-06, 'rewards/chosen': -0.23545341193675995, 'rewards/rejected': -0.4009670317173004, 'rewards/accuracies': 0.7837499976158142, 'rewards/margins': 0.16551363468170166, 'logps/rejected': -4.847218990325928, 'logps/chosen': -4.418150424957275, 'logits/rejected': -34.76899719238281, 'logits/chosen': -34.65069580078125, 'epoch': 2.8}\n{'loss': 0.9965, 'grad_norm': 6.498790740966797, 'learning_rate': 2e-06, 'rewards/chosen': -0.23926058411598206, 'rewards/rejected': -0.40381932258605957, 'rewards/accuracies': 0.7829687595367432, 'rewards/margins': 0.16455870866775513, 'logps/rejected': -4.869410514831543, 'logps/chosen': -4.437243461608887, 'logits/rejected': -35.133514404296875, 'logits/chosen': -35.013362884521484, 'epoch': 2.84}\n{'loss': 0.9853, 'grad_norm': 6.480871200561523, 'learning_rate': 2e-06, 'rewards/chosen': -0.23744384944438934, 'rewards/rejected': -0.4034779369831085, 'rewards/accuracies': 0.7834374904632568, 'rewards/margins': 0.16603410243988037, 'logps/rejected': -4.845790863037109, 'logps/chosen': -4.432608127593994, 'logits/rejected': -34.848854064941406, 'logits/chosen': -34.71460723876953, 'epoch': 2.89}\n{'loss': 1.0016, 'grad_norm': 6.819617748260498, 'learning_rate': 2e-06, 'rewards/chosen': -0.23818454146385193, 'rewards/rejected': -0.40297073125839233, 'rewards/accuracies': 0.7776562571525574, 'rewards/margins': 0.1647862195968628, 'logps/rejected': -4.8297529220581055, 'logps/chosen': -4.421870708465576, 'logits/rejected': -34.915714263916016, 'logits/chosen': -34.86787033081055, 'epoch': 2.93}\n{'loss': 0.9883, 'grad_norm': 6.597874641418457, 'learning_rate': 2e-06, 'rewards/chosen': -0.2385484129190445, 'rewards/rejected': -0.40382203459739685, 'rewards/accuracies': 0.7798437476158142, 'rewards/margins': 0.16527357697486877, 'logps/rejected': -4.865983486175537, 'logps/chosen': -4.423307418823242, 'logits/rejected': -35.477691650390625, 'logits/chosen': -35.48833084106445, 'epoch': 2.98}\n{'train_runtime': 3827.8168, 'train_samples_per_second': 111.213, 'train_steps_per_second': 0.869, 'train_loss': 1.136828659603393, 'epoch': 3.0}\n100%|█████████████████████████████████████| 3327/3327 [1:03:31<00:00,  1.15s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-3327\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-2218\n!python ppo.py --exp_name=eval --log_with=wandb --eval_model=./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-1109","metadata":{"execution":{"iopub.status.busy":"2024-05-24T05:33:21.973315Z","iopub.execute_input":"2024-05-24T05:33:21.973725Z","iopub.status.idle":"2024-05-24T05:47:50.296642Z","shell.execute_reply.started":"2024-05-24T05:33:21.973679Z","shell.execute_reply":"2024-05-24T05:47:50.295604Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2024-05-24 05:33:28.357184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 05:33:28.357233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 05:33:28.358604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-3327', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.05.33'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-3327\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_053340-rcisjyf7\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.05.33\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/rcisjyf7\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:26<03:35, 26.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:53<03:07, 26.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:20<02:39, 26.66s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:46<02:13, 26.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:13<01:46, 26.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:40<01:20, 26.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:06<00:53, 26.68s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:33<00:26, 26.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:00<00:00, 26.74s/it]\nmean test reward 0.9309203574581488 +/- 0.0052017678727126915 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9988538026809692 from 0.00922983093187213\nmean KL 1.521124797174707 +/- 0.13529681040594174 full 10.438205039956504 +/- 0.05462096685189217\nmedian KL 2.3695101737976074 full 10.105747699737549\n2024-05-24 05:38:13.747000: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 05:38:13.747053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 05:38:13.748410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-2218', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.05.38'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-2218\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_053828-gtxj5ycv\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.05.38\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/gtxj5ycv\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:27<03:36, 27.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:53<03:07, 26.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:20<02:40, 26.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:47<02:14, 26.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:14<01:47, 26.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:41<01:20, 26.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:07<00:53, 26.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:34<00:26, 26.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:01<00:00, 26.87s/it]\nmean test reward 0.9157944924236922 +/- 0.005707623421324272 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9988377094268799 from 0.00922983093187213\nmean KL 2.4117763757240027 +/- 0.11903731853660854 full 9.133908377960324 +/- 0.051905856141545935\nmedian KL 3.22012996673584 full 8.720876693725586\n2024-05-24 05:43:03.073400: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-24 05:43:03.073456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-24 05:43:03.074893: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', dry_run=False, eval_model='./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-1109', seed=0, log_with='wandb', task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.24.05.43'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./test_ipo_0_4_lr2e_6-2024.05.24.04.22/checkpoint-1109\n****** hi ******\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240524_054319-hh25o9w2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meval-2024.05.24.05.43\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/cs234/runs/hh25o9w2\u001b[0m\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|█████                                        | 1/9 [00:27<03:39, 27.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|██████████                                   | 2/9 [00:54<03:10, 27.16s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|███████████████                              | 3/9 [01:21<02:42, 27.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|████████████████████                         | 4/9 [01:48<02:15, 27.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|█████████████████████████                    | 5/9 [02:15<01:48, 27.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 238, in <module>\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|██████████████████████████████               | 6/9 [02:42<01:21, 27.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|███████████████████████████████████          | 7/9 [03:09<00:54, 27.11s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|████████████████████████████████████████     | 8/9 [03:36<00:27, 27.08s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|█████████████████████████████████████████████| 9/9 [04:03<00:00, 27.10s/it]\nmean test reward 0.8979132248365305 +/- 0.006217516381308352 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9988201260566711 from 0.00922983093187213\nmean KL 3.2950041798434944 +/- 0.11032650063995615 full 7.773983780605097 +/- 0.04990733724585774\nmedian KL 4.006210088729858 full 7.391429901123047\n","output_type":"stream"}]}]}