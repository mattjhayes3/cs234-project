{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !conda install -y gdown\n# print('hi')\n!git clone https://github.com/mattjhayes3/cs234-project.git\n%cd /kaggle/working/cs234-project\n!git pull --rebase\n!pip install wandb\nimport wandb\n\nwandb.login(key=\"KEY\")\n# wandb.init()\n\nfrom huggingface_hub import notebook_login,login\n\n# notebook_login(\"KEY\")\nlogin(\"KEY\")\n\n# !gdown --id 1TTg8s_dj60EKl4No2unSvYmMyMopPVJ6\n# !gdown --id 1Z7HvAokBQu65jew4ou2DjOYRi23OrJdr\n!mkdir results\n!pip install datasets>=1.17.0 torch>=1.4.0 tqdm transformers accelerate peft>=0.3.0 tyro>=0.5.7\n!pip install git+https://github.com/mattjhayes3/trl.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T21:53:03.943034Z","iopub.execute_input":"2024-05-31T21:53:03.943685Z","iopub.status.idle":"2024-05-31T21:54:11.016182Z","shell.execute_reply.started":"2024-05-31T21:53:03.943650Z","shell.execute_reply":"2024-05-31T21:54:11.014973Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'cs234-project' already exists and is not an empty directory.\n/kaggle/working/cs234-project\nAlready up to date.\nCurrent branch main is up to date.\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nmkdir: cannot create directory 'results': File exists\nCollecting git+https://github.com/mattjhayes3/trl.git\n  Cloning https://github.com/mattjhayes3/trl.git to /tmp/pip-req-build-c3y7s92f\n  Running command git clone --filter=blob:none --quiet https://github.com/mattjhayes3/trl.git /tmp/pip-req-build-c3y7s92f\n  Resolved https://github.com/mattjhayes3/trl.git to commit b6bd36541159b8ce340f2a978a7837e99b9a6b29\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (4.39.3)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.29.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (2.18.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.7.dev0) (0.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.7.dev0) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.22.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.7.dev0) (4.66.1)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (13.7.0)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.7.dev0) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.8.7.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.8.7.dev0) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.7.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.7.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.7.dev0) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.7.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.7.dev0) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.7.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.7.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.7.dev0) (1.16.0)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.8.7.dev0-py3-none-any.whl size=209552 sha256=6ec9254e4fbfe78c55f8b459baf10d9332e70a6895d8504ba2aa82070b81de3c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-p_e4ewvu/wheels/b5/f5/74/f982e27bdb1c23b205f8bfcaaed174cf3d2c06bd1a5f5926cc\nSuccessfully built trl\nInstalling collected packages: trl\nSuccessfully installed trl-0.8.7.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --rebase","metadata":{"execution":{"iopub.status.busy":"2024-05-31T12:56:07.278512Z","iopub.execute_input":"2024-05-31T12:56:07.279380Z","iopub.status.idle":"2024-05-31T12:56:08.365848Z","shell.execute_reply.started":"2024-05-31T12:56:07.279346Z","shell.execute_reply":"2024-05-31T12:56:08.364814Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Already up to date.\nCurrent branch main is up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git rm --cached -r .\n!git reset --hard\n!git rm .gitattributes\n!git reset .\n!git checkout .\n!git stash && git pull --rebase","metadata":{"execution":{"iopub.status.busy":"2024-05-31T12:55:24.031137Z","iopub.execute_input":"2024-05-31T12:55:24.031992Z","iopub.status.idle":"2024-05-31T12:55:38.123848Z","shell.execute_reply.started":"2024-05-31T12:55:24.031948Z","shell.execute_reply":"2024-05-31T12:55:38.122794Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"rm '.gitattributes'\nrm '.gitignore'\nrm '.gitmodules'\nrm '__init__.py'\nrm 'dpo.py'\nrm 'gen_pref_pairs.py'\nrm 'gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/.DS_Store'\nrm 'gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/dataset_dict.json'\nrm 'gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/data-00000-of-00001.arrow'\nrm 'gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/dataset_info.json'\nrm 'gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/state.json'\nrm 'gpt2-imdb-16-token-pref-pairs-3-choose-2-div-3_tokenized/dataset_dict.json'\nrm 'gpt2-imdb-16-token-pref-pairs-3-choose-2-div-3_tokenized/train/data-00000-of-00001.arrow'\nrm 'gpt2-imdb-16-token-pref-pairs-3-choose-2-div-3_tokenized/train/dataset_info.json'\nrm 'gpt2-imdb-16-token-pref-pairs-3-choose-2-div-3_tokenized/train/state.json'\nrm 'gpt2-imdb-16-token-pref-pairs-3-distinct-pairs_tokenized/dataset_dict.json'\nrm 'gpt2-imdb-16-token-pref-pairs-3-distinct-pairs_tokenized/train/data-00000-of-00001.arrow'\nrm 'gpt2-imdb-16-token-pref-pairs-3-distinct-pairs_tokenized/train/dataset_info.json'\nrm 'gpt2-imdb-16-token-pref-pairs-3-distinct-pairs_tokenized/train/state.json'\nrm 'gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/dataset_dict.json'\nrm 'gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/cache-13e54457644d5a49.arrow'\nrm 'gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/cache-33e29b9c6ccd8398.arrow'\nrm 'gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/data-00000-of-00001.arrow'\nrm 'gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/dataset_info.json'\nrm 'gpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/state.json'\nrm 'ppo.py'\nrm 'pref_pairs_16_token_tokenized_split/data-00000-of-00002.arrow'\nrm 'pref_pairs_16_token_tokenized_split/data-00001-of-00002.arrow'\nrm 'pref_pairs_16_token_tokenized_split/dataset_info.json'\nrm 'pref_pairs_16_token_tokenized_split/state.json'\nrm 'trl'\nrm 'wandb/latest-run'\nrm 'wandb/run-20240518_231328-3tf8l8r7/files/config.yaml'\nrm 'wandb/run-20240518_231328-3tf8l8r7/files/media/table/game_log_0_0315135c0cbd7e4a2159.table.json'\nrm 'wandb/run-20240518_231328-3tf8l8r7/files/requirements.txt'\nrm 'wandb/run-20240518_231328-3tf8l8r7/files/wandb-metadata.json'\nrm 'wandb/run-20240518_231328-3tf8l8r7/files/wandb-summary.json'\nrm 'wandb/run-20240518_231328-3tf8l8r7/run-3tf8l8r7.wandb'\nrm 'wandb/run-20240518_232934-yej431s5/files/config.yaml'\nrm 'wandb/run-20240518_232934-yej431s5/files/media/table/game_log_0_b978061da43c5b2110ba.table.json'\nrm 'wandb/run-20240518_232934-yej431s5/files/requirements.txt'\nrm 'wandb/run-20240518_232934-yej431s5/files/wandb-metadata.json'\nrm 'wandb/run-20240518_232934-yej431s5/files/wandb-summary.json'\nrm 'wandb/run-20240518_232934-yej431s5/run-yej431s5.wandb'\nrm 'wandb/run-20240518_233640-ti3nodte/files/config.yaml'\nrm 'wandb/run-20240518_233640-ti3nodte/files/media/table/game_log_0_1a5767e1ad7c44af05c8.table.json'\nrm 'wandb/run-20240518_233640-ti3nodte/files/requirements.txt'\nrm 'wandb/run-20240518_233640-ti3nodte/files/wandb-metadata.json'\nrm 'wandb/run-20240518_233640-ti3nodte/files/wandb-summary.json'\nrm 'wandb/run-20240518_233640-ti3nodte/run-ti3nodte.wandb'\nrm 'wandb/run-20240519_024512-1wprcjoj/files/config.yaml'\nrm 'wandb/run-20240519_024512-1wprcjoj/files/requirements.txt'\nrm 'wandb/run-20240519_024512-1wprcjoj/files/wandb-metadata.json'\nrm 'wandb/run-20240519_024512-1wprcjoj/files/wandb-summary.json'\nrm 'wandb/run-20240519_024512-1wprcjoj/run-1wprcjoj.wandb'\nrm 'wandb/run-20240519_024626-1m51eeeu/files/config.yaml'\nrm 'wandb/run-20240519_024626-1m51eeeu/files/requirements.txt'\nrm 'wandb/run-20240519_024626-1m51eeeu/files/wandb-metadata.json'\nrm 'wandb/run-20240519_024626-1m51eeeu/files/wandb-summary.json'\nrm 'wandb/run-20240519_024626-1m51eeeu/run-1m51eeeu.wandb'\nrm 'wandb/run-20240519_024741-2gocnabz/files/config.yaml'\nrm 'wandb/run-20240519_024741-2gocnabz/files/requirements.txt'\nrm 'wandb/run-20240519_024741-2gocnabz/files/wandb-metadata.json'\nrm 'wandb/run-20240519_024741-2gocnabz/files/wandb-summary.json'\nrm 'wandb/run-20240519_024741-2gocnabz/run-2gocnabz.wandb'\nUpdating files: 100% (65/65), done.\nHEAD is now at 832656a incorp eval into dpo.py\nEncountered 8 file(s) that should have been pointers, but weren't:\n\tgpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/data-00000-of-00001.arrow\n\tgpt2-imdb-16-token-pref-pairs-3-choose-2-div-3_tokenized/train/data-00000-of-00001.arrow\n\tgpt2-imdb-16-token-pref-pairs-3-distinct-pairs_tokenized/train/data-00000-of-00001.arrow\n\tgpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/cache-13e54457644d5a49.arrow\n\tgpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/cache-33e29b9c6ccd8398.arrow\n\tgpt2-imdb-16-token-pref-pairs-4-choose-2-div-6_tokenized/train/data-00000-of-00001.arrow\n\tpref_pairs_16_token_tokenized_split/data-00000-of-00002.arrow\n\tpref_pairs_16_token_tokenized_split/data-00001-of-00002.arrow\nrm '.gitattributes'\nUnstaged changes after reset:\nD\t.gitattributes\nUpdated 1 path from the index\n\u001b[33mhint: The '.git/hooks/post-checkout' hook was ignored because it's not set as executable.\u001b[m\n\u001b[33mhint: You can disable this warning with `git config advice.ignoredHook false`.\u001b[m\nNo local changes to save\nremote: Enumerating objects: 84, done.\u001b[K\nremote: Counting objects: 100% (39/39), done.\u001b[K\nremote: Compressing objects: 100% (11/11), done.\u001b[K\nremote: Total 84 (delta 31), reused 35 (delta 28), pack-reused 45\u001b[K\nUnpacking objects: 100% (84/84), 11.61 MiB | 4.35 MiB/s, done.\nFrom https://github.com/mattjhayes3/cs234-project\n   832656a..d2ea544  main       -> origin/main\nUpdating 832656a..d2ea544\nFast-forward\n .gitattributes                                     |     2 \u001b[31m-\u001b[m\n .gitignore                                         |     3 \u001b[32m+\u001b[m\n dpo.py                                             |    24 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n gen_pref_pairs.py                                  |     4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n .../.DS_Store                                      |   Bin \u001b[31m6148\u001b[m -> \u001b[32m0\u001b[m bytes\n ppo.py                                             |    41 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n pref_pairs_16_token_2_choose_2.csv                 | 24913 \u001b[32m+++++++\u001b[m\n pref_pairs_16_token_2_choose_2_div_216.csv         |   692 \u001b[32m+\u001b[m\n .../cache-5cffb068bc8cb725.arrow                   |   Bin \u001b[31m0\u001b[m -> \u001b[32m728\u001b[m bytes\n .../cache-b1a6823bc7870d23.arrow                   |   Bin \u001b[31m0\u001b[m -> \u001b[32m5696\u001b[m bytes\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m877592\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n pref_pairs_16_token_2_choose_2_div_36.csv          |  4152 \u001b[32m++\u001b[m\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m5264904\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n pref_pairs_16_token_3_choose_2.csv                 | 74728 \u001b[32m+++++++++++++++++++\u001b[m\n pref_pairs_16_token_3_choose_2_div_3.csv           | 24902 \u001b[32m++++++\u001b[m\n pref_pairs_16_token_3_distinct_pairs.csv           | 24916 \u001b[32m+++++++\u001b[m\n pref_pairs_16_token_4_choose_2_div_216.csv         |   692 \u001b[32m+\u001b[m\n .../cache-64ce7e16beb3f215.arrow                   |   Bin \u001b[31m0\u001b[m -> \u001b[32m728\u001b[m bytes\n .../cache-f115feeb0f9d93ca.arrow                   |   Bin \u001b[31m0\u001b[m -> \u001b[32m5696\u001b[m bytes\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m867464\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n pref_pairs_16_token_4_choose_2_div_36.csv          |  4150 \u001b[32m+\u001b[m\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m5250032\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n pref_pairs_16_token_4_choose_2_div_6.csv           | 24908 \u001b[32m++++++\u001b[m\n pref_pairs_16_token_6_choose_2_div_216.csv         |   692 \u001b[32m+\u001b[m\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m867400\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n pref_pairs_16_token_6_choose_2_div_36.csv          |  4150 \u001b[32m+\u001b[m\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m5210800\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n pref_pairs_16_token_6_choose_2_div_6.csv           | 24916 \u001b[32m+++++++\u001b[m\n .../data-00000-of-00001.arrow                      |   Bin \u001b[31m0\u001b[m -> \u001b[32m31429384\u001b[m bytes\n .../dataset_info.json                              |   114 \u001b[32m+\u001b[m\n .../state.json                                     |    13 \u001b[32m+\u001b[m\n trl                                                |     2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n wandb/latest-run                                   |     1 \u001b[31m-\u001b[m\n 45 files changed, 214746 insertions(+), 31 deletions(-)\n delete mode 100644 .gitattributes\n delete mode 100644 gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/.DS_Store\n create mode 100644 pref_pairs_16_token_2_choose_2.csv\n create mode 100644 pref_pairs_16_token_2_choose_2_div_216.csv\n create mode 100644 pref_pairs_16_token_2_choose_2_div_216_tokenized/cache-5cffb068bc8cb725.arrow\n create mode 100644 pref_pairs_16_token_2_choose_2_div_216_tokenized/cache-b1a6823bc7870d23.arrow\n create mode 100644 pref_pairs_16_token_2_choose_2_div_216_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_2_choose_2_div_216_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_2_choose_2_div_216_tokenized/state.json\n create mode 100644 pref_pairs_16_token_2_choose_2_div_36.csv\n create mode 100644 pref_pairs_16_token_2_choose_2_div_36_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_2_choose_2_div_36_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_2_choose_2_div_36_tokenized/state.json\n create mode 100644 pref_pairs_16_token_3_choose_2.csv\n create mode 100644 pref_pairs_16_token_3_choose_2_div_3.csv\n create mode 100644 pref_pairs_16_token_3_distinct_pairs.csv\n create mode 100644 pref_pairs_16_token_4_choose_2_div_216.csv\n create mode 100644 pref_pairs_16_token_4_choose_2_div_216_tokenized/cache-64ce7e16beb3f215.arrow\n create mode 100644 pref_pairs_16_token_4_choose_2_div_216_tokenized/cache-f115feeb0f9d93ca.arrow\n create mode 100644 pref_pairs_16_token_4_choose_2_div_216_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_4_choose_2_div_216_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_4_choose_2_div_216_tokenized/state.json\n create mode 100644 pref_pairs_16_token_4_choose_2_div_36.csv\n create mode 100644 pref_pairs_16_token_4_choose_2_div_36_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_4_choose_2_div_36_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_4_choose_2_div_36_tokenized/state.json\n create mode 100644 pref_pairs_16_token_4_choose_2_div_6.csv\n create mode 100644 pref_pairs_16_token_6_choose_2_div_216.csv\n create mode 100644 pref_pairs_16_token_6_choose_2_div_216_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_6_choose_2_div_216_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_6_choose_2_div_216_tokenized/state.json\n create mode 100644 pref_pairs_16_token_6_choose_2_div_36.csv\n create mode 100644 pref_pairs_16_token_6_choose_2_div_36_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_6_choose_2_div_36_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_6_choose_2_div_36_tokenized/state.json\n create mode 100644 pref_pairs_16_token_6_choose_2_div_6.csv\n create mode 100644 pref_pairs_16_token_6_choose_2_div_6_tokenized/data-00000-of-00001.arrow\n create mode 100644 pref_pairs_16_token_6_choose_2_div_6_tokenized/dataset_info.json\n create mode 100644 pref_pairs_16_token_6_choose_2_div_6_tokenized/state.json\n delete mode 120000 wandb/latest-run\n\u001b[33mhint: The '.git/hooks/post-merge' hook was ignored because it's not set as executable.\u001b[m\n\u001b[33mhint: You can disable this warning with `git config advice.ignoredHook false`.\u001b[m\nCurrent branch main is up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"! python dpo.py  --output_dir=dpo_4c2_div6-0_1  --beta=0.1 --dataset_name=/kaggle/working/cs234-project/gpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 150     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=100 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=4","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:23:38.138104Z","iopub.execute_input":"2024-05-29T09:23:38.138797Z","iopub.status.idle":"2024-05-29T09:38:25.183203Z","shell.execute_reply.started":"2024-05-29T09:23:38.138761Z","shell.execute_reply":"2024-05-29T09:38:25.182046Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2024-05-29 09:23:49.349762: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 09:23:49.349872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 09:23:49.496714: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 3.21MB/s]\npytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:01<00:00, 290MB/s]\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.0/17.0 [00:00<00:00, 83.7kB/s]\nvocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 7.17MB/s]\nmerges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 3.79MB/s]\nspecial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 399kB/s]\nds len 24895\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240529_092405-aa2vv9bu\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_4c2_div6-0_1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/aa2vv9bu\u001b[0m\n  0%|                                                   | 0/740 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 2.319899797439575, 'learning_rate': 1.3333333333333334e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -64.06637573242188, 'logps/chosen': -59.531044006347656, 'logits/rejected': -35.743865966796875, 'logits/chosen': -35.82927703857422, 'epoch': 0.01}\n{'loss': 0.6923, 'grad_norm': 2.2992494106292725, 'learning_rate': 6.666666666666666e-07, 'rewards/chosen': 0.0007271564682014287, 'rewards/rejected': -0.0009204179514199495, 'rewards/accuracies': 0.6661351919174194, 'rewards/margins': 0.0016475742449983954, 'logps/rejected': -60.48038864135742, 'logps/chosen': -60.40835189819336, 'logits/rejected': -37.338314056396484, 'logits/chosen': -36.41584396362305, 'epoch': 0.27}\n{'loss': 0.6874, 'grad_norm': 2.1247618198394775, 'learning_rate': 1.3333333333333332e-06, 'rewards/chosen': 0.003657386638224125, 'rewards/rejected': -0.00799325667321682, 'rewards/accuracies': 0.6845312714576721, 'rewards/margins': 0.011650643311440945, 'logps/rejected': -60.70516586303711, 'logps/chosen': -59.95478057861328, 'logits/rejected': -37.2431526184082, 'logits/chosen': -36.43523406982422, 'epoch': 0.54}\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 100/740 [01:48<11:25,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.67it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6843472719192505, 'eval_runtime': 5.3697, 'eval_samples_per_second': 231.857, 'eval_steps_per_second': 1.862, 'eval_rewards/chosen': 0.005438908934593201, 'eval_rewards/rejected': -0.012675915844738483, 'eval_rewards/accuracies': 0.6816112399101257, 'eval_rewards/margins': 0.01811482384800911, 'eval_logps/rejected': -61.423072814941406, 'eval_logps/chosen': -59.95650100708008, 'eval_logits/rejected': -36.89436340332031, 'eval_logits/chosen': -35.70960235595703, 'epoch': 0.54}\n 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 100/740 [01:53<11:25,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\n{'loss': 0.6781, 'grad_norm': 2.225443124771118, 'learning_rate': 2e-06, 'rewards/chosen': 0.003017422743141651, 'rewards/rejected': -0.028336448594927788, 'rewards/accuracies': 0.6898437738418579, 'rewards/margins': 0.031353868544101715, 'logps/rejected': -60.78423309326172, 'logps/chosen': -60.178924560546875, 'logits/rejected': -36.98976135253906, 'logits/chosen': -35.850181579589844, 'epoch': 0.81}\n{'loss': 0.6616, 'grad_norm': 2.1903529167175293, 'learning_rate': 2e-06, 'rewards/chosen': -0.012484990060329437, 'rewards/rejected': -0.08060387521982193, 'rewards/accuracies': 0.7033258676528931, 'rewards/margins': 0.0681188702583313, 'logps/rejected': -61.408538818359375, 'logps/chosen': -60.00685501098633, 'logits/rejected': -36.768306732177734, 'logits/chosen': -36.0030632019043, 'epoch': 1.08}\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 200/740 [03:42<09:40,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.02it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.87it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.661907434463501, 'eval_runtime': 5.3748, 'eval_samples_per_second': 231.637, 'eval_steps_per_second': 1.861, 'eval_rewards/chosen': -0.02921668253839016, 'eval_rewards/rejected': -0.09896187484264374, 'eval_rewards/accuracies': 0.6809307932853699, 'eval_rewards/margins': 0.06974518299102783, 'eval_logps/rejected': -62.28593063354492, 'eval_logps/chosen': -60.303062438964844, 'eval_logits/rejected': -36.216766357421875, 'eval_logits/chosen': -35.11933898925781, 'epoch': 1.08}\n 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 200/740 [03:48<09:40,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\n{'loss': 0.6431, 'grad_norm': 2.0736448764801025, 'learning_rate': 2e-06, 'rewards/chosen': -0.04443327337503433, 'rewards/rejected': -0.15664340555667877, 'rewards/accuracies': 0.7250000238418579, 'rewards/margins': 0.11221010982990265, 'logps/rejected': -61.9844970703125, 'logps/chosen': -60.40010070800781, 'logits/rejected': -36.29380416870117, 'logits/chosen': -35.5667610168457, 'epoch': 1.35}\n{'loss': 0.6313, 'grad_norm': 2.214718818664551, 'learning_rate': 2e-06, 'rewards/chosen': -0.10261175781488419, 'rewards/rejected': -0.24886471033096313, 'rewards/accuracies': 0.7076562643051147, 'rewards/margins': 0.14625293016433716, 'logps/rejected': -62.757694244384766, 'logps/chosen': -61.188941955566406, 'logits/rejected': -35.946041107177734, 'logits/chosen': -35.25832748413086, 'epoch': 1.62}\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [05:35<07:53,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.67it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.24it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.88it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6388120651245117, 'eval_runtime': 5.3618, 'eval_samples_per_second': 232.197, 'eval_steps_per_second': 1.865, 'eval_rewards/chosen': -0.13759925961494446, 'eval_rewards/rejected': -0.27226120233535767, 'eval_rewards/accuracies': 0.6783938407897949, 'eval_rewards/margins': 0.13466192781925201, 'eval_logps/rejected': -64.0189208984375, 'eval_logps/chosen': -61.38688278198242, 'eval_logits/rejected': -35.53736114501953, 'eval_logits/chosen': -34.52351760864258, 'epoch': 1.62}\n 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 300/740 [05:40<07:53,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\n{'loss': 0.6237, 'grad_norm': 2.0838842391967773, 'learning_rate': 2e-06, 'rewards/chosen': -0.1646009087562561, 'rewards/rejected': -0.3373304009437561, 'rewards/accuracies': 0.7032812237739563, 'rewards/margins': 0.1727295070886612, 'logps/rejected': -63.640018463134766, 'logps/chosen': -62.158626556396484, 'logits/rejected': -35.65841293334961, 'logits/chosen': -34.73658752441406, 'epoch': 1.89}\n{'loss': 0.5971, 'grad_norm': 2.070476531982422, 'learning_rate': 2e-06, 'rewards/chosen': -0.2310403287410736, 'rewards/rejected': -0.4757847487926483, 'rewards/accuracies': 0.7287564277648926, 'rewards/margins': 0.2447444498538971, 'logps/rejected': -65.4090805053711, 'logps/chosen': -62.3161506652832, 'logits/rejected': -35.71369552612305, 'logits/chosen': -35.073036193847656, 'epoch': 2.16}\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 400/740 [07:29<06:04,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.24it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.89it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.88it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6208000183105469, 'eval_runtime': 5.3626, 'eval_samples_per_second': 232.165, 'eval_steps_per_second': 1.865, 'eval_rewards/chosen': -0.29676368832588196, 'eval_rewards/rejected': -0.49901142716407776, 'eval_rewards/accuracies': 0.6695060729980469, 'eval_rewards/margins': 0.2022477686405182, 'eval_logps/rejected': -66.28642272949219, 'eval_logps/chosen': -62.9785270690918, 'eval_logits/rejected': -35.54949188232422, 'eval_logits/chosen': -34.601585388183594, 'epoch': 2.16}\n 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 400/740 [07:35<06:04,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\n{'loss': 0.5912, 'grad_norm': 2.161388635635376, 'learning_rate': 2e-06, 'rewards/chosen': -0.31744828820228577, 'rewards/rejected': -0.5923032164573669, 'rewards/accuracies': 0.7235937714576721, 'rewards/margins': 0.2748548984527588, 'logps/rejected': -66.5337905883789, 'logps/chosen': -63.336387634277344, 'logits/rejected': -35.738258361816406, 'logits/chosen': -35.25392150878906, 'epoch': 2.43}\n{'loss': 0.5831, 'grad_norm': 1.9922198057174683, 'learning_rate': 2e-06, 'rewards/chosen': -0.3883562386035919, 'rewards/rejected': -0.698293149471283, 'rewards/accuracies': 0.7201562523841858, 'rewards/margins': 0.3099367916584015, 'logps/rejected': -67.64063262939453, 'logps/chosen': -64.11766052246094, 'logits/rejected': -35.429847717285156, 'logits/chosen': -34.92546463012695, 'epoch': 2.7}\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 500/740 [09:22<04:16,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.63it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.57it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.88it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6094419956207275, 'eval_runtime': 5.3757, 'eval_samples_per_second': 231.599, 'eval_steps_per_second': 1.86, 'eval_rewards/chosen': -0.4370468258857727, 'eval_rewards/rejected': -0.6920305490493774, 'eval_rewards/accuracies': 0.6771253347396851, 'eval_rewards/margins': 0.25498372316360474, 'eval_logps/rejected': -68.21662139892578, 'eval_logps/chosen': -64.38136291503906, 'eval_logits/rejected': -35.231346130371094, 'eval_logits/chosen': -34.3125, 'epoch': 2.7}\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 500/740 [09:27<04:16,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\n{'loss': 0.5754, 'grad_norm': 2.0791378021240234, 'learning_rate': 2e-06, 'rewards/chosen': -0.44136282801628113, 'rewards/rejected': -0.7790487408638, 'rewards/accuracies': 0.7342187762260437, 'rewards/margins': 0.3376859724521637, 'logps/rejected': -68.23162841796875, 'logps/chosen': -64.51481628417969, 'logits/rejected': -34.95964431762695, 'logits/chosen': -34.516151428222656, 'epoch': 2.97}\n{'loss': 0.5562, 'grad_norm': 2.1040704250335693, 'learning_rate': 2e-06, 'rewards/chosen': -0.5041024088859558, 'rewards/rejected': -0.9029764533042908, 'rewards/accuracies': 0.7464126348495483, 'rewards/margins': 0.39887410402297974, 'logps/rejected': -69.55812072753906, 'logps/chosen': -65.0136489868164, 'logits/rejected': -34.683509826660156, 'logits/chosen': -34.266151428222656, 'epoch': 3.24}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [11:17<02:31,  1.08s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.58it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.24it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.03it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.96it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.89it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.88it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.600125789642334, 'eval_runtime': 5.3642, 'eval_samples_per_second': 232.092, 'eval_steps_per_second': 1.864, 'eval_rewards/chosen': -0.582657516002655, 'eval_rewards/rejected': -0.8889216184616089, 'eval_rewards/accuracies': 0.6779066324234009, 'eval_rewards/margins': 0.3062640130519867, 'eval_logps/rejected': -70.18553161621094, 'eval_logps/chosen': -65.83747100830078, 'eval_logits/rejected': -34.73503875732422, 'eval_logits/chosen': -33.83944320678711, 'epoch': 3.24}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 600/740 [11:22<02:31,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\n{'loss': 0.5536, 'grad_norm': 2.1003029346466064, 'learning_rate': 2e-06, 'rewards/chosen': -0.5823505520820618, 'rewards/rejected': -1.0003665685653687, 'rewards/accuracies': 0.7470312714576721, 'rewards/margins': 0.4180159270763397, 'logps/rejected': -70.77876281738281, 'logps/chosen': -66.0328140258789, 'logits/rejected': -34.76191329956055, 'logits/chosen': -34.30950164794922, 'epoch': 3.51}\n{'loss': 0.5549, 'grad_norm': 2.0983963012695312, 'learning_rate': 2e-06, 'rewards/chosen': -0.632660984992981, 'rewards/rejected': -1.0638481378555298, 'rewards/accuracies': 0.73828125, 'rewards/margins': 0.4311871826648712, 'logps/rejected': -70.84220886230469, 'logps/chosen': -66.73524475097656, 'logits/rejected': -34.89552307128906, 'logits/chosen': -34.43488693237305, 'epoch': 3.78}\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 700/740 [13:09<00:42,  1.07s/it]evaluation_loop\n\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 2/10 [00:00<00:02,  3.66it/s]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 3/10 [00:01<00:02,  2.56it/s]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4/10 [00:01<00:02,  2.23it/s]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 5/10 [00:02<00:02,  2.02it/s]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 6/10 [00:02<00:02,  1.95it/s]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 7/10 [00:03<00:01,  1.91it/s]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 8/10 [00:03<00:01,  1.88it/s]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9/10 [00:04<00:00,  1.88it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5938013792037964, 'eval_runtime': 5.3719, 'eval_samples_per_second': 231.762, 'eval_steps_per_second': 1.862, 'eval_rewards/chosen': -0.6733404994010925, 'eval_rewards/rejected': -1.014796495437622, 'eval_rewards/accuracies': 0.6813256144523621, 'eval_rewards/margins': 0.3414561152458191, 'eval_logps/rejected': -71.44428253173828, 'eval_logps/chosen': -66.74429321289062, 'eval_logits/rejected': -34.601619720458984, 'eval_logits/chosen': -33.72803497314453, 'epoch': 3.78}\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 700/740 [13:15<00:42,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.06it/s]\u001b[A\n{'train_runtime': 856.3823, 'train_samples_per_second': 110.465, 'train_steps_per_second': 0.864, 'train_loss': 0.612536527659442, 'epoch': 4.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 740/740 [13:59<00:00,  1.13s/it]\nEvaling every 185\nTraceback (most recent call last):\n  File \"/kaggle/working/cs234-project/dpo.py\", line 198, in <module>\n    for epoch in range(1, training_args.num_train_epochs+1):\nTypeError: 'float' object cannot be interpreted as an integer\n","output_type":"stream"}]},{"cell_type":"code","source":"!git lfs uninstall\n!git stash && git pull --rebase && git stash pop\n# !git status","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:04:03.416206Z","iopub.execute_input":"2024-05-29T10:04:03.417169Z","iopub.status.idle":"2024-05-29T10:04:09.965762Z","shell.execute_reply.started":"2024-05-29T10:04:03.417133Z","shell.execute_reply":"2024-05-29T10:04:09.964783Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Hooks for this repository have been removed.\nGlobal Git LFS configuration has been removed.\nSaved working directory and index state WIP on main: 832656a incorp eval into dpo.py\nEncountered 1 file(s) that should have been pointers, but weren't:\n\tpref_pairs_16_token_tokenized_split/data-00001-of-00002.arrow\nerror: cannot pull with rebase: You have unstaged changes.\nerror: please commit or stash them.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git status","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:03:27.082066Z","iopub.execute_input":"2024-05-29T10:03:27.082728Z","iopub.status.idle":"2024-05-29T10:03:30.215627Z","shell.execute_reply.started":"2024-05-29T10:03:27.082695Z","shell.execute_reply":"2024-05-29T10:03:30.214438Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Refresh index: 100% (65/65), done.\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\t\u001b[31mmodified:   pref_pairs_16_token_tokenized_split/data-00001-of-00002.arrow\u001b[m\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t\u001b[31m=0.3.0\u001b[m\n\t\u001b[31m=0.5.7\u001b[m\n\t\u001b[31m=1.17.0\u001b[m\n\t\u001b[31m=1.4.0\u001b[m\n\t\u001b[31mdpo_4c2_div6-0_1-2024.05.29.09.23/\u001b[m\n\t\u001b[31mgpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/cache-91bb8d98faf6cf03.arrow\u001b[m\n\t\u001b[31mgpt2-imdb-16-token-pref-pairs-2-choose-2_tokenized/train/cache-eedce7139ba41601.arrow\u001b[m\n\t\u001b[31mresults/\u001b[m\n\t\u001b[31mwandb/run-20240529_092405-aa2vv9bu/\u001b[m\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ppo.py --exp_name=eval --eval_model=./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-740\n!python ppo.py --exp_name=eval --eval_model=./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-555\n!python ppo.py --exp_name=eval --eval_model=./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-370\n!python ppo.py --exp_name=eval --eval_model=./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-185","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:42:13.466915Z","iopub.execute_input":"2024-05-29T09:42:13.467372Z","iopub.status.idle":"2024-05-29T10:00:24.193554Z","shell.execute_reply.started":"2024-05-29T09:42:13.467338Z","shell.execute_reply":"2024-05-29T10:00:24.192343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2024-05-29 09:42:19.897010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 09:42:19.897067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 09:42:19.898604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-740', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.29.09.42'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nDownloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.81k/7.81k [00:00<00:00, 22.4MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0M/21.0M [00:00<00:00, 57.8MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 81.0MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.0M/42.0M [00:00<00:00, 134MB/s]\nGenerating train split: 100%|â–ˆâ–ˆ| 25000/25000 [00:00<00:00, 147843.14 examples/s]\nGenerating test split: 100%|â–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 188367.85 examples/s]\nGenerating unsupervised split: 100%|â–ˆ| 50000/50000 [00:00<00:00, 192873.13 examp\nFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 160762.40 examples/s]\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24895/24895 [00:28<00:00, 859.74 examples/s]\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-740\n****** hi ******\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 687/687 [00:00<00:00, 3.20MB/s]\npytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [00:04<00:00, 294MB/s]\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 1.40MB/s]\nvocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 31.7MB/s]\nmerges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 7.12MB/s]\nspecial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 772kB/s]\neval batch size 256\nFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [00:00<00:00, 82621.64 examples/s]\nMap:   3%|â–Š                           | 75/2487 [00:00<00:03, 726.47 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2487/2487 [00:02<00:00, 846.77 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:28<03:47, 28.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:54<03:08, 26.93s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:20<02:38, 26.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:46<02:11, 26.35s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:12<01:45, 26.31s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 220, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:38<01:18, 26.27s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:04<00:52, 26.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:30<00:26, 26.16s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:57<00:00, 26.34s/it]\nmean test reward 0.8852109092867724 +/- 0.006555952299857035 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.998820960521698 from 0.00922983093187213\nmean KL 2.7777171390415685 +/- 0.1096998264196574 full 7.942126693100565 +/- 0.051483828245708525\nmedian KL 3.3970669507980347 full 7.5865113735198975\n2024-05-29 09:47:27.145579: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 09:47:27.145636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 09:47:27.147311: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-555', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.29.09.47'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-555\n****** hi ******\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:25<03:27, 25.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:51<03:00, 25.72s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:17<02:34, 25.72s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:43<02:08, 25.77s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:09<01:43, 25.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 220, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:34<01:17, 25.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:00<00:51, 25.80s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:26<00:25, 25.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:52<00:00, 25.82s/it]\nmean test reward 0.8664879618084645 +/- 0.006990641960835698 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9987834692001343 from 0.00922983093187213\nmean KL 3.0524879715327793 +/- 0.09790775789132283 full 6.1879449557099075 +/- 0.04521319589762177\nmedian KL 3.5800955295562744 full 5.907324314117432\n2024-05-29 09:51:47.067773: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 09:51:47.067835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 09:51:47.069450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-370', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.29.09.51'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-370\n****** hi ******\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:26<03:29, 26.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:52<03:02, 26.10s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:18<02:36, 26.02s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:44<02:10, 26.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:10<01:43, 25.98s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 220, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:36<01:17, 26.00s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:01<00:51, 25.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:27<00:25, 25.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:53<00:00, 25.98s/it]\nmean test reward 0.81070346466082 +/- 0.008073459573376749 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9985955357551575 from 0.00922983093187213\nmean KL 2.1459453833263575 +/- 0.06386425653442682 full 3.416195798717025 +/- 0.03020971527792236\nmedian KL 2.346280336380005 full 3.198375940322876\n2024-05-29 09:56:08.885462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 09:56:08.885526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 09:56:08.887250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPPOConfig: PPOConfig(exp_name='eval', normalize_scores=False, dry_run=False, eval_model='./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-185', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:siebert/sentiment-roberta-large-english', remove_unused_columns=True, tracker_kwargs={'wandb': {'name': 'eval-2024.05.29.09.56'}}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='cs234', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=False, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=256, forward_batch_size=None, mini_batch_size=256, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=256, global_backward_batch_size=None, global_batch_size=None)\nload ref model lvwerra/gpt2-imdb\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nload train model ./dpo_4c2_div6-0_1-2024.05.29.09.23/checkpoint-185\n****** hi ******\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:26<03:28, 26.04s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:51<03:00, 25.83s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:17<02:34, 25.82s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:43<02:09, 25.85s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:09<01:43, 25.90s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/ppo.py\", line 270, in <module>\n    run(ppo_config, args, full_name)\n  File \"/kaggle/working/cs234-project/ppo.py\", line 220, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n****** hi yo scores ******\n****** hi yo scores ******\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:35<01:17, 25.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:01<00:51, 25.99s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:27<00:26, 26.08s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n****** hi yo scores ******\n****** hi yo scores ******\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:53<00:00, 25.95s/it]\nmean test reward 0.622725342185984 +/- 0.009992607761410878 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9953120052814484 from 0.00922983093187213\nmean KL 0.38579599234314327 +/- 0.026655404001560166 full 0.6326338682508018 +/- 0.006804116134131183\nmedian KL 0.4585424065589905 full 0.573160856962204\n","output_type":"stream"}]},{"cell_type":"code","source":"! git pull --rebase && python dpo.py  --output_dir=dpo_2c2_div36-0_2  --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_div_36_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 50     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=50 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=14","metadata":{"execution":{"iopub.status.busy":"2024-05-31T12:56:32.298865Z","iopub.execute_input":"2024-05-31T12:56:32.299222Z","iopub.status.idle":"2024-05-31T13:23:55.991296Z","shell.execute_reply.started":"2024-05-31T12:56:32.299194Z","shell.execute_reply":"2024-05-31T13:23:55.990262Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Already up to date.\nCurrent branch main is up to date.\n2024-05-31 12:56:43.289949: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 12:56:43.290084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 12:56:43.416199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 2.83MB/s]\npytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:04<00:00, 133MB/s]\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.0/17.0 [00:00<00:00, 94.6kB/s]\nvocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 9.40MB/s]\nmerges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 19.9MB/s]\nspecial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 494kB/s]\nds len 4149\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240531_125700-bksjq0p4\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_2c2_div36-0_2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/bksjq0p4\u001b[0m\n  0%|                                                   | 0/434 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 4.457698822021484, 'learning_rate': 4e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -59.777408599853516, 'logps/chosen': -59.89320373535156, 'logits/rejected': -38.17497253417969, 'logits/chosen': -36.901248931884766, 'epoch': 0.03}\n{'loss': 0.6855, 'grad_norm': 4.779141426086426, 'learning_rate': 2e-06, 'rewards/chosen': 0.0068482644855976105, 'rewards/rejected': -0.00881270319223404, 'rewards/accuracies': 0.7177792191505432, 'rewards/margins': 0.01566096767783165, 'logps/rejected': -60.12541961669922, 'logps/chosen': -60.16237258911133, 'logits/rejected': -38.14323425292969, 'logits/chosen': -37.1422233581543, 'epoch': 1.61}\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 50/434 [00:56<06:52,  1.07s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6753953695297241, 'eval_runtime': 0.8969, 'eval_samples_per_second': 231.899, 'eval_steps_per_second': 2.23, 'eval_rewards/chosen': 0.015026548877358437, 'eval_rewards/rejected': -0.021298756822943687, 'eval_rewards/accuracies': 0.725781261920929, 'eval_rewards/margins': 0.036325301975011826, 'eval_logps/rejected': -59.63201904296875, 'eval_logps/chosen': -60.3870964050293, 'eval_logits/rejected': -38.618595123291016, 'eval_logits/chosen': -36.915306091308594, 'epoch': 1.61}\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 50/434 [00:57<06:52,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.17it/s]\u001b[A\n{'loss': 0.6425, 'grad_norm': 4.061528205871582, 'learning_rate': 2e-06, 'rewards/chosen': 0.03371657431125641, 'rewards/rejected': -0.07416805624961853, 'rewards/accuracies': 0.8340579271316528, 'rewards/margins': 0.10788463801145554, 'logps/rejected': -60.83827590942383, 'logps/chosen': -60.12892150878906, 'logits/rejected': -37.59474563598633, 'logits/chosen': -36.678531646728516, 'epoch': 3.23}\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 100/434 [01:55<06:25,  1.15s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6468830108642578, 'eval_runtime': 0.8954, 'eval_samples_per_second': 232.305, 'eval_steps_per_second': 2.234, 'eval_rewards/chosen': 0.014868051744997501, 'eval_rewards/rejected': -0.08696494996547699, 'eval_rewards/accuracies': 0.7320312261581421, 'eval_rewards/margins': 0.10183300077915192, 'eval_logps/rejected': -59.960350036621094, 'eval_logps/chosen': -60.38788604736328, 'eval_logits/rejected': -38.28150177001953, 'eval_logits/chosen': -36.58568572998047, 'epoch': 3.23}\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 100/434 [01:56<06:25,  1.15s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.5891, 'grad_norm': 3.888270854949951, 'learning_rate': 2e-06, 'rewards/chosen': 0.04808701574802399, 'rewards/rejected': -0.18539811670780182, 'rewards/accuracies': 0.8686030507087708, 'rewards/margins': 0.2334851324558258, 'logps/rejected': -61.1788215637207, 'logps/chosen': -60.0180778503418, 'logits/rejected': -37.466529846191406, 'logits/chosen': -36.466148376464844, 'epoch': 4.84}\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 150/434 [02:52<05:03,  1.07s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6230628490447998, 'eval_runtime': 0.8954, 'eval_samples_per_second': 232.299, 'eval_steps_per_second': 2.234, 'eval_rewards/chosen': -0.012049821205437183, 'eval_rewards/rejected': -0.17742815613746643, 'eval_rewards/accuracies': 0.7320312261581421, 'eval_rewards/margins': 0.16537833213806152, 'eval_logps/rejected': -60.41266632080078, 'eval_logps/chosen': -60.52247619628906, 'eval_logits/rejected': -37.91494369506836, 'eval_logits/chosen': -36.225059509277344, 'epoch': 4.84}\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 150/434 [02:52<05:03,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\u001b[A\n{'loss': 0.5417, 'grad_norm': 3.5543665885925293, 'learning_rate': 2e-06, 'rewards/chosen': 0.04019637405872345, 'rewards/rejected': -0.31783145666122437, 'rewards/accuracies': 0.8842265605926514, 'rewards/margins': 0.358027845621109, 'logps/rejected': -61.76299285888672, 'logps/chosen': -60.08187484741211, 'logits/rejected': -37.078216552734375, 'logits/chosen': -36.21443176269531, 'epoch': 6.45}\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 200/434 [03:51<04:12,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6052026152610779, 'eval_runtime': 0.8972, 'eval_samples_per_second': 231.826, 'eval_steps_per_second': 2.229, 'eval_rewards/chosen': -0.055661194026470184, 'eval_rewards/rejected': -0.2765278220176697, 'eval_rewards/accuracies': 0.7242187261581421, 'eval_rewards/margins': 0.2208666205406189, 'eval_logps/rejected': -60.908164978027344, 'eval_logps/chosen': -60.740535736083984, 'eval_logits/rejected': -37.735050201416016, 'eval_logits/chosen': -36.042625427246094, 'epoch': 6.45}\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 200/434 [03:52<04:12,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\u001b[A\n{'loss': 0.499, 'grad_norm': 3.660104274749756, 'learning_rate': 2e-06, 'rewards/chosen': 0.017626995220780373, 'rewards/rejected': -0.4650396406650543, 'rewards/accuracies': 0.8947787284851074, 'rewards/margins': 0.48266661167144775, 'logps/rejected': -62.62276840209961, 'logps/chosen': -60.033241271972656, 'logits/rejected': -36.82243728637695, 'logits/chosen': -36.063995361328125, 'epoch': 8.06}\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 250/434 [04:50<04:52,  1.59s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5899829268455505, 'eval_runtime': 0.8961, 'eval_samples_per_second': 232.116, 'eval_steps_per_second': 2.232, 'eval_rewards/chosen': -0.11548011004924774, 'eval_rewards/rejected': -0.39125925302505493, 'eval_rewards/accuracies': 0.725781261920929, 'eval_rewards/margins': 0.275779128074646, 'eval_logps/rejected': -61.48181915283203, 'eval_logps/chosen': -61.03962707519531, 'eval_logits/rejected': -37.53144836425781, 'eval_logits/chosen': -35.82957077026367, 'epoch': 8.06}\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 250/434 [04:51<04:52,  1.59s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\u001b[A\n{'loss': 0.4555, 'grad_norm': 3.1591882705688477, 'learning_rate': 2e-06, 'rewards/chosen': -0.006895406171679497, 'rewards/rejected': -0.6274450421333313, 'rewards/accuracies': 0.9092698097229004, 'rewards/margins': 0.6205495595932007, 'logps/rejected': -63.5665168762207, 'logps/chosen': -60.262939453125, 'logits/rejected': -36.53800964355469, 'logits/chosen': -35.76393127441406, 'epoch': 9.68}\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 300/434 [05:47<02:23,  1.07s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5784097909927368, 'eval_runtime': 0.8966, 'eval_samples_per_second': 231.987, 'eval_steps_per_second': 2.231, 'eval_rewards/chosen': -0.18331176042556763, 'eval_rewards/rejected': -0.5097421407699585, 'eval_rewards/accuracies': 0.733593761920929, 'eval_rewards/margins': 0.3264303505420685, 'eval_logps/rejected': -62.07423400878906, 'eval_logps/chosen': -61.378787994384766, 'eval_logits/rejected': -37.40387725830078, 'eval_logits/chosen': -35.689361572265625, 'epoch': 9.68}\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 300/434 [05:48<02:23,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.4172, 'grad_norm': 2.8968937397003174, 'learning_rate': 2e-06, 'rewards/chosen': -0.0385768786072731, 'rewards/rejected': -0.7920372486114502, 'rewards/accuracies': 0.9205089807510376, 'rewards/margins': 0.7534604072570801, 'logps/rejected': -63.98247146606445, 'logps/chosen': -60.438907623291016, 'logits/rejected': -36.29678726196289, 'logits/chosen': -35.62824630737305, 'epoch': 11.29}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 350/434 [06:46<01:34,  1.13s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5705616474151611, 'eval_runtime': 0.8973, 'eval_samples_per_second': 231.81, 'eval_steps_per_second': 2.229, 'eval_rewards/chosen': -0.2597673535346985, 'eval_rewards/rejected': -0.6322852373123169, 'eval_rewards/accuracies': 0.725781261920929, 'eval_rewards/margins': 0.3725178837776184, 'eval_logps/rejected': -62.686946868896484, 'eval_logps/chosen': -61.76106643676758, 'eval_logits/rejected': -37.166221618652344, 'eval_logits/chosen': -35.456233978271484, 'epoch': 11.29}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 350/434 [06:47<01:34,  1.13s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\u001b[A\n{'loss': 0.3759, 'grad_norm': 3.1730213165283203, 'learning_rate': 2e-06, 'rewards/chosen': -0.08098553866147995, 'rewards/rejected': -0.9892446398735046, 'rewards/accuracies': 0.9366243481636047, 'rewards/margins': 0.9082591533660889, 'logps/rejected': -65.41328430175781, 'logps/chosen': -60.729957580566406, 'logits/rejected': -36.005836486816406, 'logits/chosen': -35.353607177734375, 'epoch': 12.9}\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/434 [07:43<00:36,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5662203431129456, 'eval_runtime': 0.8951, 'eval_samples_per_second': 232.375, 'eval_steps_per_second': 2.234, 'eval_rewards/chosen': -0.34989914298057556, 'eval_rewards/rejected': -0.7664862871170044, 'eval_rewards/accuracies': 0.7132812738418579, 'eval_rewards/margins': 0.41658714413642883, 'eval_logps/rejected': -63.35795593261719, 'eval_logps/chosen': -62.21172332763672, 'eval_logits/rejected': -37.03529357910156, 'eval_logits/chosen': -35.338050842285156, 'epoch': 12.9}\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/434 [07:44<00:36,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'train_runtime': 522.4838, 'train_samples_per_second': 105.599, 'train_steps_per_second': 0.831, 'train_loss': 0.5117325156515095, 'epoch': 14.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 434/434 [08:25<00:00,  1.17s/it]\nEvaling epochs [14, 13, 12, 11]\nLoading from dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-434\nDownloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.81k/7.81k [00:00<00:00, 14.1MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0M/21.0M [00:00<00:00, 44.0MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 71.3MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.0M/42.0M [00:00<00:00, 77.9MB/s]\nGenerating train split: 100%|â–ˆâ–ˆ| 25000/25000 [00:00<00:00, 118807.10 examples/s]\nGenerating test split: 100%|â–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 135386.95 examples/s]\nGenerating unsupervised split: 100%|â–ˆ| 50000/50000 [00:00<00:00, 155436.13 examp\nFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 148219.52 examples/s]\nMap:   0%|                                     | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24895/24895 [00:30<00:00, 829.67 examples/s]\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-434\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\nconfig.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 687/687 [00:00<00:00, 1.68MB/s]\npytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [00:09<00:00, 145MB/s]\ntokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [00:00<00:00, 700kB/s]\nvocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 7.93MB/s]\nmerges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 5.83MB/s]\nspecial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 487kB/s]\neval batch size 256\nFilter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [00:00<00:00, 3318.44 examples/s]\nMap:   2%|â–‹                           | 56/2487 [00:00<00:04, 544.38 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\nMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2487/2487 [00:03<00:00, 808.58 examples/s]\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:53, 29.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:56<03:16, 28.14s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:23<02:46, 27.76s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:51<02:18, 27.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:18<01:50, 27.58s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/dpo.py\", line 207, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(checkpoint, f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 267, in eval\n    return run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 237, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:46<01:22, 27.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:13<00:54, 27.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:40<00:27, 27.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:08<00:00, 27.57s/it]\nmean test reward 0.7788095767055488 +/- 0.008534075360061153 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.998336136341095 from 0.00922983093187213\nmean KL 0.11373422345948508 +/- 0.08207846509860249 full 3.2612975133169027 +/- 0.02336270030822857\nmedian KL 0.6921874284744263 full 3.0900312662124634\nLoading from dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-403\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-403\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:27<03:36, 27.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:54<03:09, 27.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:21<02:42, 27.13s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:48<02:16, 27.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:16<01:49, 27.42s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:43<01:22, 27.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:11<00:54, 27.40s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:38<00:27, 27.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:05<00:00, 27.33s/it]\nmean test reward 0.7666216387250794 +/- 0.008698218592813835 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9982960224151611 from 0.00922983093187213\nmean KL 0.25533394899038186 +/- 0.0740646769470192 full 2.9068666654897646 +/- 0.021860948213741904\nmedian KL 0.7413899898529053 full 2.729762077331543\nLoading from dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-372\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-372\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:26<03:35, 26.95s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:54<03:09, 27.01s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:21<02:42, 27.07s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:48<02:15, 27.16s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:16<01:49, 27.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:43<01:22, 27.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:11<00:54, 27.48s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:38<00:27, 27.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:06<00:00, 27.35s/it]\nmean test reward 0.7518777677059157 +/- 0.008889930635978724 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9982061982154846 from 0.00922983093187213\nmean KL 0.4632180293750328 +/- 0.06630164214644417 full 2.580733753663177 +/- 0.02035531768823465\nmedian KL 0.8835456371307373 full 2.4148108959198\nLoading from dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-341\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_2-2024.05.31.12.56/checkpoint-341\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:27<03:37, 27.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:54<03:09, 27.13s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:21<02:42, 27.12s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:48<02:15, 27.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:15<01:48, 27.22s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:43<01:21, 27.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:11<00:54, 27.40s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:38<00:27, 27.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:05<00:00, 27.31s/it]\nmean test reward 0.7435291696344595 +/- 0.008991659373749796 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9981276988983154 from 0.00922983093187213\nmean KL 0.45835215082236874 +/- 0.06165984829422358 full 2.2413805371357336 +/- 0.018486413788620697\nmedian KL 0.8887109458446503 full 2.111499786376953\ndpo_2c2_div36-0_2-2024.05.31.12.56,0.2,epoch 14,0.7788095767055488,0.008534075360061153,3.2612975133169027,0.02336270030822857\ndpo_2c2_div36-0_2-2024.05.31.12.56,0.2,epoch 13,0.7666216387250794,0.008698218592813835,2.9068666654897646,0.021860948213741904\ndpo_2c2_div36-0_2-2024.05.31.12.56,0.2,epoch 12,0.7518777677059157,0.008889930635978724,2.580733753663177,0.02035531768823465\ndpo_2c2_div36-0_2-2024.05.31.12.56,0.2,epoch 11,0.7435291696344595,0.008991659373749796,2.2413805371357336,0.018486413788620697\n","output_type":"stream"}]},{"cell_type":"code","source":"! rm -rf */*/optimizer.pt && git pull --rebase && python dpo.py  --output_dir=dpo_2c2_div36-0_6  --beta=0.6 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_2_choose_2_div_36_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 50     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=50 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=10","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:06:47.161260Z","iopub.execute_input":"2024-05-31T22:06:47.161656Z","iopub.status.idle":"2024-05-31T22:32:10.389622Z","shell.execute_reply.started":"2024-05-31T22:06:47.161622Z","shell.execute_reply":"2024-05-31T22:32:10.388334Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Already up to date.\nCurrent branch main is up to date.\n2024-05-31 22:06:55.501560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 22:06:55.501627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 22:06:55.503372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 4149\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240531_220702-yoetp9ow\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_2c2_div36-0_6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/yoetp9ow\u001b[0m\n  0%|                                                   | 0/310 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 13.373095512390137, 'learning_rate': 4e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -59.777408599853516, 'logps/chosen': -59.89320373535156, 'logits/rejected': -38.17497253417969, 'logits/chosen': -36.901248931884766, 'epoch': 0.03}\n{'loss': 0.6712, 'grad_norm': 13.83750057220459, 'learning_rate': 2e-06, 'rewards/chosen': 0.020435109734535217, 'rewards/rejected': -0.026046497747302055, 'rewards/accuracies': 0.7198519110679626, 'rewards/margins': 0.04648160561919212, 'logps/rejected': -60.1247673034668, 'logps/chosen': -60.16255187988281, 'logits/rejected': -38.144405364990234, 'logits/chosen': -37.14326477050781, 'epoch': 1.61}\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 50/310 [00:56<04:40,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6449757814407349, 'eval_runtime': 0.9054, 'eval_samples_per_second': 229.732, 'eval_steps_per_second': 2.209, 'eval_rewards/chosen': 0.044213440269231796, 'eval_rewards/rejected': -0.06083408743143082, 'eval_rewards/accuracies': 0.725781261920929, 'eval_rewards/margins': 0.10504752397537231, 'eval_logps/rejected': -59.626914978027344, 'eval_logps/chosen': -60.38854217529297, 'eval_logits/rejected': -38.62693786621094, 'eval_logits/chosen': -36.924034118652344, 'epoch': 1.61}\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 50/310 [00:57<04:40,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.17it/s]\u001b[A\n{'loss': 0.5627, 'grad_norm': 10.248089790344238, 'learning_rate': 2e-06, 'rewards/chosen': 0.10624582320451736, 'rewards/rejected': -0.2024117261171341, 'rewards/accuracies': 0.853870689868927, 'rewards/margins': 0.30865755677223206, 'logps/rejected': -60.80479431152344, 'logps/chosen': -60.12042999267578, 'logits/rejected': -37.634273529052734, 'logits/chosen': -36.712493896484375, 'epoch': 3.23}\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 100/310 [01:55<04:02,  1.16s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5921259522438049, 'eval_runtime': 0.9221, 'eval_samples_per_second': 225.569, 'eval_steps_per_second': 2.169, 'eval_rewards/chosen': 0.05496162176132202, 'eval_rewards/rejected': -0.20663511753082275, 'eval_rewards/accuracies': 0.721875011920929, 'eval_rewards/margins': 0.2615967392921448, 'eval_logps/rejected': -59.869911193847656, 'eval_logps/chosen': -60.37062454223633, 'eval_logits/rejected': -38.35560607910156, 'eval_logits/chosen': -36.661102294921875, 'epoch': 3.23}\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 100/310 [01:56<04:02,  1.16s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.15it/s]\u001b[A\n{'loss': 0.452, 'grad_norm': 9.7669095993042, 'learning_rate': 2e-06, 'rewards/chosen': 0.18884025514125824, 'rewards/rejected': -0.44291210174560547, 'rewards/accuracies': 0.9168115854263306, 'rewards/margins': 0.6317523717880249, 'logps/rejected': -60.990020751953125, 'logps/chosen': -59.94377136230469, 'logits/rejected': -37.595176696777344, 'logits/chosen': -36.57362747192383, 'epoch': 4.84}\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 150/310 [02:52<02:51,  1.07s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5651960372924805, 'eval_runtime': 0.9069, 'eval_samples_per_second': 229.348, 'eval_steps_per_second': 2.205, 'eval_rewards/chosen': 0.031827278435230255, 'eval_rewards/rejected': -0.34551459550857544, 'eval_rewards/accuracies': 0.729687511920929, 'eval_rewards/margins': 0.3773418962955475, 'eval_logps/rejected': -60.10137939453125, 'eval_logps/chosen': -60.4091796875, 'eval_logits/rejected': -38.09440994262695, 'eval_logits/chosen': -36.40609359741211, 'epoch': 4.84}\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 150/310 [02:53<02:51,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.3685, 'grad_norm': 7.596974849700928, 'learning_rate': 2e-06, 'rewards/chosen': 0.2519274652004242, 'rewards/rejected': -0.6680903434753418, 'rewards/accuracies': 0.9473422169685364, 'rewards/margins': 0.9200178384780884, 'logps/rejected': -61.287315368652344, 'logps/chosen': -59.862972259521484, 'logits/rejected': -37.35255432128906, 'logits/chosen': -36.4424934387207, 'epoch': 6.45}\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 200/310 [03:52<01:59,  1.09s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.553561806678772, 'eval_runtime': 0.9061, 'eval_samples_per_second': 229.56, 'eval_steps_per_second': 2.207, 'eval_rewards/chosen': 0.0006102230399847031, 'eval_rewards/rejected': -0.45275580883026123, 'eval_rewards/accuracies': 0.7523437738418579, 'eval_rewards/margins': 0.4533660411834717, 'eval_logps/rejected': -60.28011703491211, 'eval_logps/chosen': -60.46120834350586, 'eval_logits/rejected': -38.05343246459961, 'eval_logits/chosen': -36.36341857910156, 'epoch': 6.45}\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 200/310 [03:53<01:59,  1.09s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.3032, 'grad_norm': 7.089446544647217, 'learning_rate': 2e-06, 'rewards/chosen': 0.30487605929374695, 'rewards/rejected': -0.879740297794342, 'rewards/accuracies': 0.971457302570343, 'rewards/margins': 1.1846164464950562, 'logps/rejected': -61.763797760009766, 'logps/chosen': -59.613250732421875, 'logits/rejected': -37.233951568603516, 'logits/chosen': -36.39678192138672, 'epoch': 8.06}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 250/310 [04:51<01:35,  1.59s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5464204549789429, 'eval_runtime': 0.9076, 'eval_samples_per_second': 229.182, 'eval_steps_per_second': 2.204, 'eval_rewards/chosen': -0.04135894775390625, 'eval_rewards/rejected': -0.562824010848999, 'eval_rewards/accuracies': 0.7445312738418579, 'eval_rewards/margins': 0.5214650630950928, 'eval_logps/rejected': -60.463565826416016, 'eval_logps/chosen': -60.531158447265625, 'eval_logits/rejected': -37.98679733276367, 'eval_logits/chosen': -36.28525924682617, 'epoch': 8.06}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 250/310 [04:52<01:35,  1.59s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.17it/s]\u001b[A\n{'loss': 0.2427, 'grad_norm': 5.8688507080078125, 'learning_rate': 2e-06, 'rewards/chosen': 0.36744415760040283, 'rewards/rejected': -1.0975956916809082, 'rewards/accuracies': 0.9865207076072693, 'rewards/margins': 1.465039849281311, 'logps/rejected': -62.258609771728516, 'logps/chosen': -59.61605453491211, 'logits/rejected': -37.14727783203125, 'logits/chosen': -36.25774383544922, 'epoch': 9.68}\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 300/310 [05:49<00:10,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5439543724060059, 'eval_runtime': 0.9124, 'eval_samples_per_second': 227.971, 'eval_steps_per_second': 2.192, 'eval_rewards/chosen': -0.08732309192419052, 'eval_rewards/rejected': -0.6674565076828003, 'eval_rewards/accuracies': 0.7468750476837158, 'eval_rewards/margins': 0.5801333785057068, 'eval_logps/rejected': -60.63795471191406, 'eval_logps/chosen': -60.607765197753906, 'eval_logits/rejected': -38.029754638671875, 'eval_logits/chosen': -36.315582275390625, 'epoch': 9.68}\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 300/310 [05:49<00:10,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'train_runtime': 379.718, 'train_samples_per_second': 103.788, 'train_steps_per_second': 0.816, 'train_loss': 0.4266352776558168, 'epoch': 10.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [06:03<00:00,  1.17s/it]\nEvaling epochs [10, 9, 8, 7]\nLoading from dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-310\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-310\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:01, 30.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:31, 30.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:00, 30.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:00<02:29, 29.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:29<01:58, 29.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/dpo.py\", line 207, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(checkpoint, f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 267, in eval\n    return run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 237, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:58<01:28, 29.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:28<00:59, 29.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [03:58<00:29, 29.70s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:28<00:00, 29.87s/it]\nmean test reward 0.6391212524596818 +/- 0.009897906565370469 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9960315227508545 from 0.00922983093187213\nmean KL -0.26916779354117654 +/- 0.032779898184985945 full 0.7726878673420288 +/- 0.007736882675559126\nmedian KL -0.08361129462718964 full 0.6954281330108643\nLoading from dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-279\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-279\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:59, 29.97s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:59<03:29, 29.99s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:00, 30.15s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:31, 30.39s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:31<02:01, 30.26s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:01<01:30, 30.18s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:30<00:59, 29.99s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:00<00:29, 29.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:30<00:00, 30.05s/it]\nmean test reward 0.6351716098881752 +/- 0.009912935462829576 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9959022402763367 from 0.00922983093187213\nmean KL -0.1435468545856161 +/- 0.02978147810407998 full 0.6774464803133419 +/- 0.007005335718325589\nmedian KL 0.001112222671508789 full 0.6057892739772797\nLoading from dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-248\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-248\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:56, 29.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [00:58<03:24, 29.23s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:28<02:56, 29.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [01:58<02:29, 29.89s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:29<02:00, 30.09s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [02:59<01:30, 30.20s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:29<01:00, 30.21s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:00<00:30, 30.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:30<00:00, 30.04s/it]\nmean test reward 0.6213208134074143 +/- 0.009984045052045337 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9953671395778656 from 0.00922983093187213\nmean KL -0.03951135013550003 +/- 0.026761263710026587 full 0.5946950046345592 +/- 0.006450082791726419\nmedian KL 0.0839647501707077 full 0.5304930508136749\nLoading from dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-217\nload ref model lvwerra/gpt2-imdb\nload train model dpo_2c2_div36-0_6-2024.05.31.22.06/checkpoint-217\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:01, 30.17s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:31, 30.24s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:31<03:02, 30.38s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:31, 30.30s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:31<02:01, 30.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:02<01:31, 30.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:32<01:00, 30.44s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:03<00:30, 30.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:34<00:00, 30.47s/it]\nmean test reward 0.60487221450665 +/- 0.01006853441441633 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9944943487644196 from 0.00922983093187213\nmean KL 0.03841546146334925 +/- 0.023763910016111358 full 0.5006596317107324 +/- 0.005610986144052309\nmedian KL 0.12166450917720795 full 0.44634170830249786\ndpo_2c2_div36-0_6-2024.05.31.22.06,0.6,epoch 10,0.6391212524596818,0.009897906565370469,0.7726878673420288,0.007736882675559126\ndpo_2c2_div36-0_6-2024.05.31.22.06,0.6,epoch 9,0.6351716098881752,0.009912935462829576,0.6774464803133419,0.007005335718325589\ndpo_2c2_div36-0_6-2024.05.31.22.06,0.6,epoch 8,0.6213208134074143,0.009984045052045337,0.5946950046345592,0.006450082791726419\ndpo_2c2_div36-0_6-2024.05.31.22.06,0.6,epoch 7,0.60487221450665,0.01006853441441633,0.5006596317107324,0.005610986144052309\n","output_type":"stream"}]},{"cell_type":"code","source":"! rm -rf */*/optimizer.pt && git pull --rebase && python dpo.py  --output_dir=dpo_6c2_div36-0_6  --beta=0.6 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_6_choose_2_div_36_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 50     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=50 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=10","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:33:56.437622Z","iopub.execute_input":"2024-05-31T22:33:56.438441Z","iopub.status.idle":"2024-05-31T22:59:56.313471Z","shell.execute_reply.started":"2024-05-31T22:33:56.438402Z","shell.execute_reply":"2024-05-31T22:59:56.312342Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Already up to date.\nCurrent branch main is up to date.\n2024-05-31 22:34:04.736398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 22:34:04.736472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 22:34:04.738176: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 4149\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240531_223411-35ufu5ss\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_6c2_div36-0_6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/35ufu5ss\u001b[0m\n  0%|                                                   | 0/310 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 14.495854377746582, 'learning_rate': 4e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -60.114646911621094, 'logps/chosen': -60.310203552246094, 'logits/rejected': -38.24871826171875, 'logits/chosen': -36.57445526123047, 'epoch': 0.03}\n{'loss': 0.6535, 'grad_norm': 12.631240844726562, 'learning_rate': 2e-06, 'rewards/chosen': 0.04100152105093002, 'rewards/rejected': -0.044826406985521317, 'rewards/accuracies': 0.7765599489212036, 'rewards/margins': 0.08582792431116104, 'logps/rejected': -59.68994140625, 'logps/chosen': -60.61904525756836, 'logits/rejected': -37.69643020629883, 'logits/chosen': -36.74563217163086, 'epoch': 1.61}\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 50/310 [00:56<04:40,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6010024547576904, 'eval_runtime': 0.9014, 'eval_samples_per_second': 230.743, 'eval_steps_per_second': 2.219, 'eval_rewards/chosen': 0.10197781026363373, 'eval_rewards/rejected': -0.10563637316226959, 'eval_rewards/accuracies': 0.8179687261581421, 'eval_rewards/margins': 0.20761418342590332, 'eval_logps/rejected': -59.6917724609375, 'eval_logps/chosen': -60.00627899169922, 'eval_logits/rejected': -39.04746627807617, 'eval_logits/chosen': -38.051063537597656, 'epoch': 1.61}\n 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 50/310 [00:57<04:40,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.4927, 'grad_norm': 9.556352615356445, 'learning_rate': 2e-06, 'rewards/chosen': 0.21580006182193756, 'rewards/rejected': -0.29843220114707947, 'rewards/accuracies': 0.8878511786460876, 'rewards/margins': 0.5142322778701782, 'logps/rejected': -60.22511291503906, 'logps/chosen': -60.56940460205078, 'logits/rejected': -37.184818267822266, 'logits/chosen': -36.31275177001953, 'epoch': 3.23}\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 100/310 [01:55<04:01,  1.15s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.48203644156455994, 'eval_runtime': 0.9032, 'eval_samples_per_second': 230.285, 'eval_steps_per_second': 2.214, 'eval_rewards/chosen': 0.23216640949249268, 'eval_rewards/rejected': -0.35107630491256714, 'eval_rewards/accuracies': 0.8578125238418579, 'eval_rewards/margins': 0.5832427144050598, 'eval_logps/rejected': -60.10084533691406, 'eval_logps/chosen': -59.789306640625, 'eval_logits/rejected': -38.50817108154297, 'eval_logits/chosen': -37.58325958251953, 'epoch': 3.23}\n 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 100/310 [01:56<04:01,  1.15s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.3639, 'grad_norm': 7.8414225578308105, 'learning_rate': 2e-06, 'rewards/chosen': 0.3711971044540405, 'rewards/rejected': -0.617413341999054, 'rewards/accuracies': 0.9404053688049316, 'rewards/margins': 0.9886104464530945, 'logps/rejected': -60.57741165161133, 'logps/chosen': -60.044090270996094, 'logits/rejected': -36.834228515625, 'logits/chosen': -35.98499298095703, 'epoch': 4.84}\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 150/310 [02:51<02:52,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.41368529200553894, 'eval_runtime': 0.9144, 'eval_samples_per_second': 227.474, 'eval_steps_per_second': 2.187, 'eval_rewards/chosen': 0.3078690767288208, 'eval_rewards/rejected': -0.6042134761810303, 'eval_rewards/accuracies': 0.8601562976837158, 'eval_rewards/margins': 0.9120824933052063, 'eval_logps/rejected': -60.522743225097656, 'eval_logps/chosen': -59.66313171386719, 'eval_logits/rejected': -38.20564270019531, 'eval_logits/chosen': -37.34175109863281, 'epoch': 4.84}\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 150/310 [02:52<02:52,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.14it/s]\u001b[A\n{'loss': 0.2823, 'grad_norm': 7.417255878448486, 'learning_rate': 2e-06, 'rewards/chosen': 0.49745434522628784, 'rewards/rejected': -0.9049980640411377, 'rewards/accuracies': 0.9679161310195923, 'rewards/margins': 1.4024523496627808, 'logps/rejected': -61.199798583984375, 'logps/chosen': -60.038963317871094, 'logits/rejected': -36.59610366821289, 'logits/chosen': -35.73332595825195, 'epoch': 6.45}\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 200/310 [03:51<01:59,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.367236465215683, 'eval_runtime': 0.9099, 'eval_samples_per_second': 228.608, 'eval_steps_per_second': 2.198, 'eval_rewards/chosen': 0.37579280138015747, 'eval_rewards/rejected': -0.8408477306365967, 'eval_rewards/accuracies': 0.862500011920929, 'eval_rewards/margins': 1.2166404724121094, 'eval_logps/rejected': -60.91712951660156, 'eval_logps/chosen': -59.5499267578125, 'eval_logits/rejected': -37.98344802856445, 'eval_logits/chosen': -37.17255401611328, 'epoch': 6.45}\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 200/310 [03:52<01:59,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.2312, 'grad_norm': 5.45327091217041, 'learning_rate': 2e-06, 'rewards/chosen': 0.5563752055168152, 'rewards/rejected': -1.1967607736587524, 'rewards/accuracies': 0.9802289605140686, 'rewards/margins': 1.7531360387802124, 'logps/rejected': -61.680057525634766, 'logps/chosen': -59.7828483581543, 'logits/rejected': -36.23818588256836, 'logits/chosen': -35.55148696899414, 'epoch': 8.06}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 250/310 [04:50<01:35,  1.58s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A                                                                             {'eval_loss': 0.3319481313228607, 'eval_runtime': 0.9072, 'eval_samples_per_second': 229.273, 'eval_steps_per_second': 2.205, 'eval_rewards/chosen': 0.42218226194381714, 'eval_rewards/rejected': -1.09774911403656, 'eval_rewards/accuracies': 0.8765624761581421, 'eval_rewards/margins': 1.5199313163757324, 'eval_logps/rejected': -61.345298767089844, 'eval_logps/chosen': -59.47261047363281, 'eval_logits/rejected': -37.853092193603516, 'eval_logits/chosen': -37.08833694458008, 'epoch': 8.06}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 250/310 [04:51<01:35,  1.58s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.1874, 'grad_norm': 5.407650470733643, 'learning_rate': 2e-06, 'rewards/chosen': 0.6521129608154297, 'rewards/rejected': -1.4522608518600464, 'rewards/accuracies': 0.9926144480705261, 'rewards/margins': 2.1043739318847656, 'logps/rejected': -61.947364807128906, 'logps/chosen': -59.69477081298828, 'logits/rejected': -36.190399169921875, 'logits/chosen': -35.26322937011719, 'epoch': 9.68}\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 300/310 [05:47<00:10,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.30556780099868774, 'eval_runtime': 0.912, 'eval_samples_per_second': 228.073, 'eval_steps_per_second': 2.193, 'eval_rewards/chosen': 0.45191067457199097, 'eval_rewards/rejected': -1.3534469604492188, 'eval_rewards/accuracies': 0.8804687261581421, 'eval_rewards/margins': 1.8053576946258545, 'eval_logps/rejected': -61.771461486816406, 'eval_logps/chosen': -59.423057556152344, 'eval_logits/rejected': -37.744686126708984, 'eval_logits/chosen': -37.02537536621094, 'epoch': 9.68}\n 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 300/310 [05:48<00:10,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'train_runtime': 378.4783, 'train_samples_per_second': 104.127, 'train_steps_per_second': 0.819, 'train_loss': 0.362542486575342, 'epoch': 10.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [06:01<00:00,  1.17s/it]\nEvaling epochs [10, 9, 8, 7]\nLoading from dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-310\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-310\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:07, 30.91s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:33, 30.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:31<03:03, 30.55s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:02<02:32, 30.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:02, 30.60s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/dpo.py\", line 207, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(checkpoint, f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 267, in eval\n    return run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 237, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:31, 30.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:34<01:01, 30.60s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:04<00:30, 30.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:35<00:00, 30.60s/it]\nmean test reward 0.638593593436465 +/- 0.009902939773739244 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.996141642332077 from 0.00922983093187213\nmean KL -0.45263531498171183 +/- 0.04187050923985434 full 0.906229280698527 +/- 0.008063932996935365\nmedian KL -0.19230662286281586 full 0.8271127045154572\nLoading from dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-279\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-279\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:02, 30.35s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:32, 30.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:31<03:02, 30.37s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:32, 30.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:02, 30.66s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:32, 30.71s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:34<01:01, 30.69s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:04<00:30, 30.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:36<00:00, 30.69s/it]\nmean test reward 0.6243047529399822 +/- 0.009981774045274757 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9956843852996826 from 0.00922983093187213\nmean KL -0.35450074493160677 +/- 0.037015108555770844 full 0.7716832701504851 +/- 0.007057091882015607\nmedian KL -0.12980487942695618 full 0.7025728821754456\nLoading from dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-248\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-248\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:03, 30.40s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:33, 30.50s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:32<03:05, 30.86s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:03<02:35, 31.03s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:35<02:05, 31.27s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:06<01:33, 31.25s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:37<01:02, 31.32s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:09<00:31, 31.29s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:39<00:00, 31.08s/it]\nmean test reward 0.6125855620860219 +/- 0.010037966863950474 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9950346052646637 from 0.00922983093187213\nmean KL -0.28401966163073666 +/- 0.03309932037596191 full 0.6596172492782999 +/- 0.006254294680309087\nmedian KL -0.09570351243019104 full 0.5961194932460785\nLoading from dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-217\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_6-2024.05.31.22.34/checkpoint-217\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:00, 30.06s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:31, 30.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:01, 30.19s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:31, 30.33s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:02, 30.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:32, 30.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:34<01:01, 30.84s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:05<00:30, 30.88s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:36<00:00, 30.73s/it]\nmean test reward 0.6025706213889634 +/- 0.010093055962045605 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9945091903209686 from 0.00922983093187213\nmean KL -0.2119568701438968 +/- 0.028949104422501638 full 0.5493974553246517 +/- 0.005343837302135849\nmedian KL -0.06908167898654938 full 0.49377205967903137\ndpo_6c2_div36-0_6-2024.05.31.22.34,0.6,epoch 10,0.638593593436465,0.009902939773739244,0.906229280698527,0.008063932996935365\ndpo_6c2_div36-0_6-2024.05.31.22.34,0.6,epoch 9,0.6243047529399822,0.009981774045274757,0.7716832701504851,0.007057091882015607\ndpo_6c2_div36-0_6-2024.05.31.22.34,0.6,epoch 8,0.6125855620860219,0.010037966863950474,0.6596172492782999,0.006254294680309087\ndpo_6c2_div36-0_6-2024.05.31.22.34,0.6,epoch 7,0.6025706213889634,0.010093055962045605,0.5493974553246517,0.005343837302135849\n","output_type":"stream"}]},{"cell_type":"code","source":"! rm -rf */*/optimizer.pt && git pull --rebase && python dpo.py  --output_dir=dpo_6c2_div36-0_2  --beta=0.2 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_6_choose_2_div_36_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 50     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=50 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=14","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:20:32.633685Z","iopub.execute_input":"2024-05-31T23:20:32.634178Z","iopub.status.idle":"2024-05-31T23:49:14.163549Z","shell.execute_reply.started":"2024-05-31T23:20:32.634141Z","shell.execute_reply":"2024-05-31T23:49:14.162315Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Already up to date.\nCurrent branch main is up to date.\n2024-05-31 23:20:41.084118: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 23:20:41.084190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 23:20:41.085965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 4149\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240531_232048-4s6y2ah4\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_6c2_div36-0_2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/4s6y2ah4\u001b[0m\n  0%|                                                   | 0/434 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 4.83195161819458, 'learning_rate': 4e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -60.114646911621094, 'logps/chosen': -60.310203552246094, 'logits/rejected': -38.24871826171875, 'logits/chosen': -36.57445526123047, 'epoch': 0.03}\n{'loss': 0.6791, 'grad_norm': 4.626314640045166, 'learning_rate': 2e-06, 'rewards/chosen': 0.013788385316729546, 'rewards/rejected': -0.015158697962760925, 'rewards/accuracies': 0.7748061418533325, 'rewards/margins': 0.02894708327949047, 'logps/rejected': -59.69102096557617, 'logps/chosen': -60.61844253540039, 'logits/rejected': -37.6925163269043, 'logits/chosen': -36.741905212402344, 'epoch': 1.61}\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 50/434 [00:56<06:54,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6589335799217224, 'eval_runtime': 0.9118, 'eval_samples_per_second': 228.113, 'eval_steps_per_second': 2.193, 'eval_rewards/chosen': 0.03442595899105072, 'eval_rewards/rejected': -0.036126673221588135, 'eval_rewards/accuracies': 0.8218749761581421, 'eval_rewards/margins': 0.07055263221263885, 'eval_logps/rejected': -59.69635009765625, 'eval_logps/chosen': -60.00411605834961, 'eval_logits/rejected': -39.022422790527344, 'eval_logits/chosen': -38.02882385253906, 'epoch': 1.61}\n 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 50/434 [00:57<06:54,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.6099, 'grad_norm': 4.058838844299316, 'learning_rate': 2e-06, 'rewards/chosen': 0.0729474201798439, 'rewards/rejected': -0.10963672399520874, 'rewards/accuracies': 0.8693920373916626, 'rewards/margins': 0.18258413672447205, 'logps/rejected': -60.275909423828125, 'logps/chosen': -60.5643424987793, 'logits/rejected': -37.077701568603516, 'logits/chosen': -36.21292495727539, 'epoch': 3.23}\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 100/434 [01:55<06:26,  1.16s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.598922848701477, 'eval_runtime': 0.9192, 'eval_samples_per_second': 226.28, 'eval_steps_per_second': 2.176, 'eval_rewards/chosen': 0.07563439011573792, 'eval_rewards/rejected': -0.136537104845047, 'eval_rewards/accuracies': 0.8414062261581421, 'eval_rewards/margins': 0.2121714949607849, 'eval_logps/rejected': -60.198402404785156, 'eval_logps/chosen': -59.798072814941406, 'eval_logits/rejected': -38.27984619140625, 'eval_logits/chosen': -37.387542724609375, 'epoch': 3.23}\n 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 100/434 [01:56<06:26,  1.16s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.17it/s]\u001b[A\n{'loss': 0.5329, 'grad_norm': 3.4769301414489746, 'learning_rate': 2e-06, 'rewards/chosen': 0.12043993920087814, 'rewards/rejected': -0.259487122297287, 'rewards/accuracies': 0.9041026830673218, 'rewards/margins': 0.37992703914642334, 'logps/rejected': -60.8458251953125, 'logps/chosen': -60.06056213378906, 'logits/rejected': -36.483036041259766, 'logits/chosen': -35.66364669799805, 'epoch': 4.84}\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 150/434 [02:52<05:07,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5493345856666565, 'eval_runtime': 0.9117, 'eval_samples_per_second': 228.138, 'eval_steps_per_second': 2.194, 'eval_rewards/chosen': 0.08421862125396729, 'eval_rewards/rejected': -0.2697187066078186, 'eval_rewards/accuracies': 0.84765625, 'eval_rewards/margins': 0.3539373278617859, 'eval_logps/rejected': -60.86431121826172, 'eval_logps/chosen': -59.75515365600586, 'eval_logits/rejected': -37.672142028808594, 'eval_logits/chosen': -36.903011322021484, 'epoch': 4.84}\n 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 150/434 [02:53<05:07,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.17it/s]\u001b[A\n{'loss': 0.4665, 'grad_norm': 3.528867244720459, 'learning_rate': 2e-06, 'rewards/chosen': 0.1491323709487915, 'rewards/rejected': -0.4335118532180786, 'rewards/accuracies': 0.9207177758216858, 'rewards/margins': 0.5826441645622253, 'logps/rejected': -61.85902786254883, 'logps/chosen': -60.12239074707031, 'logits/rejected': -35.965415954589844, 'logits/chosen': -35.160945892333984, 'epoch': 6.45}\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 200/434 [04:02<04:17,  1.10s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5075449347496033, 'eval_runtime': 0.9132, 'eval_samples_per_second': 227.771, 'eval_steps_per_second': 2.19, 'eval_rewards/chosen': 0.07379011809825897, 'eval_rewards/rejected': -0.42398494482040405, 'eval_rewards/accuracies': 0.8382812738418579, 'eval_rewards/margins': 0.4977750778198242, 'eval_logps/rejected': -61.63563919067383, 'eval_logps/chosen': -59.80729675292969, 'eval_logits/rejected': -37.189048767089844, 'eval_logits/chosen': -36.55571365356445, 'epoch': 6.45}\n 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 200/434 [04:03<04:17,  1.10s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\u001b[A\n{'loss': 0.4121, 'grad_norm': 3.042264223098755, 'learning_rate': 2e-06, 'rewards/chosen': 0.13856634497642517, 'rewards/rejected': -0.6426743865013123, 'rewards/accuracies': 0.9259467124938965, 'rewards/margins': 0.7812407612800598, 'logps/rejected': -62.89882278442383, 'logps/chosen': -60.017311096191406, 'logits/rejected': -35.381309509277344, 'logits/chosen': -34.77543640136719, 'epoch': 8.06}\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 250/434 [05:11<08:22,  2.73s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.469551146030426, 'eval_runtime': 0.9085, 'eval_samples_per_second': 228.952, 'eval_steps_per_second': 2.201, 'eval_rewards/chosen': 0.04509248211979866, 'eval_rewards/rejected': -0.6075199842453003, 'eval_rewards/accuracies': 0.8484375476837158, 'eval_rewards/margins': 0.6526124477386475, 'eval_logps/rejected': -62.553314208984375, 'eval_logps/chosen': -59.950782775878906, 'eval_logits/rejected': -36.785743713378906, 'eval_logits/chosen': -36.29619598388672, 'epoch': 8.06}\n 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 250/434 [05:12<08:22,  2.73s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.3609, 'grad_norm': 2.8253023624420166, 'learning_rate': 2e-06, 'rewards/chosen': 0.1427137702703476, 'rewards/rejected': -0.8536545038223267, 'rewards/accuracies': 0.9385101795196533, 'rewards/margins': 0.9963682293891907, 'logps/rejected': -63.795196533203125, 'logps/chosen': -60.06806182861328, 'logits/rejected': -35.07378005981445, 'logits/chosen': -34.261375427246094, 'epoch': 9.68}\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 300/434 [06:11<02:24,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.43525126576423645, 'eval_runtime': 0.912, 'eval_samples_per_second': 228.079, 'eval_steps_per_second': 2.193, 'eval_rewards/chosen': 0.0009196735918521881, 'eval_rewards/rejected': -0.813834011554718, 'eval_rewards/accuracies': 0.8640625476837158, 'eval_rewards/margins': 0.8147536516189575, 'eval_logps/rejected': -63.58488845825195, 'eval_logps/chosen': -60.17164611816406, 'eval_logits/rejected': -36.43834686279297, 'eval_logits/chosen': -36.100730895996094, 'epoch': 9.68}\n 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 300/434 [06:12<02:24,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.14it/s]\u001b[A\n{'loss': 0.3172, 'grad_norm': 2.708061933517456, 'learning_rate': 2e-06, 'rewards/chosen': 0.09081394225358963, 'rewards/rejected': -1.1243865489959717, 'rewards/accuracies': 0.9515718221664429, 'rewards/margins': 1.2152005434036255, 'logps/rejected': -65.315673828125, 'logps/chosen': -60.20964813232422, 'logits/rejected': -34.79054641723633, 'logits/chosen': -34.279319763183594, 'epoch': 11.29}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 350/434 [07:12<01:34,  1.13s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4076433777809143, 'eval_runtime': 0.9055, 'eval_samples_per_second': 229.707, 'eval_steps_per_second': 2.209, 'eval_rewards/chosen': -0.06973026692867279, 'eval_rewards/rejected': -1.0541967153549194, 'eval_rewards/accuracies': 0.8640625476837158, 'eval_rewards/margins': 0.9844664931297302, 'eval_logps/rejected': -64.78669738769531, 'eval_logps/chosen': -60.52489471435547, 'eval_logits/rejected': -36.119171142578125, 'eval_logits/chosen': -35.975399017333984, 'epoch': 11.29}\n 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 350/434 [07:13<01:34,  1.13s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.17it/s]\u001b[A\n{'loss': 0.278, 'grad_norm': 2.3987042903900146, 'learning_rate': 2e-06, 'rewards/chosen': 0.054450761526823044, 'rewards/rejected': -1.3937684297561646, 'rewards/accuracies': 0.9591243863105774, 'rewards/margins': 1.4482191801071167, 'logps/rejected': -66.72360229492188, 'logps/chosen': -60.5969123840332, 'logits/rejected': -34.5665397644043, 'logits/chosen': -33.921321868896484, 'epoch': 12.9}\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/434 [08:10<00:36,  1.08s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3810839354991913, 'eval_runtime': 0.9075, 'eval_samples_per_second': 229.202, 'eval_steps_per_second': 2.204, 'eval_rewards/chosen': -0.14776965975761414, 'eval_rewards/rejected': -1.3243545293807983, 'eval_rewards/accuracies': 0.874218761920929, 'eval_rewards/margins': 1.1765849590301514, 'eval_logps/rejected': -66.13749694824219, 'eval_logps/chosen': -60.91509246826172, 'eval_logits/rejected': -35.742958068847656, 'eval_logits/chosen': -35.79013442993164, 'epoch': 12.9}\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/434 [08:11<00:36,  1.08s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.16it/s]\u001b[A\n{'train_runtime': 553.3374, 'train_samples_per_second': 99.711, 'train_steps_per_second': 0.784, 'train_loss': 0.44084196376361057, 'epoch': 14.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 434/434 [08:56<00:00,  1.24s/it]\nEvaling epochs [14, 13, 12, 11]\nLoading from dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-434\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-434\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:07, 30.94s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:35, 30.74s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:31<03:03, 30.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:02<02:32, 30.45s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:33<02:02, 30.64s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/kaggle/working/cs234-project/dpo.py\", line 207, in <module>\n    r, r_sem, kl, kl_sem = ppo.eval(checkpoint, f\"epoch {epoch}\")\n  File \"/kaggle/working/cs234-project/ppo.py\", line 267, in eval\n    return run(PPOConfig(exp_name=\"eval\", eval_model=model), args=ScriptArguments(), full_name=f'{model}_{notes}')\n  File \"/kaggle/working/cs234-project/ppo.py\", line 237, in run\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:31, 30.56s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:34<01:01, 30.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:04<00:30, 30.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:35<00:00, 30.64s/it]\nmean test reward 0.7769019584109174 +/- 0.008558729777375705 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9983251690864563 from 0.00922983093187213\nmean KL -0.15031821575636664 +/- 0.10685749679587082 full 4.265136510547665 +/- 0.02762386244546228\nmedian KL 0.5646680891513824 full 4.072755575180054\nLoading from dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-403\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-403\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:04, 30.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:34, 30.59s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:31<03:02, 30.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:32, 30.42s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:02, 30.51s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:02<01:31, 30.42s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:32<01:00, 30.34s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:03<00:30, 30.36s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:34<00:00, 30.47s/it]\nmean test reward 0.7611028814823536 +/- 0.008772539336659875 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9981992542743683 from 0.00922983093187213\nmean KL -0.16251317844984847 +/- 0.09792017912598289 full 3.73052333911053 +/- 0.025010015188367932\nmedian KL 0.47074437141418457 full 3.5702948570251465\nLoading from dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-372\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-372\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:30<04:04, 30.61s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:01<03:34, 30.69s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:32<03:04, 30.70s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:02<02:33, 30.73s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:33<02:02, 30.57s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:31, 30.52s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:34<01:01, 30.63s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:05<00:30, 30.65s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:35<00:00, 30.61s/it]\nmean test reward 0.7469752771133548 +/- 0.008953612055003052 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9981527924537659 from 0.00922983093187213\nmean KL -0.10499067451908356 +/- 0.08796105395276872 full 3.2587599068000497 +/- 0.022377555302206777\nmedian KL 0.47793130576610565 full 3.1105254888534546\nLoading from dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-341\nload ref model lvwerra/gpt2-imdb\nload train model dpo_6c2_div36-0_2-2024.05.31.23.20/checkpoint-341\ndevice cuda\ndevice 0\nLoad reward model siebert/sentiment-roberta-large-english\neval batch size 256\ntest len 9\n  0%|                                                     | 0/9 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 1/9 [00:29<03:58, 29.87s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 2/9 [01:00<03:32, 30.31s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 3/9 [01:30<03:01, 30.24s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/9 [02:01<02:32, 30.47s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 5/9 [02:32<02:02, 30.53s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 6/9 [03:03<01:32, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 7/9 [03:33<01:01, 30.67s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/9 [04:04<00:30, 30.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nFor input shapes ftorch.Size([256, 22, 50257]), computed KL shape torch.Size([256, 22, 50257])\nKL approx for shape ftorch.Size([256, 22])\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:35<00:00, 30.58s/it]\nmean test reward 0.7410854388344761 +/- 0.009018999181029778 from 0.4351168664733096 +/- 0.010216795621720676\nmedian test reward 0.9980040788650513 from 0.00922983093187213\nmean KL 0.03716596584611883 +/- 0.07653269888530363 full 2.793973121455767 +/- 0.019904787014129073\nmedian KL 0.5180458426475525 full 2.6431432962417603\ndpo_6c2_div36-0_2-2024.05.31.23.20,0.2,epoch 14,0.7769019584109174,0.008558729777375705,4.265136510547665,0.02762386244546228\ndpo_6c2_div36-0_2-2024.05.31.23.20,0.2,epoch 13,0.7611028814823536,0.008772539336659875,3.73052333911053,0.025010015188367932\ndpo_6c2_div36-0_2-2024.05.31.23.20,0.2,epoch 12,0.7469752771133548,0.008953612055003052,3.2587599068000497,0.022377555302206777\ndpo_6c2_div36-0_2-2024.05.31.23.20,0.2,epoch 11,0.7410854388344761,0.009018999181029778,2.793973121455767,0.019904787014129073\n","output_type":"stream"}]},{"cell_type":"code","source":"! rm -rf */*/optimizer.pt && git pull --rebase && python dpo.py  --output_dir=dpo_6c2_div36-0_05  --beta=0.05 --dataset_name=/kaggle/working/cs234-project/pref_pairs_16_token_6_choose_2_div_36_tokenized     --model_name_or_path=lvwerra/gpt2-imdb     --per_device_train_batch_size 128  --per_device_eval_batch_size 128     --learning_rate 2e-6     --gradient_accumulation_steps 1  --warmup_steps 50     --report_to wandb     --logging_first_step --logging_strategy='steps' --logging_steps 50  --eval_steps=50 --no_remove_unused_columns --save_total_limit=4 --evaluation_strategy='steps' --lr_scheduler_type=constant_with_warmup --save_strategy='epoch' --num_train_epochs=19","metadata":{"execution":{"iopub.status.busy":"2024-05-31T23:57:45.102055Z","iopub.execute_input":"2024-05-31T23:57:45.102886Z","iopub.status.idle":"2024-06-01T00:00:44.031128Z","shell.execute_reply.started":"2024-05-31T23:57:45.102849Z","shell.execute_reply":"2024-06-01T00:00:44.029677Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Already up to date.\nCurrent branch main is up to date.\n2024-05-31 23:57:53.582990: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 23:57:53.583054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 23:57:53.584729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoad model  lvwerra/gpt2-imdb\nLoad ref model  lvwerra/gpt2-imdb\nds len 4149\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmjh388\u001b[0m (\u001b[33mmhayes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdpo_6c2_div36-0_05\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mhayes/huggingface/runs/i7xpbdrj\u001b[0m\n  0%|                                                   | 0/589 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6931, 'grad_norm': 1.207987904548645, 'learning_rate': 4e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -60.114646911621094, 'logps/chosen': -60.310203552246094, 'logits/rejected': -38.24871826171875, 'logits/chosen': -36.57445526123047, 'epoch': 0.03}\n{'loss': 0.6895, 'grad_norm': 1.2010098695755005, 'learning_rate': 2e-06, 'rewards/chosen': 0.003457672195509076, 'rewards/rejected': -0.0038091838359832764, 'rewards/accuracies': 0.7744872570037842, 'rewards/margins': 0.007266854867339134, 'logps/rejected': -59.691410064697266, 'logps/chosen': -60.61823272705078, 'logits/rejected': -37.69108200073242, 'logits/chosen': -36.74053955078125, 'epoch': 1.61}\n  8%|â–ˆâ–ˆâ–ˆâ–Œ                                      | 50/589 [00:56<09:38,  1.07s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6842409372329712, 'eval_runtime': 0.9029, 'eval_samples_per_second': 230.381, 'eval_steps_per_second': 2.215, 'eval_rewards/chosen': 0.008640723302960396, 'eval_rewards/rejected': -0.00911322794854641, 'eval_rewards/accuracies': 0.8218749761581421, 'eval_rewards/margins': 0.017753951251506805, 'eval_logps/rejected': -59.69798278808594, 'eval_logps/chosen': -60.0034294128418, 'eval_logits/rejected': -39.013282775878906, 'eval_logits/chosen': -38.020729064941406, 'epoch': 1.61}\n  8%|â–ˆâ–ˆâ–ˆâ–Œ                                      | 50/589 [00:57<09:38,  1.07s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n{'loss': 0.6704, 'grad_norm': 1.1371697187423706, 'learning_rate': 2e-06, 'rewards/chosen': 0.01822102628648281, 'rewards/rejected': -0.028400465846061707, 'rewards/accuracies': 0.8637670278549194, 'rewards/margins': 0.04662149399518967, 'logps/rejected': -60.29573059082031, 'logps/chosen': -60.56465148925781, 'logits/rejected': -37.03486633300781, 'logits/chosen': -36.172908782958984, 'epoch': 3.23}\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 100/589 [01:55<09:32,  1.17s/it]evaluation_loop\n\n  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6664658188819885, 'eval_runtime': 0.9068, 'eval_samples_per_second': 229.374, 'eval_steps_per_second': 2.206, 'eval_rewards/chosen': 0.01842322200536728, 'eval_rewards/rejected': -0.03629284352064133, 'eval_rewards/accuracies': 0.831250011920929, 'eval_rewards/margins': 0.054716065526008606, 'eval_logps/rejected': -60.241573333740234, 'eval_logps/chosen': -59.807777404785156, 'eval_logits/rejected': -38.18230438232422, 'eval_logits/chosen': -37.30451202392578, 'epoch': 3.23}\n 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 100/589 [01:56<09:32,  1.17s/it]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.18it/s]\u001b[A\n 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 124/589 [02:22<07:43,  1.00it/s]\u001b[ATraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 619, in save\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 853, in _save\n    zip_file.write_record(name, storage.data_ptr(), num_bytes)\nRuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/1: file write failed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/kaggle/working/cs234-project/dpo.py\", line 195, in <module>\n    trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1780, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2213, in _inner_training_loop\n    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2588, in _maybe_log_save_evaluate\n    self._save_checkpoint(model, trial, metrics=metrics)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2660, in _save_checkpoint\n    self._save_optimizer_and_scheduler(output_dir)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2762, in _save_optimizer_and_scheduler\n    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 618, in save\n    with _open_zipfile_writer(f) as opened_zipfile:\n  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 466, in __exit__\n    self.file_like.write_end_of_file()\nRuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 38272 vs 38164\n","output_type":"stream"},{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 203, in finish\n    logger.info(\"Stopping system monitor\")\nMessage: 'Stopping system monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 179, in _start\n    logger.debug(\"Finished system metrics aggregation loop\")\nMessage: 'Finished system metrics aggregation loop'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 163, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined cpu monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n--- Logging error ---\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 183, in _start\n    logger.debug(\"Publishing last batch of metrics\")\nTraceback (most recent call last):\nMessage: 'Publishing last batch of metrics'\nArguments: ()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 163, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined cpu monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n--- Logging error ---\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\nOSError: [Errno 28] No space left on device\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 183, in _start\n    logger.debug(\"Publishing last batch of metrics\")\nMessage: 'Publishing last batch of metrics'\nArguments: ()\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/disk.py\", line 210, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined disk monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/disk.py\", line 210, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined disk monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/gpu.py\", line 388, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined gpu monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/gpu.py\", line 388, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined gpu monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/memory.py\", line 152, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined memory monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/memory.py\", line 152, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined memory monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/memory.py\", line 152, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined memory monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/network.py\", line 96, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined network monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/network.py\", line 96, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined network monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 282, in _finish\n    self._hm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 868, in finish\n    self._system_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 206, in finish\n    asset.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/network.py\", line 96, in finish\n    self.metrics_monitor.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 202, in finish\n    logger.info(f\"Joined {thread_name} monitor\")\nMessage: 'Joined network monitor'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n","output_type":"stream"},{"name":"stdout","text":" 21%|â–ˆâ–ˆ        | 124/589 [02:25<09:06,  1.18s/it]                               \n","output_type":"stream"},{"name":"stderr","text":"--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1546, in finish\n    logger.info(\"shutting down sender\")\nMessage: 'shutting down sender'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1546, in finish\n    logger.info(\"shutting down sender\")\nMessage: 'shutting down sender'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1546, in finish\n    logger.info(\"shutting down sender\")\nMessage: 'shutting down sender'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 358, in finish\n    logger.info(\"shutting down directory watcher\")\nMessage: 'shutting down directory watcher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 358, in finish\n    logger.info(\"shutting down directory watcher\")\nMessage: 'shutting down directory watcher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 358, in finish\n    logger.info(\"shutting down directory watcher\")\nMessage: 'shutting down directory watcher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 358, in finish\n    logger.info(\"shutting down directory watcher\")\nMessage: 'shutting down directory watcher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 199, in run\n    self.dispatch_events(self.event_queue, self.timeout)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/observers/api.py\", line 368, in dispatch_events\n    handler.dispatch(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/watchdog_0_9_0/wandb_watchdog/events.py\", line 454, in dispatch\n    _method_map[event_type](event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 288, in _on_file_modified\n    logger.info(f\"file/dir modified: { event.src_path}\")\nMessage: 'file/dir modified: /kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 388, in finish\n    logger.info(\"scan: %s\", self._dir)\nMessage: 'scan: %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 388, in finish\n    logger.info(\"scan: %s\", self._dir)\nMessage: 'scan: %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 388, in finish\n    logger.info(\"scan: %s\", self._dir)\nMessage: 'scan: %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 388, in finish\n    logger.info(\"scan: %s\", self._dir)\nMessage: 'scan: %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/conda-environment.yaml', 'conda-environment.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/conda-environment.yaml', 'conda-environment.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/conda-environment.yaml', 'conda-environment.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/conda-environment.yaml', 'conda-environment.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json', 'wandb-summary.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json', 'wandb-summary.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json', 'wandb-summary.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json', 'wandb-summary.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json', 'wandb-summary.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-metadata.json', 'wandb-metadata.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-metadata.json', 'wandb-metadata.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-metadata.json', 'wandb-metadata.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-metadata.json', 'wandb-metadata.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-metadata.json', 'wandb-metadata.json')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log', 'output.log')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log', 'output.log')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log', 'output.log')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log', 'output.log')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log', 'output.log')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml', 'config.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml', 'config.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml', 'config.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml', 'config.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml', 'config.yaml')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1551, in finish\n    self._dir_watcher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 402, in finish\n    logger.info(\"scan save: %s %s\", file_path, save_name)\nMessage: 'scan save: %s %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt', 'requirements.txt')\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1554, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 172, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1554, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 172, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1554, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 172, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1554, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 172, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1554, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 172, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1554, in finish\n    self._pusher.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 172, in finish\n    logger.info(\"shutting down file pusher\")\nMessage: 'shutting down file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1555, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 178, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1555, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 178, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1555, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 178, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1555, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 178, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1555, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 178, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1555, in finish\n    self._pusher.join()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_pusher.py\", line 178, in join\n    logger.info(\"waiting for file pusher\")\nMessage: 'waiting for file pusher'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 279, in _process\n    self._hm.handle(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle\n    handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 146, in handle_request\n    logger.debug(f\"handle_request: {request_type}\")\nMessage: 'handle_request: status_report'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/wandb-summary.json',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/output.log',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/config.yaml',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 83, in _worker\n    work_item.run()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 288, in run_and_notify\n    self._do_upload_sync(event)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/step_upload.py\", line 330, in _do_upload_sync\n    job.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 57, in run\n    self.push()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/filesync/upload_job.py\", line 131, in push\n    logger.info(\"Uploaded file %s\", self.save_path)\nMessage: 'Uploaded file %s'\nArguments: ('/kaggle/working/cs234-project/wandb/run-20240531_235800-i7xpbdrj/files/requirements.txt',)\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 614, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 614, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 614, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 614, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 614, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 614, in finish\n    logger.info(\"file stream finish called\")\nMessage: 'file stream finish called'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 618, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 618, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 618, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 618, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 618, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n    self._finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 331, in _finish\n    self._sm.finish()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1558, in finish\n    self._fs.finish(self._exit_code)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/file_stream.py\", line 618, in finish\n    logger.info(\"file stream finish is done\")\nMessage: 'file stream finish is done'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1104, in emit\n    self.flush()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1084, in flush\n    self.stream.flush()\nOSError: [Errno 28] No space left on device\nCall stack:\n  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n    self._bootstrap_inner()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 49, in run\n    self._target(**self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 174, in wandb_internal\n    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\nMessage: 'Thread WriterThread:'\nArguments: ()\nThread WriterThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n    self._run()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n    self._process(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 380, in _process\n    self._wm.write(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 154, in write\n    write_handler(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 135, in _write\n    self._write_record(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 109, in _write_record\n    ret = self._ds.write(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 291, in write\n    ret = self._write_data(s)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 247, in _write_data\n    self._write_record(s)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 226, in _write_record\n    self._fp.write(s)\nOSError: [Errno 28] No space left on device\nwandb: ERROR Internal wandb error: file data was not synced\n","output_type":"stream"}]}]}